<!DOCTYPE html>
<html>
<head>
    <title>Interactive Knowledge Graph with Hyperlinks, Descriptions, and Info Panel</title>
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script type="text/javascript" src="https://unpkg.com/vis-network/standalone/umd/vis-network.min.js"></script>
    <style type="text/css">
        body {
            margin: 0;
            padding: 0;
            font-family: Arial, sans-serif;
            background-color: #2e4482;
        }

        #mynetwork {
            width: calc(100% - 200px);
            height: 100%;
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        #infopanel {
            position: absolute;
            top: 0;
            right: 0;
            width: 500px;
            height: 100%;
            padding: 10px;
            overflow: auto;
            background-color: #ffffff;
            box-sizing: border-box;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.2);
        }

        #infopanel a {
            color: #2B7CE9;
            text-decoration: none;
            font-weight: bold;
        }

        #infopanel a:hover {
            text-decoration: underline;
        }

        #graphpanel {
            position: absolute;
            top: 0;
            left: 0;
            width: calc(100% - 200px);
            height: 100%;
            box-sizing: border-box;
            background-color: #f5f5f5;
        }

        #profile {
            border-radius: 1vh;
            width: 30%;
            height: auto;
        }

        .al {
            text-align: center;
        }

    </style>
</head>
<body>
<div id="graphpanel">
    <div id="mynetwork"></div>
</div>
<div id="infopanel"></div>
<script type="text/javascript">
    var data = {"edges": [{"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 1}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 2, "to": 1}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 3, "to": 1}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 28, "to": 1}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100002, "to": 1}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 6, "to": 1}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100003, "to": 1}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100004, "to": 1}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100005, "to": 1}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100006, "to": 1}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100007, "to": 1}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 1}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100009, "to": 1}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100010, "to": 1}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100011, "to": 1}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100012, "to": 2}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100013, "to": 2}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100014, "to": 2}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100015, "to": 2}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100016, "to": 2}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100017, "to": 2}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100018, "to": 2}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100019, "to": 2}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100020, "to": 2}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100021, "to": 2}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100022, "to": 2}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 5, "to": 3}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100023, "to": 3}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100024, "to": 3}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100025, "to": 3}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100026, "to": 3}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100027, "to": 3}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100028, "to": 3}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100029, "to": 3}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100030, "to": 4}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100031, "to": 4}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100032, "to": 4}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100033, "to": 4}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100034, "to": 4}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100035, "to": 4}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 5, "to": 4}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100036, "to": 4}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100037, "to": 4}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100038, "to": 5}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100039, "to": 5}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100040, "to": 5}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100041, "to": 5}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100042, "to": 5}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100043, "to": 5}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100044, "to": 5}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100045, "to": 5}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100046, "to": 5}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100047, "to": 5}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100048, "to": 5}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100049, "to": 6}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100050, "to": 6}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100051, "to": 6}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 7, "to": 6}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 2, "to": 6}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100052, "to": 6}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100053, "to": 6}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100054, "to": 6}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 4, "to": 7}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100055, "to": 7}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100056, "to": 7}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100057, "to": 7}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100058, "to": 8}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 9, "to": 8}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100059, "to": 8}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100060, "to": 8}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100061, "to": 8}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100062, "to": 8}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100063, "to": 8}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100064, "to": 8}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100065, "to": 8}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100066, "to": 9}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100067, "to": 9}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100068, "to": 9}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100069, "to": 9}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100070, "to": 9}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100071, "to": 9}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100072, "to": 9}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100073, "to": 9}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100074, "to": 9}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100075, "to": 10}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100076, "to": 10}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100077, "to": 10}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100078, "to": 10}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100079, "to": 10}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100080, "to": 10}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100081, "to": 10}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100082, "to": 10}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100083, "to": 10}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100084, "to": 11}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100085, "to": 11}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100086, "to": 11}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100087, "to": 11}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100088, "to": 11}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100089, "to": 11}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100090, "to": 11}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100091, "to": 11}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100092, "to": 11}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100093, "to": 11}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100094, "to": 11}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100095, "to": 11}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100096, "to": 12}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100097, "to": 12}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100098, "to": 12}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100099, "to": 12}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100100, "to": 12}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100101, "to": 12}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100102, "to": 13}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100103, "to": 13}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100104, "to": 13}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 22, "to": 14}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 5, "to": 14}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100105, "to": 14}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 4, "to": 14}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100106, "to": 14}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100107, "to": 14}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100108, "to": 15}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100109, "to": 15}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100110, "to": 15}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100111, "to": 15}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100112, "to": 16}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100113, "to": 16}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100114, "to": 17}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 29, "to": 17}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100115, "to": 17}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100116, "to": 17}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100117, "to": 17}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100118, "to": 17}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 20, "to": 18}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100119, "to": 18}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100120, "to": 18}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100121, "to": 18}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100122, "to": 18}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100123, "to": 19}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100124, "to": 19}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100125, "to": 19}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100126, "to": 19}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100127, "to": 19}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100128, "to": 20}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100129, "to": 20}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100130, "to": 20}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100131, "to": 20}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100132, "to": 20}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100133, "to": 20}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100134, "to": 21}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 26, "to": 21}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100135, "to": 21}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 27, "to": 21}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100136, "to": 22}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100137, "to": 22}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100138, "to": 22}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100139, "to": 22}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100140, "to": 22}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100141, "to": 22}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100142, "to": 22}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100143, "to": 22}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 4, "to": 22}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100144, "to": 22}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100145, "to": 22}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100146, "to": 22}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 24, "to": 23}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 30, "to": 23}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100147, "to": 23}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 30, "to": 24}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100148, "to": 24}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100149, "to": 24}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 30, "to": 25}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 29, "to": 25}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100150, "to": 25}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100151, "to": 25}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100152, "to": 26}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 29, "to": 26}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100153, "to": 26}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 27, "to": 26}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100154, "to": 26}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100155, "to": 27}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 29, "to": 27}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100156, "to": 28}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100157, "to": 28}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100158, "to": 29}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100159, "to": 30}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100160, "to": 30}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100161, "to": 30}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100162, "to": 30}], "nodes": [{"color": "#fdfd96", "counter": 1, "description": "Amazon\u0027s new Time-Series Forecasting model", "domain": "https://towardsdatascience.com/deep-gpvar-upgrading-deepar-for-multi-dimensional-forecasting-e39204d90af3?gi=e6671780346b", "font": {"color": "#000000", "size": 20}, "id": 1, "label": "Deep GPVAR: Upgradin", "main": 1, "main_title": "Deep GPVAR: Upgrading DeepAR For Multi-Dimensional Forecasting", "shape": "star", "size": 32.47557003257329, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Deep GPVAR: Upgrading DeepAR For Multi-Dimensional Forecasting\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Amazon\u0027s new Time-Series Forecasting model\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e This text is about the new version of multi-dimensional time series forecasting model called Deep GPVAR by Amazon\u0027s engineers. The article discusses how Deep GPVAR differs from its predecessor, DeepAR, and how the former solves multiple problems better. It leverages low-rank Gaussian processes to model thousands of time series jointly, considering interdependencies. Moreover, it allows extra features and covariates and scales training multiple time series simultaneously. It uses a special kind of multivariate Gaussian distribution called Gaussian Copula to learn the correlation among different time series. The article provides an in-depth tutorial on how to prepare data, preprocess it, and train Deep GPVAR for the demand energy forecasting task. The author demonstrated how to convert data to the TimeSeriesDataset format and normalize it for better training results.\n\nKEYWORDS\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2023-03-24 (\u0027afternoon\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 1.9%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 8.0 (37 / 296)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 3\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 4599\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 2626 (very large)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 57.1% (2626 / 4599)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 26.0% (1198 / 4599)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 18.8% (1198 / 4599)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 14.9% (686 / 4599)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 11.9% (545 / 4599)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 35.9% (1651 / 4599)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e deep gpvar, deepar, multi-dimensional forecasting, time series, gaussian copula, low-rank gaussian processes, interdependencies, timeseriesdataset, demand energy forecasting, normalization\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e time(80), series(56), model(56), gaussian(55), using(43), deep(34), copula(34), gpvar(30), forecasting(28), distributed(25), variable(25), data(21), prediction(21), training(20), also(20), covariance(19), transformer(19), random(19), rank(18), dataset(17), learn(16), observation(16), function(15), step(14), parameter(14), true(14), deepar(13), low(13), matrix(13), multivariate(13)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e time series(56), deep gpvar(27), random variable(17), gaussian copula(16), low rank(13), covariance matrix(11), series dataset(9), time step(7), rank gaussian(6), multiple time(6), forecasting model(5), model using(5), multivariate gaussian(5), gaussian distributed(5), copula function(5)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e time series dataset(9), multiple time series(6), low rank gaussian(5), gaussian logs likelihood(5), low rank normalize(4), multivariate gaussian distributed(4), random variable marginal(4), observation time series(4), time series forecasting(3), model deep gpvar(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/deep-gpvar-upgrading-deepar-for-multi-dimensional-forecasting-e39204d90af3?gi=e6671780346b", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "A Deep Learning model for zero-shot time-series forecasting", "domain": "https://towardsdatascience.com/n-beats-time-series-forecasting-with-neural-basis-expansion-af09ea39f538?gi=c4bebdd60d00", "font": {"color": "#000000", "size": 20}, "id": 2, "label": "N-BEATS: Time-Series", "main": 1, "main_title": "N-BEATS: Time-Series Forecasting with Neural Basis Expansion", "shape": "star", "size": 35.0814332247557, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: N-BEATS: Time-Series Forecasting with Neural Basis Expansion\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: A Deep Learning model for zero-shot time-series forecasting\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article discusses beats, a deep learning-based time series forecasting model that outperforms established statistical models and provides interpretable forecasts. The model uses neural basis expansion for backcast and forecast signals and achieves unmatched zero-shot transfer learning capabilities. The article discusses the key features of beats and highlights its interpretability, suitability for transfer learning, and ensembling capabilities. The authors also describe the model\u0027s architecture, including the basic block, trend and seasonality stacks, and their shared weights. They also discuss the model\u0027s successful performance on various benchmark datasets compared to other models, including those trained on zero-shot learning. Finally, the article discusses beats\u0027 novel contributions to the fields of deep learning and forecasting, including its meta-learning architecture, which allows for efficient transfer learning across multiple datasets.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-11-25 (\u0027evening\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 2.9%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 10.5 (57 / 598)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 3\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 2795\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 1544 (large)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 55.2% (1544 / 2795)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 29.6% (826 / 2795)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 20.2% (826 / 2795)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 16.2% (452 / 2795)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 12.5% (349 / 2795)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 34.1% (952 / 2795)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e beats, time series forecasting, deep learning model, zero shot time series forecasting, neural basis expansion, interpretability, transfer learning, arima, ensembling, meta learning\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e models(65), beats(49), learning(40), forecasting(30), time(26), block(26), series(24), interpretable(22), stack(18), using(17), dataset(17), transferable(16), shot(15), zero(14), figure(13), results(12), arima(11), layer(11), authors(11), parameter(11), architecture(10), signal(10), generalize(9), backcast(9), basis(8), let(8), describe(8), residuals(8), first(8), trained(8)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e time series(22), zero shot(14), transferable learning(13), series forecasting(8), learning models(8), deep learning(6), forecasting models(5), trend seasonality(5), parameter estimation(5), shot transferable(4), backcast forecasting(4), forecasting signal(4), box jenkins(4), shown figure(4), shot learning(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e time series forecasting(8), deep learning models(4), zero shot transferable(4), shot transferable learning(4), neural basis expansion(3), box jenkins method(3), shot learning models(3), zero shot learning(3), zero shot beats(3), zero shot time(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/n-beats-time-series-forecasting-with-neural-basis-expansion-af09ea39f538?gi=c4bebdd60d00", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Create accurate and interpretable predictions", "domain": "https://towardsdatascience.com/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91?gi=de7ed39cfe79", "font": {"color": "#000000", "size": 20}, "id": 3, "label": "Temporal Fusion Tran", "main": 1, "main_title": "Temporal Fusion Transformer: Time Series Forecasting with Deep Learning - Complete Tutorial", "shape": "star", "size": 49.153094462540714, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Temporal Fusion Transformer: Time Series Forecasting with Deep Learning - Complete Tutorial\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Create accurate and interpretable predictions\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e This article provides a tutorial on how to build and evaluate accurate and interpretable time series forecasting models using the Temporal Fusion Transformer (TFT), which outperforms other popular deep learning models. The article explains the novelties of TFT, including its end-to-end project framework for energy demand forecasting, and covers topics such as data preparation, model architecture, hyperparameter optimization, and model evaluation. It also highlights TFT\u0027s unique features, such as its interpretable predictions based on attention mechanisms and variable importance, and its robustness to extreme events. The article uses the ElectricityLoadDiagrams20112014 dataset for demonstration purposes and explores various interpretability aspects of TFT, including seasonality-wise and feature-wise interpretability using attention scores and variable selection network. Finally, the article discusses the importance of hyperparameter tuning and model evaluation and concludes that TFT is a groundbreaking model for time series forecasting, providing both state-of-the-art results and interpretability.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-11-05 (\u0027night\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 8.5%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 7.8 (165 / 1291)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 12\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 2744\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 1585 (large)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 57.8% (1585 / 2744)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 29.7% (816 / 2744)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 21.4% (816 / 2744)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 15.8% (433 / 2744)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 10.8% (296 / 2744)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 32.5% (891 / 2744)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e time series forecasting, deep learning models, temporal fusion transformer, interpretable predictions, attention mechanism, feature importance, seasonality patterns, robustness, variable selection network, hyperparameter tuning\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e time(62), models(45), series(37), predictions(30), data(22), tft(22), feature(21), use(19), forecasting(18), temporal(16), transformers(15), attention(15), variable(15), fusion(14), created(14), values(14), plot(14), consumer(13), also(12), interpretable(11), training(11), dataset(11), validation(10), seasonal(10), timeseriesdataset(10), events(9), one(9), figure(9), notice(9), learning(8)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e time series(37), temporal fusion(14), fusion transformers(14), series forecasting(6), time varying(6), power usage(6), target variable(5), attention weights(5), deep learning(4), predictions validation(4), validation data(4), time invariant(4), timeseriesdataset instance(4), time steps(4), outputs predictions(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e temporal fusion transformers(14), time series forecasting(6), deep learning models(3), time series data(3), predictions validation data(3), ld2011_2014 txt zip(3), variable selection networks(3), learning models time(2), models time series(2), get predictions validation(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/temporal-fusion-transformer-time-series-forecasting-with-deep-learning-complete-tutorial-d32c1e51cd91?gi=de7ed39cfe79", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Everything you need to know about Time Series and Deep Learning", "domain": "https://towardsdatascience.com/the-best-deep-learning-models-for-time-series-forecasting-690767bc63f0?gi=3a7e472d8f8c", "font": {"color": "#000000", "size": 20}, "id": 4, "label": "The Best Deep Learni", "main": 1, "main_title": "The Best Deep Learning Models for Time Series Forecasting", "shape": "star", "size": 70.0, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: The Best Deep Learning Models for Time Series Forecasting\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Everything you need to know about Time Series and Deep Learning\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article discusses the use of deep learning models for time series forecasting and covers the landscape of time series forecasting over the past two years, during which time there have been significant changes due to the results of several high-profile competitions. The article discusses various deep learning models for time series forecasting, including Beats, deepAR, SpaceTimeFormer, and TFT, and focuses on their advantages and novel features. In addition, the article emphasizes the importance of interpretability in time series forecasting models, and highlights the advantage of models that can leverage multiple time series data.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2021-12-20 (\u0027afternoon\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 16.8%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 6.2 (325 / 1999)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 13\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 2288\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 1359 (large)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 59.4% (1359 / 2288)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 32.7% (749 / 2288)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 23.8% (749 / 2288)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 15.8% (361 / 2288)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 13.1% (300 / 2288)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 32.6% (746 / 2288)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e deep learning, time series forecasting, competitions, models, beats, deepar, spacetimeformer, tft, interpretability, multiple time series\n\nsummary\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e time(50), models(44), series(36), forecasting(32), use(26), learning(20), deep(14), feature(14), deepar(14), inputs(13), sequences(12), temporal(12), competition(11), tft(11), transformers(10), block(10), beats(9), predicted(9), multiple(9), interpretable(9), also(9), known(8), architecture(8), based(7), differs(7), stack(7), scaling(7), power(7), dependent(7), embedding(7)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e time series(34), series forecasting(11), deep learning(9), multiple time(7), time sequences(5), time dependent(5), use models(4), temporal fusion(4), fusion transformers(4), shown figure(4), spatial relationships(4), known present(4), top levels(3), models key(3), key advantage(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e time series forecasting(11), multiple time series(7), temporal fusion transformers(4), models key advantage(3), demand forecasting scenario(3), time dependent feature(3), dependent feature known(3), feature known present(3), landscape time series(2), ability use models(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/the-best-deep-learning-models-for-time-series-forecasting-690767bc63f0?gi=3a7e472d8f8c", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Google\u0027s state-of-the-art Transformer has it all", "domain": "https://towardsdatascience.com/temporal-fusion-transformer-googles-model-for-interpretable-time-series-forecasting-5aa17beb621?gi=06c1d71acee9", "font": {"color": "#000000", "size": 20}, "id": 5, "label": "Temporal Fusion Tran", "main": 1, "main_title": "Temporal Fusion Transformer: Time Series Forecasting with Interpretability", "shape": "star", "size": 43.28990228013029, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Temporal Fusion Transformer: Time Series Forecasting with Interpretability\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Google\u0027s state-of-the-art Transformer has it all\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e Temporal Fusion Transformer (TFT) is a state-of-the-art transformer-based model for time series forecasting with interpretable multi-head attention mechanism proposed by Google. TFT is designed to account for both univariate and multivariate time series with additional exploratory variables, and to make use of historical information in order to create competitive models. It supports multiple time series coming from different distributions and provides feature interpretability by incorporating a variable selection network. TFT is also capable of multi-step predictions and can output prediction intervals using quantile regression. The model outperformed traditional statistical models as well as other deep learning models, such as ARIMA and DeepAR, on standard time series benchmark datasets.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2021-11-23 (\u0027night\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 6.2%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 8.9 (120 / 1071)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 10\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 3008\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 1748 (large)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 58.1% (1748 / 3008)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 30.4% (913 / 3008)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 21.4% (913 / 3008)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 16.0% (481 / 3008)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 13.2% (398 / 3008)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 31.8% (958 / 3008)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e temporal fusion transformer, time series forecasting, interpretability, google, transformer, multi-head attention, deep learning, quantile regression, feature selection, lstm encoder-decoder\nsummary\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e modeling(42), time(35), variable(24), using(24), temporal(22), transformer(22), predictions(22), features(21), series(20), also(18), tft(18), forecasting(17), input(16), fusion(15), dataset(15), different(13), step(13), attention(13), networks(13), interpretable(12), quantile(12), data(11), training(11), take(10), static(10), multi(10), encoded(10), architecture(9), implementation(9), grn(9)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e time series(20), temporal fusion(15), fusion transformer(15), static variable(7), target variable(7), predictions intervals(7), headed attention(5), lstm encoded(5), encoded decoder(5), series forecasting(4), multi horizon(4), quantile loss(4), interpretable multi(4), multi headed(4), linear regression(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e temporal fusion transformer(15), lstm encoded decoder(5), time series forecasting(4), multi headed attention(4), output predictions intervals(3), variable selection networks(3), context aware embeddings(3), interpretable multi headed(3), time series modeling(2), multiple time series(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/temporal-fusion-transformer-googles-model-for-interpretable-time-series-forecasting-5aa17beb621?gi=06c1d71acee9", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "What are copula functions and why do we need them", "domain": "https://towardsdatascience.com/copulas-an-essential-guide-applications-in-time-series-forecasting-f5c93dcd6e99?gi=988f61663eb5", "font": {"color": "#000000", "size": 20}, "id": 6, "label": "Copulas: An Essentia", "main": 1, "main_title": "Copulas: An Essential Guide \u0026 Applications in Time Series Forecasting", "shape": "star", "size": 32.99674267100977, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Copulas: An Essential Guide \u0026 Applications in Time Series Forecasting\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: What are copula functions and why do we need them\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e This article provides an introductory guide to copulas, their applications in time series forecasting, and how to create Gaussian copulas using Python. It explains several statistical concepts, including probability integral transform and inverse sampling, and provides step-by-step instructions and plots to illustrate how copulas work. The article discusses the need for copulas in modeling complex dependencies among multiple variables, particularly in high-dimensional data analysis, and highlights the importance of copulas in various applications, including finance and deep learning models. Finally, it showcases several novel papers that use copulas in architecture, such as deep GPVAR and TACTIS, and introduces CopulaCPTS, a model that combines copulas with conformal prediction for multi-step time series forecasting.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2023-03-06 (\u0027afternoon\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 2.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 8.6 (41 / 353)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 4\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 3460\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 1947 (very large)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 56.3% (1947 / 3460)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 24.1% (834 / 3460)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 17.4% (834 / 3460)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 15.1% (521 / 3460)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 13.3% (459 / 3460)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 35.8% (1238 / 3460)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e copulas, time series forecasting, multivariate distribution, probability integral transform, empirical distribution function, gaussian copula, deep learning models, transformer-based models, attention mechanism, conformal prediction\n\nsummary\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e copula(84), distribution(58), use(46), sampled(41), gaussian(34), cdf(34), plt(29), gamma(24), fontsize(22), models(21), stats(21), transformer(20), beta(20), time(19), variables(19), uniform(19), random(18), data(18), function(17), inverse(16), step(15), sns(15), series(14), let(14), plots(14), forecasting(13), created(12), density(12), gamma1(12), axis(11)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e gaussian copula(25), fontsize plt(20), time series(14), random variables(12), inverse sampled(11), probability integral(9), integral transformer(9), uniform sampled(8), plt show(8), series forecasting(7), use copula(7), distribution function(7), beta gamma(7), sns histplot(6), stats density(6)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e probability integral transformer(9), fontsize plt show(8), time series forecasting(7), stats density plt(6), density plt xlabel(6), fontsize plt ylabel(6), plt ylabel density(6), ylabel density fontsize(6), density fontsize plt(6), fontsize plt title(6)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/copulas-an-essential-guide-applications-in-time-series-forecasting-f5c93dcd6e99?gi=988f61663eb5", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Amazon\u0027s autoregressive deep network", "domain": "https://towardsdatascience.com/deepar-mastering-time-series-forecasting-with-deep-learning-bc717771ce85?gi=2131a70c1281", "font": {"color": "#000000", "size": 20}, "id": 7, "label": "DeepAR: Mastering Ti", "main": 1, "main_title": "DeepAR: Mastering Time-Series Forecasting with Deep Learning", "shape": "star", "size": 36.384364820846905, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: DeepAR: Mastering Time-Series Forecasting with Deep Learning\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Amazon\u0027s autoregressive deep network\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article discusses deepar, an autoregressive recurrent network developed by Amazon that combines deep learning and traditional probabilistic forecasting to enhance forecasting accuracy. Deep learning models like deepar use LSTM networks to create probabilistic outputs, unlike traditional statistical models like ARIMA. Deepar stands out by supporting multiple time series and leveraging extra features and covariates to improve forecasts. The article also discusses other deep learning models that compete with deepar and present notable differences, such as the temporal fusion transformer (TFT), which is an interpretable multi-horizon time series forecasting model. Deepar constitutes a milestone in the time series community and is prevalent in production as part of Amazon\u0027s GluonTS toolkit for time series forecasting.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-11-14 (\u0027evening\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 3.5%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 5.6 (67 / 377)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 1\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1875\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 1042 (large)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 55.6% (1042 / 1875)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 32.3% (606 / 1875)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 22.5% (606 / 1875)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 17.0% (318 / 1875)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 11.6% (218 / 1875)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 35.4% (663 / 1875)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e deepar, time series forecasting, deep learning, autoregressive, lstm, probabilistic forecasting, transformer, multi-horizon, amazon, forecasting accuracy \nsummary\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e time(37), model(37), deepar(36), series(27), lstm(24), forecasting(21), steps(15), gaussian(14), learning(13), predictions(11), use(11), parameters(11), works(10), one(10), distribution(10), autoregressive(9), trained(9), calculates(9), deep(8), data(8), different(8), networks(7), created(7), first(7), likelihood(7), function(7), z_i(7), tft(7), amazon(6), multiple(6)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e time series(27), time steps(9), deep learning(7), series forecasting(6), gaussian distribution(6), multiple time(5), learning model(5), hidden state(5), deepar works(4), forecasting model(4), gaussian likelihood(4), likelihood function(4), dense layers(4), multi horizon(4), recurrent networks(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e time series forecasting(6), multiple time series(5), deep learning model(5), mean standard deviation(3), target variable z_i(3), previous time steps(3), multi horizon forecasting(3), horizon forecasting model(3), international journal forecasting(3), time series deepar(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/deepar-mastering-time-series-forecasting-with-deep-learning-bc717771ce85?gi=2131a70c1281", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Whisper Use Cases", "domain": "https://medium.com/geekculture/create-subtitles-for-youtube-videos-and-movies-in-97-languages-for-free-3f98dce3c7bf", "font": {"color": "#000000", "size": 20}, "id": 8, "label": "Use AI to Create Sub", "main": 1, "main_title": "Use AI to Create Subtitles For Youtube Videos and Movies in 97 Languages - For Free", "shape": "star", "size": 31.302931596091206, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Use AI to Create Subtitles For Youtube Videos and Movies in 97 Languages - For Free\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Whisper Use Cases\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e This text discusses Whisper, an open source and free speech recognition model that can transcribe audio files with human-level performance, and also offer translation in multiple languages. Whisper is a large deep learning model created by OpenAI that challenges and sometimes outperforms commercial applications like Amazon Alexa and Apple Siri. Although it requires specifying parameters and a GPU to function optimally, Whisper is easy to use and can transcribe and translate audio and video files with subtitles. The text also provides detailed instructions on how to use Whisper with Google Colab, a free workspace service that provides GPU pre-configured environment.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027https://medium.com/geekculture\u0027\u003eGeek Culture\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-10-17 (\u0027afternoon\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 1.4%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 27.9 (28 / 780)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 9\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 988\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 543 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 55.0% (543 / 988)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 38.3% (378 / 988)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 26.0% (378 / 988)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 17.4% (172 / 988)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 9.6% (95 / 988)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 32.0% (316 / 988)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e whisper, speech recognition, open source, audio files, transcribing, translation, deep learning, gpu, video files, subtitles\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e files(27), whisper(16), video(15), use(12), models(8), notebook(8), upload(8), run(7), language(6), also(6), audio(6), code(6), gpu(6), colab(6), downloaded(6), choose(6), follow(6), select(6), translation(5), configured(5), step(5), left(5), matrix(5), create(4), subtitles(4), youtube(4), transcribe(4), human(4), large(4), take(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e video files(7), audio files(4), files upload(4), code block(4), create subtitles(3), youtube video(3), use whisper(3), tab select(3), downloaded video(3), run code(3), matrix human(3), human beings(3), beings disease(3), disease cancer(3), cancer planet(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e matrix human beings(3), human beings disease(3), beings disease cancer(3), disease cancer planet(3), cancer planet mp4(3), youtube video movie(2), use google colab(2), also drag drop(2), files see follow(2), upload video files(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://medium.com/geekculture/create-subtitles-for-youtube-videos-and-movies-in-97-languages-for-free-3f98dce3c7bf", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "An AI model compatible with 97 languages - and how to use it", "domain": "https://towardsdatascience.com/whisper-transcribe-translate-audio-files-with-human-level-performance-df044499877?gi=1dda9c402c66", "font": {"color": "#000000", "size": 20}, "id": 9, "label": "Whisper: Transcribe ", "main": 1, "main_title": "Whisper: Transcribe \u0026 Translate Audio Files With Human-Level Performance", "shape": "star", "size": 33.127035830618894, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Whisper: Transcribe \u0026 Translate Audio Files With Human-Level Performance\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: An AI model compatible with 97 languages - and how to use it\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article introduces \"Whisper,\" a powerful and innovative automatic speech recognition model based on deep learning. Whisper is trained using multitask training, which can transcribe and translate audio files of multiple languages with remarkable human-level performance. The model is based on a novel architecture that generalizes well and is resilient to distribution shift. It uses weakly supervised learning and zero-shot classification to train on large amounts of diverse data. Whisper has remarkable applications across many industries, from transcribing large audio files to creating subtitles, and improving the quality of life for those with hearing impairment. This article provides a detailed overview of the model\u0027s development, architecture, and benefits, along with code examples showing how to use the model.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-10-07 (\u0027afternoon\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 2.2%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 15.0 (42 / 630)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 3\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 2247\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 1286 (large)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 57.2% (1286 / 2247)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 35.1% (788 / 2247)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 25.3% (788 / 2247)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 17.3% (388 / 2247)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 12.4% (279 / 2247)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 30.4% (684 / 2247)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e whisper, transcribe, translate, audio files, human level performance, deep learning, multitask training, weakly supervised learning, generalization, zero-shot classification\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e whisper(48), model(40), used(24), audio(21), trained(17), task(15), translation(14), language(13), english(13), speech(13), learning(12), dataset(12), transcribe(10), file(10), human(10), let(10), install(10), example(9), transcription(9), authors(9), also(9), scale(9), large(8), decoder(8), performing(7), paper(7), openai(7), better(7), input(7), generalize(6)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e audio file(9), speech recognition(6), input audio(6), language identification(5), human level(4), level performing(4), audio language(4), used whisper(4), install ffmpeg(4), generalize well(3), zero shot(3), timestamps input(3), large scale(3), whisper outperforms(3), pip install(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e human level performing(4), input audio language(4), audio language identification(4), timestamps input audio(3), audio file human(2), file human level(2), zero shot classification(2), transcribe large audio(2), large audio file(2), whisper trained language(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/whisper-transcribe-translate-audio-files-with-human-level-performance-df044499877?gi=1dda9c402c66", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Find out how the model works - coding example included", "domain": "https://towardsdatascience.com/clip-the-most-influential-ai-model-from-openai-and-how-to-use-it-f8ee408958b1?gi=7da8886ca927", "font": {"color": "#000000", "size": 20}, "id": 10, "label": "CLIP: The Most Influ", "main": 1, "main_title": "CLIP: The Most Influential AI Model From OpenAI - And How To Use It", "shape": "star", "size": 35.21172638436482, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: CLIP: The Most Influential AI Model From OpenAI - And How To Use It\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Find out how the model works - coding example included\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article explains the breakthroughs and components of OpenAI\u0027s influential model called CLIP (Contrastive Language-Image Pre-Training). CLIP is a state-of-the-art open-source multi-modal zero-shot model that can index photos and predict relevant text descriptions without optimizing for a particular task. It uses contrastive pre-training, a type of loss that minimizes the difference between the cosine similarities of similar image-text pairs and maximizes the difference between dissimilar ones. CLIP is a milestone in the community of natural language processing (NLP) and computer vision, and its success has inspired the development of other CLIP-based models. While CLIP suffers from some limitations like polysemy, data efficiency, and distribution shift, it has the potential for improvement and constitutes a significant model for researchers interested in text-image models that revolutionized research.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-09-26 (\u0027evening\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 3.0%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 12.4 (58 / 718)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 5\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 2674\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 1509 (large)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 56.4% (1509 / 2674)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 31.4% (839 / 2674)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 22.3% (839 / 2674)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 17.1% (456 / 2674)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 10.9% (292 / 2674)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 32.3% (865 / 2674)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e open source, multi-modal, zero-shot, contrastive pre-training, image-text, natural language processing, computer vision, distribution shift, data efficiency, polysemy\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e clip(80), image(62), models(54), use(26), training(26), text(25), shot(19), descriptions(16), classifier(15), learning(13), similar(12), authors(12), zero(11), datasets(11), example(10), let(10), contrastive(10), pre(10), data(10), language(9), label(9), paired(9), first(9), figure(9), embedding(9), specific(8), class(8), prompt(8), encoder(8), classification(8)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e zero shot(11), image text(9), pre training(9), text descriptions(7), distribution shift(7), contrastive pre(6), cosine similar(6), shot classification(6), clip use(5), text embedding(5), image encoder(5), use clip(5), open source(4), shot learning(4), text prompt(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e contrastive pre training(6), image text descriptions(4), clip zero shot(3), zero shot classification(3), zero shot classifier(3), dalle stable diffusion(2), clip milestone community(2), milestone community let(2), zero shot models(2), wearing beanie girl(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/clip-the-most-influential-ai-model-from-openai-and-how-to-use-it-f8ee408958b1?gi=7da8886ca927", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "From building the app to deployment - with code included", "domain": "https://towardsdatascience.com/build-a-named-entity-recognition-app-with-streamlit-f157672f867f?gi=4065bdbd378c", "font": {"color": "#000000", "size": 20}, "id": 11, "label": "Build a Named Entity", "main": 1, "main_title": "Build a Named Entity Recognition App with Streamlit", "shape": "star", "size": 30.912052117263844, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Build a Named Entity Recognition App with Streamlit\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: From building the app to deployment - with code included\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e This article presents a step-by-step guide on how to integrate a named entity recognition (NER) model trained with the WNUT_17 dataset into a Streamlit app, using HuggingFace and Roberta. The goal of the app is to tag input sentences per user request in real-time, and the article addresses the challenges of deploying large language models in Streamlit. The piece highlights the strengths of Streamlit as an easy-to-use tool for creating interactive applications, with a strong community and monthly updates, and provides a full example of building and deploying an NER model in Streamlit. It also discusses various app deployment options, such as Streamlit Cloud, HuggingFace Spaces, and Heroku, and concludes with some closing remarks on the efficiency of Streamlit in creating interactive data science projects with almost zero knowledge of CSS, HTML, or JavaScript.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-08-31 (\u0027evening\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 1.3%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 24.2 (25 / 606)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 1\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1232\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 683 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 55.4% (683 / 1232)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 38.9% (479 / 1232)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 27.8% (479 / 1232)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 16.4% (202 / 1232)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 10.0% (123 / 1232)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 30.6% (377 / 1232)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e named entity recognition, streamlit, app deployment, huggingface, roberta, data pipeline, web application, interactive, community, git\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e streamlit(25), model(15), apps(14), used(12), files(10), spaces(9), project(8), add(8), hugginface(7), input(7), sentence(7), also(7), create(7), follow(7), required(7), functionality(7), building(6), deploy(6), tags(6), large(6), data(6), libraries(6), download(6), git(6), tool(5), science(5), example(5), generates(5), page(5), code(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e input sentence(6), data science(5), helper functionality(4), entity recognition(3), building apps(3), model streamlit(3), hugginface spaces(3), tags input(3), science project(3), required txt(3), txt files(3), add helper(3), named entity(2), apps streamlit(2), streamlit easy(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e tags input sentence(3), data science project(3), required txt files(3), add helper functionality(3), named entity recognition(2), streamlit easy used(2), easy used tool(2), apps free streamlit(2), free streamlit cloud(2), helper functionality download(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/build-a-named-entity-recognition-app-with-streamlit-f157672f867f?gi=4065bdbd378c", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "From data preparation to model training for NER tasks - and how to tag your own sentences", "domain": "https://towardsdatascience.com/named-entity-recognition-with-deep-learning-bert-the-essential-guide-274c6965e2d?gi=3754c3d6ab58", "font": {"color": "#000000", "size": 20}, "id": 12, "label": "Named Entity Recogni", "main": 1, "main_title": "Named Entity Recognition with Deep Learning (BERT) - The Essential Guide", "shape": "star", "size": 33.90879478827362, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Named Entity Recognition with Deep Learning (BERT) - The Essential Guide\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: From data preparation to model training for NER tasks - and how to tag your own sentences\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e This article is an essential guide for building a Named Entity Recognition (NER) model using deep learning techniques with BERT pre-trained models, Hugging Face library, and the WNUT_17 dataset. The article covers topics such as data preparation, tokenization, fine-tuning, and evaluation. It also provides clear instructions, code snippets, and examples for each step of the process. Overall, the article offers a comprehensive and practical approach to building a high-performance NER model.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-08-12 (\u0027afternoon\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 2.5%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 17.5 (48 / 840)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 5\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1232\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 707 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 57.4% (707 / 1232)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 36.3% (447 / 1232)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 24.8% (447 / 1232)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 18.7% (230 / 1232)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 8.7% (107 / 1232)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 31.9% (393 / 1232)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e named entity recognition, deep learning, bert, data preparation, model training, ner tasks, tokenization, padding, evaluation, model prediction\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e tokens(32), entity(21), model(20), train(19), examples(13), datasets(13), use(12), tagged(10), labels(9), task(8), classifier(8), let(8), word(8), function(8), data(7), sentences(7), location(7), also(7), 100(7), recognition(6), learning(6), bert(6), ner(6), baseline(6), named(5), deep(5), works(5), correctly(5), ner_tag(5), corporation(5)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e train examples(8), entity recognition(6), named entity(5), deep learning(5), ner task(3), nlp task(3), train datasets(3), test set(3), indicates tokens(3), entity indicates(3), fine tune(3), baseline model(3), correctly tagged(3), model train(2), predefined entity(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e named entity recognition(4), entity indicates tokens(2), train validation datasets(2), bert base uncased(2), sample train examples(2), train examples see(2), examples see works(2), special tokens cls(2), tokens cls sep(2), loss function ignoring(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/named-entity-recognition-with-deep-learning-bert-the-essential-guide-274c6965e2d?gi=3754c3d6ab58", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Mini Example", "domain": "https://towardsdatascience.com/data-science-write-robust-python-with-static-typing-c71b9c9c8044?gi=4719cf92db28", "font": {"color": "#000000", "size": 20}, "id": 13, "label": "Data Science: Write ", "main": 1, "main_title": "Data Science: Write Robust Python With Static Typing", "shape": "star", "size": 34.4299674267101, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Data Science: Write Robust Python With Static Typing\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Mini Example\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e This article discusses the importance of type annotations in Python for writing clean, readable and robust code. It explains how annotations can be used for both variable and function declarations, and provides examples and demonstrations of their use. The article also covers advanced annotations, such as callable and sequence annotations, and introduces the concept of type variables for introducing generics. It concludes by highlighting the importance of readable code and summarizing the benefits of using type annotations in Python.\n\nKEYWORDS\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-07-27 (\u0027afternoon\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 2.7%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 14.3 (52 / 743)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 4\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1386\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 772 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 55.7% (772 / 1386)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 32.3% (447 / 1386)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 21.1% (447 / 1386)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 18.4% (255 / 1386)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 9.6% (133 / 1386)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 28.6% (396 / 1386)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e python, type annotations, clean code, robust code, variable annotations, function annotations, callable annotations, sequence annotations, type variables, generics, readability, typing module\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e types(51), annotations(36), function(28), use(28), python(24), variables(21), mypy(14), let(11), languages(9), example(9), follow(9), integers(9), statically(7), code(7), checking(7), also(7), module(7), output(7), sequences(7), data(6), two(6), see(6), strings(6), script(6), write(5), arguments(5), return(5), pep(5), however(5), hints(5)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e statically types(7), types annotations(7), types variables(6), types checking(6), types module(6), types languages(5), use annotations(5), use types(4), let see(4), function sum_2_numbers(4), follow output(4), data types(3), types hints(3), annotations function(3), function annotations(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e statically types languages(3), let see example(3), get follow output(3), dynamic types languages(2), types checking runtime(2), types languages typically(2), use types annotations(2), annotations arguments return(2), force types checking(2), run script use(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/data-science-write-robust-python-with-static-typing-c71b9c9c8044?gi=4719cf92db28", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "A powerful model that combines the best of both worlds", "domain": "https://towardsdatascience.com/block-recurrent-transformer-lstm-and-transformer-combined-ec3e64af971a?gi=dda1879095d9", "font": {"color": "#000000", "size": 20}, "id": 14, "label": "Block-Recurrent Tran", "main": 1, "main_title": "Block-Recurrent Transformer: LSTM and Transformer Combined", "shape": "star", "size": 33.51791530944625, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Block-Recurrent Transformer: LSTM and Transformer Combined\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: A powerful model that combines the best of both worlds\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e This article discusses a new deep-learning model, called the Block Recurrent Transformer, which is a hybrid of the Transformer and LSTM Transformer models, making it a powerful tool for time-series forecasting. The article highlights the shortcomings of the Transformer model, such as cost and scalability, and demonstrates how the Block Recurrent Transformer seeks to address these issues. The Block Recurrent Transformer combines self and cross attention, as well as sliding self attention, to achieve a linear cost, which is ideal for analyzing long-form texts. The feedback mode is also investigated, and the paper provides optimal configurations for the model, which outperforms the Transformer in terms of perplexity and speed.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-07-06 (\u0027afternoon\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 2.3%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 13.7 (45 / 616)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 1\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 2828\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 1654 (large)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 58.5% (1654 / 2828)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 29.9% (845 / 2828)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 20.7% (845 / 2828)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 16.0% (453 / 2828)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 11.4% (321 / 2828)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 34.5% (975 / 2828)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e recurrent transformer, lstm transformer, deep learning, time series forecasting, transformer, self attention, cross attention, sliding self attention, long-form texts, linear complexity, perplexity, feedback mode, scalability\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e recurrent(62), transformers(50), attention(43), models(38), block(32), tokens(26), input(24), self(20), using(19), layers(18), sentence(17), long(16), state(15), time(13), vector(13), processing(11), words(11), encoder(11), cells(11), sliding(11), authors(11), figure(10), steps(10), mode(10), series(9), sequence(9), embeddings(9), keys(9), lstm(8), rnns(8)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e block recurrent(19), self attention(19), recurrent transformers(12), recurrent cells(11), time series(9), state vector(9), _block recurrent(7), keys value(7), input sentence(6), sliding self(6), language models(5), long range(5), tokens input(5), transformers layers(5), cross attention(5)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e block recurrent transformers(12), sliding self attention(6), time series forecasting(3), recurrent transformers block(3), transformers block recurrent(3), large attention window(3), tokens input sentence(3), series forecasting task(2), recurrent transformers novel(2), tokens input sequence(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/block-recurrent-transformer-lstm-and-transformer-combined-ec3e64af971a?gi=dda1879095d9", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "On Referred Memberships", "domain": "https://medium.com/illumination/analyze-your-referred-memberships-with-this-maths-formula-5b5627959830", "font": {"color": "#000000", "size": 20}, "id": 15, "label": "Analyze Your Referre", "main": 1, "main_title": "Analyze Your Referred Memberships With This Maths Formula", "shape": "star", "size": 30.781758957654723, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Analyze Your Referred Memberships With This Maths Formula\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: On Referred Memberships\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article analyzes the impact of referred memberships on Medium writers\u0027 revenue growth. It suggests that focusing on referred memberships can provide a significant boost to the writer\u0027s reach and conversion rate. The writing strategy involves prioritizing quantity over quality and promoting the membership link heavily. The article also discusses the types of memberships and the need to calculate the monthly and annual memberships ratio to minimize the churn rate. The main dashboard provides crucial information, including the cumulative sum of earnings and the number of members gained and lost. The article also provides an online tool to facilitate the calculation of earnings.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027https://medium.com/illumination\u0027\u003eILLUMINATION\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-06-24 (\u0027morning\u0027, \u0027morning\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 1.2%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 42.5 (24 / 1019)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 6\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 750\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 397 (normal)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 52.9% (397 / 750)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 41.7% (313 / 750)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 27.7% (313 / 750)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 19.2% (144 / 750)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 10.9% (82 / 750)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 27.2% (204 / 750)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e medium writers, referred memberships, writing strategy, reach, external views, conversion, dashboard, types of memberships, earnings, churn rate\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e memberships(32), referred(19), members(12), annual(10), number(8), earnings(7), amount(7), monthly(7), equation(7), writers(5), total(5), gain(5), lose(5), let(5), medium(4), follow(4), two(4), shows(4), type(4), calculate(4), write(3), strategy(3), increases(3), use(3), different(3), article(3), example(3), values(3), see(3), ratio(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e referred memberships(13), annual memberships(6), referred members(4), memberships earnings(3), type memberships(3), monthly annual(3), monthly memberships(3), external views(2), medium com(2), partner program(2), members lose(2), memberships gain(2), different amount(2), memberships monthly(2), annual referred(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e annual referred memberships(2), total amount earnings(2), amount earnings memberships(2), number referred members(2), equation referred total(2), monthly annual memberships(2), annual memberships ratio(2), analyze referred memberships(1), referred memberships maths(1), memberships maths formula(1)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://medium.com/illumination/analyze-your-referred-memberships-with-this-maths-formula-5b5627959830", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Tip 1: Create smaller paragraphs", "domain": "https://medium.com/illumination/become-a-master-writer-with-15-powerful-writing-tips-29bd919dd09d", "font": {"color": "#000000", "size": 20}, "id": 16, "label": "Become a Master Writ", "main": 1, "main_title": "Become a Master Writer With 15 Powerful Writing Tips", "shape": "star", "size": 34.56026058631922, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Become a Master Writer With 15 Powerful Writing Tips\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Tip 1: Create smaller paragraphs\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article discusses tips to upgrade one\u0027s writing style, such as using concise and powerful sentences, active voice, and formatting that is consistent and easy to read. It emphasizes the importance of coherence, clarity, and readability in writing, and suggests avoiding filler words, adverbs, and passive voice. The article also encourages breaking down longer paragraphs into smaller chunks, and using a table of contents to help readers navigate the article. In addition, it addresses the use of phrases, acronyms, and formatting symbols, and provides bonus tips for those interested in freelance writing.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027https://medium.com/illumination\u0027\u003eILLUMINATION\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-06-23 (\u0027evening\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 2.7%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 27.0 (53 / 1430)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 22\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1363\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 713 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 52.3% (713 / 1363)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 38.3% (522 / 1363)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 27.4% (522 / 1363)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 19.3% (263 / 1363)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 12.5% (170 / 1363)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 24.9% (339 / 1363)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e writing tips, concise writing, active voice, formatting, coherence, clarity, readability, phrases, filler words, adverbs\nsummary\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e writing(28), tips(24), use(15), word(12), articles(11), sentences(8), make(8), example(7), would(7), writer(6), phrases(6), tried(6), readers(6), become(5), parts(5), also(5), start(5), terms(5), little(4), paragraphs(4), statement(4), text(4), first(4), drive(4), problem(4), expensive(4), however(4), messages(4), format(4), bold(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e writing tips(4), make writing(4), writing style(3), tips delete(3), use bold(3), become master(2), master writer(2), style also(2), online writing(2), many people(2), paragraphs may(2), tips avoid(2), clutter writing(2), tasks must(2), must done(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e become master writer(2), writing style also(2), tasks must done(2), weather bad yesterday(2), distracted drive significant(2), drive significant problem(2), significant problem today(2), people teens adults(2), teens adults continue(2), adults continue text(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://medium.com/illumination/become-a-master-writer-with-15-powerful-writing-tips-29bd919dd09d", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "It finally arrived!", "domain": "https://towardsdatascience.com/installing-pytorch-on-apple-m1-chip-with-gpu-acceleration-3351dc44d67c?gi=f016057caeba", "font": {"color": "#000000", "size": 20}, "id": 17, "label": "Installing PyTorch o", "main": 1, "main_title": "Installing PyTorch on Apple M1 chip with GPU Acceleration", "shape": "star", "size": 41.074918566775246, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Installing PyTorch on Apple M1 chip with GPU Acceleration\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: It finally arrived!\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article discusses how the new built-in graphics processor in Apple MacBooks enables GPU acceleration for deep learning tasks, and how this has made MacBooks suitable for deep learning. The compatibility of popular frameworks, including TensorFlow and PyTorch, is also discussed. The article provides a step-by-step guide on how to install TensorFlow and PyTorch on Apple\u0027s new silicon devices and highlights the importance of checking for updates regularly. A sanity test is given to check whether PyTorch works properly with Metal Performance Shaders (MPS).\n\nKEYWORDS\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-06-17 (\u0027evening\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 5.3%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 5.4 (103 / 561)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 10\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1000\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 605 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 60.5% (605 / 1000)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 40.2% (402 / 1000)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 27.2% (402 / 1000)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 16.1% (161 / 1000)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 8.5% (85 / 1000)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 42.3% (423 / 1000)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e pytorch, apple silicon, gpu acceleration, deep learning, tensorflow, metal, mps, installation, benchmark, performance\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e pytorch(20), gpu(19), device(18), installation(16), torch(16), apple(11), dtype(11), mps(10), macos(8), using(8), learning(7), follow(7), conda(7), deep(6), compute(6), step(6), check(6), frameworks(5), metal(5), performance(5), commands(5), print(5), learning_rate(5), sum(5), item(5), grad_y_pred(5), acceleration(4), support(4), tensorflow(4), silicon(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e deep learning(6), dtype torch(5), device device(5), device dtype(5), dtype dtype(5), gpu acceleration(4), apple silicon(4), torch randn(4), randn device(4), grad_y_pred sum(4), installation pytorch(3), backends mps(3), installation commands(3), torch gpu(3), conda installation(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e device device dtype(5), device dtype dtype(5), dtype dtype torch(4), torch randn device(4), randn device device(4), dtype torch randn(3), metal performance shaders(2), print torch backends(2), torch backends mps(2), torch device mps(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/installing-pytorch-on-apple-m1-chip-with-gpu-acceleration-3351dc44d67c?gi=f016057caeba", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Leverage Bayesian Theory to boost your performance", "domain": "https://towardsdatascience.com/tune-deep-neural-networks-using-bayesian-optimization-c9f6503a049f?gi=6cd4c4eb6dc6", "font": {"color": "#000000", "size": 20}, "id": 18, "label": "Tune Deep Neural Net", "main": 1, "main_title": "Tune Deep Neural Networks using Bayesian Optimization", "shape": "star", "size": 32.08469055374593, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Tune Deep Neural Networks using Bayesian Optimization\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Leverage Bayesian Theory to boost your performance\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e This article discusses the use of Bayesian optimization to tune the hyperparameters of deep neural networks for image classification tasks using the fashion MNIST dataset and TensorFlow. The article covers the importance of hyperparameter tuning, introduces the Keras Tuner library, and demonstrates the use of the Bayesian Optimization Tuner for tuning the hyperparameters of both a multilayered perceptron and a convolutional neural network. The article concludes with a comparison of the accuracy of the baseline models and the optimized models, and an overview of the various types of Keras Tuner available for optimizing deep neural networks.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-06-08 (\u0027afternoon\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 1.8%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 20.9 (34 / 711)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 5\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1195\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 691 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 57.8% (691 / 1195)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 37.7% (451 / 1195)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 26.8% (451 / 1195)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 15.3% (183 / 1195)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 10.0% (120 / 1195)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 32.5% (388 / 1195)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e tune, deep neural networks, bayesian optimization, performance, image classification, tensorflow, machine learning, hyperparameter optimization, fashion mnist dataset, keras tuner\nsummary\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e model(21), hyperparameters(21), tuner(19), optimal(15), layers(13), test(11), number(11), tuning(10), networks(10), train(10), set(10), uses(9), best(9), cnn(8), accuracy(8), keras(7), baseline(7), neural(6), learning(6), bayesian(5), previous(5), article(5), datasets(5), value(5), size(5), mlp(5), dense(5), dropout(5), follow(5), deep(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e neural networks(6), keras tuner(6), test accuracy(6), hyperparameters tuning(4), cnn model(4), deep neural(3), bayesian optimal(3), test set(3), previous article(3), number hidden(3), optimal hyperparameters(3), learning rates(3), number iterations(3), mlp model(3), dense layers(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e deep neural networks(3), machine learning project(2), fashion mnist datasets(2), number hidden units(2), search space hyperparameters(2), multilayered perceptron mlp(2), convolutional neural networks(2), neural networks cnn(2), baseline mlp model(2), dense layers size(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/tune-deep-neural-networks-using-bayesian-optimization-c9f6503a049f?gi=6cd4c4eb6dc6", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "The Original Web 3", "domain": "https://medium.com/geekculture/web3-was-proposed-in-2001-by-the-internet-inventor-and-failed-a8faab035fe0", "font": {"color": "#000000", "size": 20}, "id": 19, "label": "Web3 was Proposed in", "main": 1, "main_title": "Web3 was Proposed in 2001, by the Internet Inventor. And Failed.", "shape": "star", "size": 31.693811074918568, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Web3 was Proposed in 2001, by the Internet Inventor. And Failed.\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: The Original Web 3\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The text discusses the failed idea of the Semantic Web proposed by the inventor of the World Wide Web, Sir Tim Berners-Lee in 2001, and how it differs from the current proposed concept of Web3, which includes blockchain, NFTs, and decentralized apps that could create a metaverse. The article reviews how the early web was different and focuses on the concept of the Semantic Web, the challenges faced with creating such a web due to its heterogeneity in data, and how a new framework was proposed using ontology, RDF schema, and queries. However, this idea did not gain traction due to excessive hype and the practical issues of tagging thousands of entities, ontology relationships, and lack of simplicity. The article highlights that the proposed features of Web3, such as a metaverse, decentralized apps, and NFTs, currently do not exist, and the hype surrounding them creates unrealistic expectations that could lead to reckless decisions in investing. The article concludes that the underlying technology of Web3 is brilliant, but its purpose and potential applications should not be lost in the hype that surrounds it.\n\nKEYWORDS\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027https://medium.com/geekculture\u0027\u003eGeek Culture\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-05-31 (\u0027afternoon\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 1.6%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 33.1 (31 / 1025)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 8\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1538\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 853 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 55.5% (853 / 1538)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 41.2% (634 / 1538)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 30.0% (634 / 1538)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 16.7% (257 / 1538)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 11.8% (182 / 1538)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 28.7% (442 / 1538)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e web3, semantic web, sir tim berners-lee, ontology, rdf schema, heterogeneity, metaverse, decentralized apps, nfts, hype\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e web(44), semantic(27), new(12), ontology(10), use(7), data(7), pages(7), proposed(6), app(6), every(6), hype(6), idea(6), understand(6), issue(6), languages(6), also(6), technology(6), metaverse(5), two(5), tim(5), back(5), creates(5), provides(5), many(5), people(5), web3(4), inventor(4), fail(4), different(4), decentralized(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e semantic web(18), tim berners(4), berners lee(4), new web(4), decentralized app(3), web first(3), inventor world(3), world wide(3), wide web(3), web pages(3), two decades(2), decades ago(2), lee inventor(2), different back(2), original web(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e tim berners lee(4), inventor world wide(3), world wide web(3), two decades ago(2), berners lee inventor(2), lee inventor world(2), semantic web mission(2), content web pages(2), bottom line semantic(2), web pages html(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://medium.com/geekculture/web3-was-proposed-in-2001-by-the-internet-inventor-and-failed-a8faab035fe0", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "An end-to-end tutorial - from data preparation to model training and evaluation", "domain": "https://towardsdatascience.com/create-image-classification-models-with-tensorflow-in-10-minutes-d0caef7ca011?gi=bcd3222b51fc", "font": {"color": "#000000", "size": 20}, "id": 20, "label": "Create Image Classif", "main": 1, "main_title": "Create Image Classification Models with TensorFlow in 10 Minutes", "shape": "star", "size": 31.042345276872965, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Create Image Classification Models with TensorFlow in 10 Minutes\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: An end-to-end tutorial - from data preparation to model training and evaluation\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e This article presents a tutorial on creating image classification models using TensorFlow. The author explains the data preparation, model training, and evaluation using the Fashion MNIST dataset. Two types of neural networks are used for training - MLP and CNN. Overfitting is discussed, and earlystopping is used to prevent it. Finally, the accuracy results of the MLP and CNN models are compared, with the CNN model outperforming the MLP model.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-04-29 (\u0027night\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 1.3%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 22.4 (26 / 583)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 1\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 2260\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 1399 (large)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 61.9% (1399 / 2260)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 31.9% (722 / 2260)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 23.4% (722 / 2260)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 13.7% (309 / 2260)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 8.8% (198 / 2260)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 35.8% (809 / 2260)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e tensorflow, image classification, model training, evaluation, machine learning, fashion mnist dataset, mlp, cnn, overfitting, earlystopping\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e accuracy(35), train(29), epochs(28), loss(26), models(21), 1782(21), plt(18), used(17), images(16), network(16), 100(16), validation(15), layer(15), metric(15), datasets(14), print(14), test(12), neural(12), model_cnn(12), function(11), activation(11), val_loss(11), step(11), cnn(10), model_mlp(10), 5ms(10), val_accuracy(10), label(9), train_y(9), let(9)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e epochs 100(13), neural network(12), 1782 1782(10), 5ms step(10), step loss(10), 100 1782(9), 1782 5ms(9), train validation(7), verbose print(6), validation test(5), epochs metric(5), test accuracy(5), model_cnn add(5), target values(4), hidden layer(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e 5ms step loss(10), epochs 100 1782(9), 100 1782 1782(9), 1782 1782 5ms(9), 1782 5ms step(9), train validation test(5), convolutional neural network(3), neural network cnn(3), fashion mnist datasets(2), belongs one categories(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/create-image-classification-models-with-tensorflow-in-10-minutes-d0caef7ca011?gi=bcd3222b51fc", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Requirements", "domain": "https://pub.towardsai.net/installing-tensorflow-on-mac-m1-pro-m1-max-2af765243eaa?gi=8c2a029c7e0c", "font": {"color": "#000000", "size": 20}, "id": 21, "label": "Installing Tensorflo", "main": 1, "main_title": "Installing Tensorflow on Mac M1 Pro \u0026 M1 Max", "shape": "star", "size": 31.172638436482085, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Installing Tensorflow on Mac M1 Pro \u0026 M1 Max\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Requirements\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article provides a detailed installation guide for running TensorFlow on a Mac, particularly on the Apple Silicon chip that revolutionized the field of deep learning. The silicon chip includes a built-in graphics processor that enables GPU acceleration, making Apple computers suitable for deep learning tasks. The article describes how to install TensorFlow on macOS using Anaconda or Miniforge, including setup of a Conda environment, Python installation, and Jupyter Notebook. The article also mentions some common issues that may arise during installation and offers some workarounds. Finally, a simple example is provided to check whether everything is working correctly.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027pub.towardsai.net\u0027\u003eTowards AI\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-04-03 (\u0027afternoon\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 1.4%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 22.2 (27 / 600)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 5\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 972\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 561 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 57.7% (561 / 972)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 42.3% (411 / 972)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 31.3% (411 / 972)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 16.6% (161 / 972)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 11.1% (108 / 972)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 33.6% (327 / 972)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e tensorflow, apple, deep learning, gpu acceleration, mac pro max, installation guide, silicon chip, anaconda, miniforge, macos, jupyter notebook\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e install(26), conda(19), tensorflow(16), apple(11), miniforge(9), macos(8), deep(7), learning(7), using(7), keras(7), gpu(5), python(5), environment(5), importing(5), works(4), making(4), articles(4), xcode(4), deps(4), jupyter(4), forge(4), mnist(4), x_train(4), model(4), layers(4), numpy(4), mac(3), pro(3), max(3), chips(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e install tensorflow(8), deep learning(7), conda install(5), conda environment(4), tensorflow deps(4), conda forge(4), keras layers(4), tensorflow metal(3), apple silicon(3), pip install(3), pro max(2), gpu acceleration(2), macos fan(2), learning without(2), guide install(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e pip install tensorflow(3), install tensorflow deps(3), deep learning without(2), using least python(2), install tensorflow macos(2), conda install conda(2), install conda forge(2), keras layers dense(2), install tensorflow mac(1), tensorflow mac pro(1)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://pub.towardsai.net/installing-tensorflow-on-mac-m1-pro-m1-max-2af765243eaa?gi=8c2a029c7e0c", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "If they are dead, why do they still win Kaggle Competitions?", "domain": "https://towardsdatascience.com/deep-learning-no-lstms-are-not-dead-20217553b87a?gi=b7f5cb878072", "font": {"color": "#000000", "size": 20}, "id": 22, "label": "Time-Series Forecast", "main": 1, "main_title": "Time-Series Forecasting: No, LSTMs Are Not Dead!", "shape": "star", "size": 37.94788273615635, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Time-Series Forecasting: No, LSTMs Are Not Dead!\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: If they are dead, why do they still win Kaggle Competitions?\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article discusses the relevance and usefulness of LSTM networks and transformers in time series forecasting tasks. While statistical methods still win the first round, deep learning models are becoming increasingly competitive in this area. The article also highlights the advantages and disadvantages of using LSTM networks and introduces other novel models such as Temporal Convolutional Networks (TCNs) and Vision Transformers (ViTs). The author provides a comprehensive overview of the field\u0027s latest developments, including notable research papers and competitive benchmarks such as Kaggle competitions, showing the state-of-the-art models used in winning solutions.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-03-22 (\u0027afternoon\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 4.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 11.5 (79 / 906)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 7\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 3066\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 1725 (large)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 56.3% (1725 / 3066)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 30.2% (926 / 3066)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 21.6% (926 / 3066)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 16.2% (497 / 3066)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 12.7% (390 / 3066)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 30.0% (920 / 3066)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e time series forecasting, lstms, transformers, machine learning, deep learning, statistical methods, nlp, attention, tcns, computer vision\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e modeling(49), lstms(43), time(27), learning(25), useful(23), transformers(22), sequence(20), series(19), networks(17), forecasting(16), deep(16), words(15), attention(14), tcn(14), also(13), data(13), layers(12), recurrent(11), machine(10), nlp(10), different(10), however(10), statistical(10), methods(10), tft(10), convolutional(10), paper(9), one(9), article(9), components(9)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e time series(19), deep learning(12), series forecasting(8), statistical methods(7), machine learning(5), neural networks(5), recurrent networks(5), machine translation(5), shown figure(5), data science(4), encoder decoder(4), attention mechanical(4), learning modeling(4), breakthrough field(3), state art(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e time series forecasting(8), deep learning modeling(4), temporal fusion transformers(3), time series modeling(3), family recurrent neural(2), recurrent neural networks(2), every big tech(2), big tech company(2), tech company embraced(2), long short term(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/deep-learning-no-lstms-are-not-dead-20217553b87a?gi=b7f5cb878072", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Why plagiarism is a serious issue?", "domain": "https://medium.com/illumination/this-amazing-medium-feature-can-detect-plagiarism-and-more-97f9fd3bbdf6", "font": {"color": "#000000", "size": 20}, "id": 23, "label": "This Amazing Medium ", "main": 1, "main_title": "This Amazing Medium Feature Can Detect Plagiarism - And More!", "shape": "star", "size": 31.563517915309447, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: This Amazing Medium Feature Can Detect Plagiarism - And More!\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Why plagiarism is a serious issue?\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The text discusses the issue of plagiarism in articles and how the Medium platform has a feature that detects plagiarism. The author explains that plagiarism is a critical issue that can harm user engagement, content exposure, and growth. The text explores how the detection feature works and how it helps locate different sources that drive traffic to articles. The author emphasizes the usefulness of the feature in detecting plagiarism and tracking social media channels\u0027 traffic. In conclusion, the author encourages the use of the feature to protect valuable work and boost content growth.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027https://medium.com/illumination\u0027\u003eILLUMINATION\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-02-17 (\u0027night\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 1.5%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 30.1 (30 / 903)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 9\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1323\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 669 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 50.6% (669 / 1323)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 36.4% (482 / 1323)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 25.8% (482 / 1323)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 19.2% (254 / 1323)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 9.8% (129 / 1323)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 24.7% (327 / 1323)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e medium, plagiarism, detection, social media, traffic, growth, statistics, article, title, sources\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e articles(35), plagiarism(20), use(10), first(9), post(9), feature(8), shared(8), traffic(8), linkedin(8), platform(8), cases(7), social(7), would(7), happened(6), media(6), work(6), views(5), also(5), fake(5), plagiarists(5), title(5), search(5), results(5), trying(5), different(5), medium(4), detect(4), one(4), content(4), issue(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e social media(6), plagiarism first(4), fake articles(4), feature detect(2), detect plagiarism(2), plagiarism one(2), happened cases(2), thousand views(2), shared linkedin(2), articles plagiarism(2), first place(2), original articles(2), plagiarism articles(2), keep mind(2), work articles(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e use title articles(2), fake articles would(2), social media platform(2), amazing medium feature(1), medium feature detect(1), feature detect plagiarism(1), detect plagiarism one(1), plagiarism one seems(1), one seems know(1), seems know everyone(1)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://medium.com/illumination/this-amazing-medium-feature-can-detect-plagiarism-and-more-97f9fd3bbdf6", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Introduction", "domain": "https://bettermarketing.pub/always-look-out-for-plagiarism-it-happened-to-my-100k-views-article-adce1bf6a05b?gi=b0ffa7646633", "font": {"color": "#000000", "size": 20}, "id": 24, "label": "My 100k Views Articl", "main": 1, "main_title": "My 100k Views Article Was Plagiarized. Here\u0027s What I Did", "shape": "star", "size": 35.472312703583064, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: My 100k Views Article Was Plagiarized. Here\u0027s What I Did\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Introduction\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The text discusses the issue of plagiarism in content creation and how it can affect creators. The author shares a personal experience of having their article plagiarized and the difficulty of contacting the plagiarist. The text also explores the concept of plagiarism in academic writing and how it differs from creative writing. The author provides an example of detecting plagiarism through technical knowledge and sharing knowledge as a way to prevent plagiarism. The text concludes with a reminder of the negative consequences of plagiarism, including loss of web traffic, views, user engagement, and kudos, as well as potential legal consequences.\n\nKEYWORDS\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027bettermarketing.pub\u0027\u003eBetter Marketing\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-02-01 (\u0027night\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 3.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 21.7 (60 / 1301)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 13\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1490\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 767 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 51.5% (767 / 1490)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 37.7% (561 / 1490)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 27.4% (561 / 1490)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 19.7% (294 / 1490)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 11.1% (166 / 1490)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 22.7% (338 / 1490)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e plagiarism, content creation, academic writing, creative writing, detecting plagiarism, technical knowledge, consequences of plagiarism, loss of web traffic, legal consequences, personal story\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e article(30), plagiarism(26), one(14), original(10), could(10), paraphrased(9), programming(9), first(8), writing(8), algorithms(7), projects(7), dynamic(7), use(6), different(6), technical(6), content(5), later(5), result(5), authors(5), version(5), plagiarist(5), fake(5), even(5), better(5), data(5), detect(5), inspire(4), instead(4), tried(4), professor(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e plagiarism article(5), dynamic programming(5), first article(4), original article(3), even better(3), data structures(3), plagiarism paraphrased(2), article one(2), someone else(2), original one(2), technical knowledge(2), tried contact(2), version article(2), could made(2), creative writing(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e quite similar dynamic(2), similar dynamic programming(2), cheap imitation original(2), teach others learned(2), others learned even(2), learned even better(2), even better inspire(2), better inspire improve(2), inspire improve upon(2), improve upon work(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://bettermarketing.pub/always-look-out-for-plagiarism-it-happened-to-my-100k-views-article-adce1bf6a05b?gi=b0ffa7646633", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Quality of Content", "domain": "https://medium.com/new-writers-welcome/how-i-became-eligible-for-mediums-partner-program-in-1-5-months-9d23174d1719", "font": {"color": "#000000", "size": 20}, "id": 25, "label": "How I Reached 100 Fo", "main": 1, "main_title": "How I Reached 100 Followers the Right Way", "shape": "star", "size": 32.47557003257329, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: How I Reached 100 Followers the Right Way\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Quality of Content\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The author has been successful in achieving the threshold of 100 followers on Medium by publishing high-quality articles on data science and software engineering. They share their strategy, including engagement with users via feedback and promoting content on social media, and emphasize the importance of producing original and engaging content. They also discuss the role of platforms such as Quora in attracting readers and building trust with potential followers.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027https://medium.com/new-writers-welcome\u0027\u003eNew Writers Welcome\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2021-11-15 (\u0027evening\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 1.9%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 28.8 (37 / 1066)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 8\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1155\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 595 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 51.5% (595 / 1155)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 42.0% (485 / 1155)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 30.3% (485 / 1155)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 19.1% (221 / 1155)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 9.4% (109 / 1155)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 26.6% (307 / 1155)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e data science, software engineering, medium partner program, claps, follows, engagement, feedback, quality articles, social media promotion, quora\nsummary\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e articles(20), followers(17), medium(10), also(8), start(7), clap(7), published(6), first(6), day(6), content(6), 100(5), one(5), platform(5), time(5), data(5), would(5), comment(5), feedback(5), users(5), decided(4), become(4), partner(4), hours(4), already(4), could(4), least(4), new(4), success(4), well(4), cases(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e 100 followers(5), first articles(4), partner program(3), data science(3), clap followers(3), articles followers(3), decided become(2), medium partner(2), threshold 100(2), also mentioned(2), hours reading(2), reading time(2), day published(2), success also(2), well written(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e medium partner program(2), threshold 100 followers(2), hours reading time(2), consequence start formulating(2), start formulating plan(2), formulating plan convert(2), plan convert clap(2), convert clap followers(2), reached 100 followers(1), 100 followers right(1)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://medium.com/new-writers-welcome/how-i-became-eligible-for-mediums-partner-program-in-1-5-months-9d23174d1719", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Including M1 Macbook, and some tips for a smoother installation", "domain": "https://betterprogramming.pub/how-to-install-pytorch-on-apple-m1-series-512b3ad9bc6?gi=e82978d3ac5b", "font": {"color": "#000000", "size": 20}, "id": 26, "label": "How to Install PyTor", "main": 1, "main_title": "How to Install PyTorch on Apple M1-series", "shape": "star", "size": 33.77850162866449, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: How to Install PyTorch on Apple M1-series\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Including M1 Macbook, and some tips for a smoother installation\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e Apple\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027betterprogramming.pub\u0027\u003eBetter Programming\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2021-11-07 (\u0027evening\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 2.4%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 8.8 (47 / 413)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 1\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 866\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 495 (normal)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 57.2% (495 / 866)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 41.0% (355 / 866)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 28.3% (355 / 866)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 15.7% (136 / 866)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 8.3% (72 / 866)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 38.7% (335 / 866)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e pytorch, apple, installation, compatibility, macos, data science, tensorflow, metal, conda, arm64\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e installing(22), conda(20), device(13), apple(12), pytorch(11), dtype(11), torch(9), new(7), tensorflow(6), miniforge(6), create(6), would(5), learning_rate(5), sum(5), item(5), grad_y_pred(5), macbooks(4), data(4), step(4), xcode(4), used(4), arm64(4), environment(4), pytorch_m1(4), activity(4), randn(4), computers(4), issue(4), series(3), including(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e conda installing(5), dtype torch(5), device device(5), device dtype(5), dtype dtype(5), installing pytorch(4), conda environment(4), torch randn(4), randn device(4), grad_y_pred sum(4), create new(3), new conda(3), conda forge(3), item item(3), series including(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e device device dtype(5), device dtype dtype(5), dtype dtype torch(4), torch randn device(4), randn device device(4), create new conda(3), new conda environment(3), dtype torch randn(3), conda create name(2), create name pytorch_m1(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://betterprogramming.pub/how-to-install-pytorch-on-apple-m1-series-512b3ad9bc6?gi=e82978d3ac5b", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "A practical use case for data scientists who use MiniForge", "domain": "https://betterprogramming.pub/switching-between-multiple-conda-distributions-on-macos-b78b6b21720?gi=09e824e6a2d9", "font": {"color": "#000000", "size": 20}, "id": 27, "label": "Switching Between Mu", "main": 1, "main_title": "Switching Between Multiple Conda Distributions on macOS", "shape": "star", "size": 30.0, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Switching Between Multiple Conda Distributions on macOS\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: A practical use case for data scientists who use MiniForge\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The use of Miniforge, a Conda distribution, has increased in popularity among MacOS developers, particularly data scientists, due to its compatibility with Apple devices. However, a small fraction of libraries may not be compatible with it. The solution is to use both Miniforge and Anaconda for their respective strengths. A script can be used to switch between the two on the same Conda environment. The script also works on Ubuntu, but modifications are necessary for MacOS versions that use bash instead of zsh.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027betterprogramming.pub\u0027\u003eBetter Programming\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2021-11-01 (\u0027evening\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.9%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 22.7 (18 / 409)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 1\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 773\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 460 (normal)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 59.5% (460 / 773)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 38.7% (299 / 773)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 26.4% (299 / 773)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 17.2% (133 / 773)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 9.4% (73 / 773)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 34.9% (270 / 773)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e conda distribution, macos, data science, miniforge, anaconda, gpu, libraries, compatibility, bash, script\n\nsummary\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e conda(25), miniforge(19), apple(9), users(8), johndoe(8), opt(8), distribution(6), use(6), initialize(6), zshrc(6), __conda_setup(6), anaconda3(6), replace(6), source(5), would(5), bin(5), executed(5), search(5), macos(4), started(4), could(4), installing(4), anaconda(4), take(4), environments(4), ubuntu(4), else(4), etc(4), profile(4), path(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e users johndoe(8), johndoe opt(8), conda distribution(6), conda initialize(4), opt miniforge(4), etc profile(4), profile conda(4), opt anaconda3(4), conda environments(3), zshrc file(3), look like(3), conda env(3), env list(3), data scientists(2), apple series(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e users johndoe opt(8), johndoe opt miniforge(4), etc profile conda(4), johndoe opt anaconda3(4), conda env list(3), trick works ubuntu(2), works ubuntu well(2), look like conda(2), like conda initialize(2), conda initialize contents(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://betterprogramming.pub/switching-between-multiple-conda-distributions-on-macos-b78b6b21720?gi=09e824e6a2d9", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Demystifying the Linear Algebra concepts behind SVD with a simple example", "domain": "https://towardsdatascience.com/how-to-use-singular-value-decomposition-svd-for-image-classification-in-python-20b1b2ac4990?gi=273bafe71044", "font": {"color": "#000000", "size": 20}, "id": 28, "label": "How to Use Singular ", "main": 1, "main_title": "How to Use Singular Value Decomposition (SVD) for Image Classification in Python", "shape": "star", "size": 30.39087947882736, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: How to Use Singular Value Decomposition (SVD) for Image Classification in Python\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Demystifying the Linear Algebra concepts behind SVD with a simple example\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e This article aims to illustrate the usefulness of singular value decomposition (SVD) in image classification, and provides a simple example in Python. The article explains the fundamental mathematical concepts behind SVD and shows its practical applications in dimensionality reduction. The article uses the USPS postal service dataset to demonstrate how SVD can be applied to handwritten digit classification.\n\nKEYWORDS\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2021-10-26 (\u0027evening\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 1.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 25.3 (21 / 531)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 0\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1874\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 993 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 53.0% (993 / 1874)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 29.8% (559 / 1874)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 20.9% (559 / 1874)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 13.4% (251 / 1874)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 13.2% (247 / 1874)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 33.2% (623 / 1874)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e singular value decomposition, svd, image classification, python, linear algebra, matrix factorization, latent dirichlet allocation, pca, eckart-young-mirsky theorem, low rank matrix, noise, dimensionality reduction\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e digits(28), matrix(27), vectors(22), singular(18), svd(17), values(16), plt(15), predictions(14), matrices(12), first(12), columns(10), test(10), shape(10), figure(10), score(10), image(9), data(9), y_test(9), 256(9), using(8), example(8), ones(8), basis(8), training(8), important(8), str(8), rank(7), dataframe(7), x_test(7), print(7)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e singular values(10), singular vectors(8), left singular(5), basis vectors(5), y_test loc(5), handwritten digits(4), loc predictions(4), linear algebra(3), digits classification(3), applying svd(3), training get(3), test get(3), shape print(3), x_test shape(3), figure display(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e left singular vectors(5), y_test loc predictions(4), reshape cmap binary(3), first left singular(3), plt figure figsize(3), number basis vectors(3), singular values decomposition(2), values decomposition svd(2), matrix written sum(2), written sum rank(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/how-to-use-singular-value-decomposition-svd-for-image-classification-in-python-20b1b2ac4990?gi=273bafe71044", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "How to enable GPU acceleration on Mac M1 and achieve a smooth installation", "domain": "https://betterprogramming.pub/installing-tensorflow-on-apple-m1-with-new-metal-plugin-6d3cb9cb00ca?gi=dee324c81917", "font": {"color": "#000000", "size": 20}, "id": 29, "label": "Installing Tensorflo", "main": 1, "main_title": "Installing Tensorflow on Apple M1 With the New Metal Plugin", "shape": "star", "size": 38.990228013029316, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Installing Tensorflow on Apple M1 With the New Metal Plugin\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: How to enable GPU acceleration on Mac M1 and achieve a smooth installation\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e This article discusses how to install Tensorflow with new Metal plugin on a Mac to achieve smooth installation and enable GPU acceleration. Apple\u0027s abandonment of Nvidia support has caused issues for users, but the recent advent of new chips that use the Apple Neural Engine component has allowed Macs to perform machine learning tasks quickly without thermal issues. The article outlines steps to install Miniforge using the Metal plugin, which is straightforward and less prone to errors. The process involves installing Xcode and Command Line Tools, downloading and installing Miniforge, and setting up the environment to install Tensorflow with the Metal plugin. The article provides bonus tips and addresses common errors that may occur during the installation process.\n\nKEYWORDS\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027betterprogramming.pub\u0027\u003eBetter Programming\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2021-10-07 (\u0027morning\u0027, \u0027morning\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 4.5%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 11.2 (87 / 978)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 12\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 885\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 542 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 61.2% (542 / 885)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 42.8% (379 / 885)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 31.4% (379 / 885)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 17.3% (153 / 885)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 11.8% (104 / 885)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 34.1% (302 / 885)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e tensorflow, metal plugin, gpu, mac, installation, neural engine, miniforge, xcode, environment, errors\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e install(30), tensorflow(23), conda(19), apple(9), use(9), environment(7), error(7), importance(7), keras(7), version(7), metal(6), package(6), miniforge(6), python(6), pip(6), model(6), download(5), forge(5), tensorflow_m1(5), deps(5), new(4), mac(4), dependencies(4), macos(4), mnist(4), x_train(4), layers(4), numpy(4), plugin(3), gpu(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e install tensorflow(11), pip install(6), conda install(5), conda forge(5), tensorflow deps(5), tensorflow metal(4), tensorflow macos(4), keras layers(4), metal plugin(3), install apple(3), make sure(3), step install(2), install xcode(2), forge channel(2), conda distributions(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e pip install tensorflow(6), install tensorflow macos(4), conda forge channel(2), conda create name(2), create name tensorflow_m1(2), name tensorflow_m1 python(2), tensorflow_m1 python conda(2), python conda activate(2), conda activate tensorflow_m1(2), conda install apple(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://betterprogramming.pub/installing-tensorflow-on-apple-m1-with-new-metal-plugin-6d3cb9cb00ca?gi=dee324c81917", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Fast and scalable time series classification by combining Dynamic Time Warping (DTW) and k-nearest neighbor (KNN)", "domain": "https://towardsdatascience.com/time-series-classification-using-dynamic-time-warping-61dcd9e143f6?gi=51e6fa3dca9b", "font": {"color": "#000000", "size": 20}, "id": 30, "label": "Time series classifi", "main": 1, "main_title": "Time series classification using Dynamic Time Warping", "shape": "star", "size": 40.814332247557005, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Time series classification using Dynamic Time Warping\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Fast and scalable time series classification by combining Dynamic Time Warping (DTW) and k-nearest neighbor (KNN)\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e This article discusses time series classification using dynamic time warping (DTW) and nearest neighbors (KNN). DTW is a popular approach to tackle this common task in many applications in numerous domains like IoT, signal processing, and human activity recognition. The article provides a starter example using the dtaidistance library in Python to compute the distance between phase-shifted sine waves using DTW. It also demonstrates how to apply DTW and KNN for time series classification using the popular UCI HAR human activity recognition dataset. It explores the time complexity of DTW and provides tips to optimize the algorithm. Finally, the article discusses other classification algorithms in the time series domain, such as the random forest classifier.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2021-09-14 (\u0027morning\u0027, \u0027morning\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 5.2%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 13.0 (101 / 1308)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 0\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1706\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 1019 (large)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 59.7% (1019 / 1706)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 34.9% (595 / 1706)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 25.6% (595 / 1706)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 16.1% (274 / 1706)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 11.9% (203 / 1706)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 35.5% (606 / 1706)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e time series classification, dynamic time warping, nearest neighbor, dtw, knn, signal processing, human activity recognition, python, uci har dataset, time complexity\n\nsummary\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e time(29), dtw(24), distance(22), idx(20), series(15), labels(15), used(13), dataset(13), x_test(13), algorithms(12), plt(12), y_test(11), x_train(10), dynamic(8), examples(8), paths(8), colors(8), knn(7), train(7), sequences(7), matrix(7), counters(7), classification(6), warping(6), class(6), also(6), one(6), walking(6), y_train(6), test(6)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e time series(14), y_test idx(6), dynamic programming(5), series classification(4), time warping(4), time complexity(4), dtw distance(4), use_pruning true(4), open uci(4), uci har(4), har dataset(4), x_test idx(4), dynamic time(3), distance dtw(3), window use_pruning(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e time series classification(4), open uci har(4), uci har dataset(4), dynamic time warping(3), distance dtw distance(3), window use_pruning true(3), sitting standing laying(3), series test set(3), plt plot x_test(3), plot x_test idx(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/time-series-classification-using-dynamic-time-warping-61dcd9e143f6?gi=51e6fa3dca9b", "urls": []}, {"counter": 3, "description": "Deep**A**R", "domain": "medium.com", "id": 100001, "label": "medium.com|3", "main": 0, "shape": "dot", "size": 14, "url": "medium.com", "urls": ["DeepAR|https://medium.com/p/bc717771ce85", "newsletter|https://medium.com/subscribe/@nikoskafritsas", "Deep**A**R|https://medium.com/towa**r**ds-data-science/deepa**r**-maste**r**ing-time-se**r**ies-fo**r**ecasting-with-deep-lea**r**ning-bc717771ce85"]}, {"counter": 3, "description": "**Figure 1:** Two training steps of Deep GPVAR. The covariance matrix \u03a3 is expressed as \u03a3=D+V*V^T ([Source", "domain": "arxiv.org", "id": 100002, "label": "arxiv.org|3", "main": 0, "shape": "dot", "size": 14, "url": "arxiv.org", "urls": ["**Figure 1:** Two training steps of Deep GPVAR. The covariance matrix \u03a3 is expressed as \u03a3=D+V*V^T ([Source|https://arxiv.org/abs/1910.03002", "_DeepAR: Probabilistic forecasting with autoregressive recurrent networks_|https://arxiv.org/pdf/1704.04110.pdf", "High-Dimensional Multivariate Forecasting with Low-Rank Gaussian Copula Processes|https://arxiv.org/abs/1910.03002"]}, {"counter": 1, "description": "Their misuse also played a significant role in the 2008 recession.", "domain": "samueldwatts.com", "id": 100003, "label": "samueldwatts.com", "main": 0, "shape": "dot", "size": 10, "url": "samueldwatts.com", "urls": ["Their misuse also played a significant role in the 2008 recession.|http://samueldwatts.com/wp-content/uploads/2016/08/Watts-Gaussian-Copula_Financial_Crisis.pdf"]}, {"counter": 1, "description": "_inverse sampling_", "domain": "en.wikipedia.org", "id": 100004, "label": "en.wikipedia.org", "main": 0, "shape": "dot", "size": 10, "url": "en.wikipedia.org", "urls": ["_inverse sampling_|https://en.wikipedia.org/wiki/Inverse_transform_sampling"]}, {"counter": 1, "description": "Gluon TS", "domain": "ts.gluon.ai", "id": 100005, "label": "ts.gluon.ai", "main": 0, "shape": "dot", "size": 10, "url": "ts.gluon.ai", "urls": ["Gluon TS|https://ts.gluon.ai/stable/getting_started/models.html"]}, {"counter": 1, "description": "here", "domain": "drive.google.com", "id": 100006, "label": "drive.google.com", "main": 0, "shape": "dot", "size": 10, "url": "drive.google.com", "urls": ["here|https://drive.google.com/file/d/1sVXcUQi0FXVZxgmiHX0oc5YZfrjwZzmA/view?usp=share_link"]}, {"counter": 1, "description": "Gluon TS library", "domain": "github.com", "id": 100007, "label": "github.com", "main": 0, "shape": "dot", "size": 10, "url": "github.com", "urls": ["Gluon TS library|https://github.com/awslabs/gluonts/tree/dev/src/gluonts/mx/model"]}, {"counter": 1, "description": "NormalDistributionLoss", "domain": "pytorch-forecasting.readthedocs.io", "id": 100008, "label": "pytorch-forecasting.readthedocs.io", "main": 0, "shape": "dot", "size": 10, "url": "pytorch-forecasting.readthedocs.io", "urls": ["NormalDistributionLoss|https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.metrics.distributions.NormalDistributionLoss.html"]}, {"counter": 2, "description": "here", "domain": "jovian.com", "id": 100009, "label": "jovian.com|2", "main": 0, "shape": "dot", "size": 12, "url": "jovian.com", "urls": ["here|https://jovian.com/nkafr/deepvar"]}, {"counter": 1, "description": "Linkedin", "domain": "www.linkedin.com", "id": 100010, "label": "linkedin.com", "main": 0, "shape": "dot", "size": 10, "url": "www.linkedin.com", "urls": ["Linkedin|https://www.linkedin.com/in/nikos-kafritsas-b3699180/"]}, {"counter": 1, "description": "ElectricityLoadDiagrams20112014", "domain": "archive.ics.uci.edu", "id": 100011, "label": "archive.ics.uci.edu", "main": 0, "shape": "dot", "size": 10, "url": "archive.ics.uci.edu", "urls": ["ElectricityLoadDiagrams20112014|https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014"]}, {"counter": 2, "description": "urated collection of the best Deep Learning models and tutorials", "domain": "medium.com", "id": 100012, "label": "medium.com|2", "main": 0, "shape": "dot", "size": 12, "url": "medium.com", "urls": ["newsletter|https://medium.com/subscribe/@nikoskafritsas", "urated collection of the best Deep Learning models and tutorials|https://medium.com/@nikoskafritsas/list/timeseries-deep-learning-ultimate-collection-3955c636a768"]}, {"counter": 1, "description": "**S**ervice**N**ow", "domain": "en.wikipedia.org", "id": 100013, "label": "en.wikipedia.org", "main": 0, "shape": "dot", "size": 10, "url": "en.wikipedia.org", "urls": ["**S**ervice**N**ow|https://en.wikipedia.org/wiki/**S**ervice**N**ow"]}, {"counter": 11, "description": "**Figure 1:**The top-level architecture of N-BEATS ([Source", "domain": "arxiv.org", "id": 100014, "label": "arxiv.org|11", "main": 0, "shape": "dot", "size": 30, "url": "arxiv.org", "urls": ["**Figure 2:**The basic Block architecture ([Source|https://arxiv.org/pdf/1905.10437.pdf", "_Learning Transferable Visual Models From Natural Language Supervision_|https://arxiv.org/pdf/2103.00020.pdf", "**Figure 9:** Meta-learning procedures in N-BEATS ([Source|https://arxiv.org/pdf/1905.10437.pdf", "**Figure 10:** Comparison of zero-shot models ([Source)|https://arxiv.org/pdf/2002.02887.pdf", "**Figure 4:**Stack of blocks (left) and stack of stacks (right) - ([Source|https://arxiv.org/pdf/1905.10437.pdf", "**Figure 1:**The top-level architecture of N-BEATS ([Source|https://arxiv.org/pdf/1905.10437.pdf", "**Figure 8:**Experimental results on M3, M4, and Tourism datasets ([Source|https://arxiv.org/pdf/1905.10437.pdf", "_Meta-learning framework with applications to zero-shot time-series forecasting_|https://arxiv.org/pdf/2002.02887.pdf", "_DeepAR: Probabilistic forecasting with autoregressive recurrent networks_|https://arxiv.org/pdf/1704.04110.pdf", "N-BEATS: Neural Basis Expansion Analysis For Interpretable Time Series Forecasting|https://arxiv.org/pdf/1905.10437.pdf", " check the original paper|https://arxiv.org/pdf/1905.10437.pdf"]}, {"counter": 1, "description": "official project repo", "domain": "github.com", "id": 100015, "label": "github.com", "main": 0, "shape": "dot", "size": 10, "url": "github.com", "urls": ["official project repo|https://github.com/ServiceNow/N-BEATS/blob/master/experiments/electricity/interpretable.gin"]}, {"counter": 1, "description": "Linkedin", "domain": "www.linkedin.com", "id": 100016, "label": "linkedin.com", "main": 0, "shape": "dot", "size": 10, "url": "www.linkedin.com", "urls": ["Linkedin|https://www.linkedin.com/in/nikos-kafritsas-b3699180/"]}, {"counter": 1, "description": "M4 Forecasting Competition: Introducing a New Hybrid ES-RNN Model", "domain": "eng.uber.com", "id": 100017, "label": "eng.uber.com", "main": 0, "shape": "dot", "size": 10, "url": "eng.uber.com", "urls": ["M4 Forecasting Competition: Introducing a New Hybrid ES-RNN Model|https://eng.uber.com/m4-forecasting-competition/"]}, {"counter": 1, "description": "ElectricityLoadDiagrams20112014", "domain": "archive.ics.uci.edu", "id": 100018, "label": "archive.ics.uci.edu", "main": 0, "shape": "dot", "size": 10, "url": "archive.ics.uci.edu", "urls": ["ElectricityLoadDiagrams20112014|https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014"]}, {"counter": 2, "description": "https://forecasters.org/resources/time-series-data/", "domain": "forecasters.org", "id": 100019, "label": "forecasters.org|2", "main": 0, "shape": "dot", "size": 12, "url": "forecasters.org", "urls": ["https://forecasters.org/resources/time-series-data/|https://forecasters.org/resources/time-series-data/"]}, {"counter": 1, "description": "https://www.kaggle.com/c/tourism1", "domain": "www.kaggle.com", "id": 100020, "label": "kaggle.com", "main": 0, "shape": "dot", "size": 10, "url": "www.kaggle.com", "urls": ["https://www.kaggle.com/c/tourism1|https://www.kaggle.com/c/tourism1"]}, {"counter": 1, "description": "Robust Speech Recognition via Large-Scale Weak Supervision", "domain": "cdn.openai.com", "id": 100021, "label": "cdn.openai.com", "main": 0, "shape": "dot", "size": 10, "url": "cdn.openai.com", "urls": ["Robust Speech Recognition via Large-Scale Weak Supervision|https://cdn.openai.com/papers/whisper.pdf"]}, {"counter": 1, "description": " _Learning a synaptic learning rule_", "domain": "mila.quebec", "id": 100022, "label": "mila.quebec", "main": 0, "shape": "dot", "size": 10, "url": "mila.quebec", "urls": [" _Learning a synaptic learning rule_|https://mila.quebec/wp-content/uploads/2019/08/bengio_1991_ijcnn.pdf"]}, {"counter": 2, "description": "article", "domain": "medium.com", "id": 100023, "label": "medium.com|2", "main": 0, "shape": "dot", "size": 12, "url": "medium.com", "urls": ["newsletter|https://medium.com/subscribe/@nikoskafritsas", "article|https://medium.com/p/5aa17beb621"]}, {"counter": 7, "description": "TemporalFusionTransformer", "domain": "arxiv.org", "id": 100024, "label": "arxiv.org|7", "main": 0, "shape": "dot", "size": 22, "url": "arxiv.org", "urls": [" _Do We Really Need Deep Learning Models for Time Series Forecasting?_|https://arxiv.org/pdf/2101.02118.pdf", "DeepAR: Probabilistic forecasting with autoregressive recurrent networks|https://arxiv.org/pdf/1704.04110.pdf", "Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting|https://arxiv.org/pdf/1912.09363.pdf", "TemporalFusionTransformer|https://arxiv.org/pdf/1912.09363.pdf", "**Figure 9:** Temporal Patterns for Electricity dataset ([Source|https://arxiv.org/pdf/1912.09363.pdf"]}, {"counter": 1, "description": "PyTorch Forecasting", "domain": "github.com", "id": 100025, "label": "github.com", "main": 0, "shape": "dot", "size": 10, "url": "github.com", "urls": ["PyTorch Forecasting|https://github.com/jdb78/pytorch-forecasting"]}, {"counter": 1, "description": "here", "domain": "drive.google.com", "id": 100026, "label": "drive.google.com", "main": 0, "shape": "dot", "size": 10, "url": "drive.google.com", "urls": ["here|https://drive.google.com/file/d/1sVXcUQi0FXVZxgmiHX0oc5YZfrjwZzmA/view?usp=share_link"]}, {"counter": 2, "description": "https://archive.ics.uci.edu/ml/machine-learning-databases/00321/LD2011_2014.txt.zip\n!unzip", "domain": "archive.ics.uci.edu", "id": 100027, "label": "archive.ics.uci.edu|2", "main": 0, "shape": "dot", "size": 12, "url": "archive.ics.uci.edu", "urls": ["https://archive.ics.uci.edu/ml/machine-learning-databases/00321/LD2011_2014.txt.zip\n!unzip|https://archive.ics.uci.edu/ml/machine-learning-databases/00321/LD2011_2014.txt.zip!unzip", "ElectricityLoadDiagrams20112014|https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014"]}, {"counter": 1, "description": "Darts", "domain": "unit8co.github.io", "id": 100028, "label": "unit8co.github.io", "main": 0, "shape": "dot", "size": 10, "url": "unit8co.github.io", "urls": ["Darts|https://unit8co.github.io/darts/"]}, {"counter": 1, "description": "Linkedin", "domain": "www.linkedin.com", "id": 100029, "label": "linkedin.com", "main": 0, "shape": "dot", "size": 10, "url": "www.linkedin.com", "urls": ["Linkedin|https://www.linkedin.com/in/nikos-kafritsas-b3699180/"]}, {"counter": 1, "description": "SUBSCRIBE here ", "domain": "medium.com", "id": 100030, "label": "medium.com", "main": 0, "shape": "dot", "size": 10, "url": "medium.com", "urls": ["SUBSCRIBE here |https://medium.com/@nikoskafritsas/membership"]}, {"counter": 2, "description": "Ventilator Pressure Prediction", "domain": "www.kaggle.com", "id": 100031, "label": "kaggle.com|2", "main": 0, "shape": "dot", "size": 12, "url": "www.kaggle.com", "urls": ["multi-level deep architecture|https://www.kaggle.com/shujun717/1-solution-lstm-cnn-transformer-1-fold", "Ventilator Pressure Prediction|https://www.kaggle.com/c/ventilator-pressure-prediction"]}, {"counter": 1, "description": "ElementAI", "domain": "www.elementai.com", "id": 100032, "label": "elementai.com", "main": 0, "shape": "dot", "size": 10, "url": "www.elementai.com", "urls": ["ElementAI|https://www.elementai.com/"]}, {"counter": 11, "description": "**Figure 1:** The N-BEATS architecture ([Source", "domain": "arxiv.org", "id": 100033, "label": "arxiv.org|11", "main": 0, "shape": "dot", "size": 30, "url": "arxiv.org", "urls": ["Source|https://arxiv.org/pdf/2109.12218.pdf", "**Figure 3:**The series of transformations leading to the spatiotemporal sequence: The paper states: \"(1) The multivariate input format with time information included. Decoder inputs have missing (\"?\") values set to zero where predictions will be made. (2) The time sequence is passed through a [Time2Vec|https://arxiv.org/abs/1907.05321", "Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting|https://arxiv.org/pdf/1912.09363.pdf", "DeepAR: Probabilistic forecasting with autoregressive recurrent networks|https://arxiv.org/pdf/1704.04110.pdf", ",Time2Vec: Learning a Vector Representation of Time|https://arxiv.org/pdf/1907.05321.pdf", "Long-Range Transformers for Dynamic Spatiotemporal Forecasting|https://arxiv.org/pdf/2109.12218.pdf", "**Figure 2:** DeepAR model architecture ([Source|https://arxiv.org/pdf/1704.04110.pdf", "**Figure 1:** The N-BEATS architecture ([Source|https://arxiv.org/pdf/1905.10437.pdf", "N-BEATS: Neural Basis Expansion Analysis For Interpretable Time Series Forecasting|https://arxiv.org/pdf/1905.10437.pdf", "Time2Vec|https://arxiv.org/abs/1907.05321", "**Figure 4:** Top level architecture of TFT, along with its main components ([Source|https://arxiv.org/pdf/1912.09363.pdf"]}, {"counter": 1, "description": "Performer attention mechanism", "domain": "ai.googleblog.com", "id": 100034, "label": "ai.googleblog.com", "main": 0, "shape": "dot", "size": 10, "url": "ai.googleblog.com", "urls": ["Performer attention mechanism|https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html"]}, {"counter": 1, "description": "Github", "domain": "github.com", "id": 100035, "label": "github.com", "main": 0, "shape": "dot", "size": 10, "url": "github.com", "urls": ["Github|https://github.com/QData/spacetimeformer"]}, {"counter": 1, "description": "**Figure 5**: Effect of external static variables on forecasting ([Source", "domain": "blogger.googleusercontent.com", "id": 100036, "label": "blogger.googleusercontent.com", "main": 0, "shape": "dot", "size": 10, "url": "blogger.googleusercontent.com", "urls": ["**Figure 5**: Effect of external static variables on forecasting ([Source|https://blogger.googleusercontent.com/img/a/AVvXsEjn-GEpuwiBa4Od21FBnTST8-z2jAgyw3rq68AYtrBosFLBgIaFnLC2NV8hwlj8xiuU4Bc5ZKNHrDPldINdgkr8Y2TmekuDp0oLKq9yYCrpooZfwpwKT9MVwQ11LGsXqBckgiPAxoWRdvxAE3RoRn4BHxVhJmnQkZT-w6DdYXEA3yP0xUSdbYDITSgOjQ=s1138"]}, {"counter": 1, "description": "The M5 Accuracy competition: Results, findings and conclusions", "domain": "www.researchgate.net", "id": 100037, "label": "researchgate.net", "main": 0, "shape": "dot", "size": 10, "url": "www.researchgate.net", "urls": ["The M5 Accuracy competition: Results, findings and conclusions|https://www.researchgate.net/publication/344487258_The_M5_Accuracy_competition_Results_findings_and_conclusions"]}, {"counter": 2, "description": "article", "domain": "medium.com", "id": 100038, "label": "medium.com|2", "main": 0, "shape": "dot", "size": 12, "url": "medium.com", "urls": ["newsletter|https://medium.com/subscribe/@nikoskafritsas", "article|https://medium.com/p/d32c1e51cd91"]}, {"counter": 11, "description": "Figure 1: Top level architecture of TFT, along with its main components ([Source", "domain": "arxiv.org", "id": 100039, "label": "arxiv.org|11", "main": 0, "shape": "dot", "size": 30, "url": "arxiv.org", "urls": ["Figure 2: Gated Residual Network ([Source|https://arxiv.org/pdf/1912.09363.pdf", "A Multi-Horizon Quantile Recurrent Forecaster|https://arxiv.org/abs/1711.11053", "Figure 5: Temporal Patterns for Electricity dataset ([Source|https://arxiv.org/pdf/1912.09363.pdf", "DeepAR: Probabilistic forecasting with autoregressive recurrent networks|https://arxiv.org/pdf/1704.04110.pdf", "Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting|https://arxiv.org/pdf/1912.09363.pdf", "Attention Is All You Need|https://arxiv.org/abs/1706.03762", "Figure 1: Top level architecture of TFT, along with its main components ([Source|https://arxiv.org/pdf/1912.09363.pdf", "Figure 3: Variable Selection Network ([Source|https://arxiv.org/pdf/1912.09363.pdf", "Table 1: Feature Importances of Electricity Dataset ([Source|https://arxiv.org/pdf/1912.09363.pdf", "Language modeling with gated convolutional networks|https://arxiv.org/pdf/1612.08083v3.pdf", "Deep Visual-Semantic Alignments for Generating Image Descriptions|https://arxiv.org/abs/1412.2306"]}, {"counter": 1, "description": "Gated Convolutional Networks", "domain": "paperswithcode.com", "id": 100040, "label": "paperswithcode.com", "main": 0, "shape": "dot", "size": 10, "url": "paperswithcode.com", "urls": ["Gated Convolutional Networks|https://paperswithcode.com/paper/language-modeling-with-gated-convolutional"]}, {"counter": 4, "description": "Electricity Load Diagrams Dataset", "domain": "archive.ics.uci.edu", "id": 100041, "label": "archive.ics.uci.edu|4", "main": 0, "shape": "dot", "size": 16, "url": "archive.ics.uci.edu", "urls": ["Electricity Load Diagrams Dataset|https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014#", "PEM-SF Traffic Dataset|https://archive.ics.uci.edu/ml/datasets/PEMS-SF", "electricity|https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014#"]}, {"counter": 2, "description": "Favorita Grocery Sales", "domain": "www.kaggle.com", "id": 100042, "label": "kaggle.com|2", "main": 0, "shape": "dot", "size": 12, "url": "www.kaggle.com", "urls": ["Kaggle|https://www.kaggle.com/azzabiala/corporacin-favorita-grocery-sales-forecasting", "Favorita Grocery Sales|https://www.kaggle.com/azzabiala/corporacin-favorita-grocery-sales-forecasting"]}, {"counter": 3, "description": "open-source implementation", "domain": "github.com", "id": 100043, "label": "github.com|3", "main": 0, "shape": "dot", "size": 14, "url": "github.com", "urls": ["https://github.com/greatwhiz/tft_tf2.git|https://github.com/greatwhiz/tft_tf2.git", "open-source implementation|https://github.com/google-research/google-research/tree/master/tft", "here|https://github.com/greatwhiz/tft_tf2"]}, {"counter": 1, "description": "comprehensive tutorial", "domain": "pytorch-forecasting.readthedocs.io", "id": 100044, "label": "pytorch-forecasting.readthedocs.io", "main": 0, "shape": "dot", "size": 10, "url": "pytorch-forecasting.readthedocs.io", "urls": ["comprehensive tutorial|https://pytorch-forecasting.readthedocs.io/en/latest/tutorials/stallion.html"]}, {"counter": 1, "description": "Gartner", "domain": "www.gartner.com", "id": 100045, "label": "gartner.com", "main": 0, "shape": "dot", "size": 10, "url": "www.gartner.com", "urls": ["Gartner|https://www.gartner.com/en/documents/3988118/hype-cycle-for-data-science-and-machine-learning-2020"]}, {"counter": 1, "description": "Linkedin", "domain": "www.linkedin.com", "id": 100046, "label": "linkedin.com", "main": 0, "shape": "dot", "size": 10, "url": "www.linkedin.com", "urls": ["Linkedin|https://www.linkedin.com/in/nikos-kafritsas-b3699180/"]}, {"counter": 1, "description": "Deep state space models for time series forecasting", "domain": "papers.nips.cc", "id": 100047, "label": "papers.nips.cc", "main": 0, "shape": "dot", "size": 10, "url": "papers.nips.cc", "urls": ["Deep state space models for time series forecasting|https://papers.nips.cc/paper/2018/file/5cf68969fb67aa6082363a6d4e6468e2-Paper.pdf"]}, {"counter": 1, "description": "The Illustrated Transformer", "domain": "jalammar.github.io", "id": 100048, "label": "jalammar.github.io", "main": 0, "shape": "dot", "size": 10, "url": "jalammar.github.io", "urls": ["The Illustrated Transformer|https://jalammar.github.io/illustrated-transformer/"]}, {"counter": 1, "description": "failing to predict significant events.", "domain": "samueldwatts.com", "id": 100049, "label": "samueldwatts.com", "main": 0, "shape": "dot", "size": 10, "url": "samueldwatts.com", "urls": ["failing to predict significant events.|http://samueldwatts.com/wp-content/uploads/2016/08/Watts-Gaussian-Copula_Financial_Crisis.pdf"]}, {"counter": 6, "description": "Probability Integral Transform", "domain": "en.wikipedia.org", "id": 100050, "label": "en.wikipedia.org|6", "main": 0, "shape": "dot", "size": 20, "url": "en.wikipedia.org", "urls": [" Kolmogorov-Smirnov test|https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test", "Probability Integral Transform|https://en.wikipedia.org/wiki/Probability_integral_transform", "Empirical Cumulative Distribution Function|https://en.wikipedia.org/wiki/Empirical_distribution_function", "Inverse sampling|https://en.wikipedia.org/wiki/Inverse_transform_sampling", "Metropolis-Hastings Monte Carlo|https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm", "Wikipedia,|https://en.wikipedia.org/wiki/Empirical_distribution_function"]}, {"counter": 1, "description": "here", "domain": "jovian.com", "id": 100051, "label": "jovian.com", "main": 0, "shape": "dot", "size": 10, "url": "jovian.com", "urls": ["here|https://jovian.com/nkafr/copulas-medium"]}, {"counter": 1, "description": "newsletter", "domain": "medium.com", "id": 100052, "label": "medium.com", "main": 0, "shape": "dot", "size": 10, "url": "medium.com", "urls": ["newsletter|https://medium.com/subscribe/@nikoskafritsas"]}, {"counter": 1, "description": "Linkedin", "domain": "www.linkedin.com", "id": 100053, "label": "linkedin.com", "main": 0, "shape": "dot", "size": 10, "url": "www.linkedin.com", "urls": ["Linkedin|https://www.linkedin.com/in/nikos-kafritsas-b3699180/"]}, {"counter": 3, "description": "_High-Dimensional Multivariate Forecasting with Low-Rank Gaussian Copula Processes_", "domain": "arxiv.org", "id": 100054, "label": "arxiv.org|3", "main": 0, "shape": "dot", "size": 14, "url": "arxiv.org", "urls": ["_High-Dimensional Multivariate Forecasting with Low-Rank Gaussian Copula Processes_|https://arxiv.org/pdf/1910.03002.pdf", "_TACTiS: Transformer-Attentional Copulas for Time Series_|https://arxiv.org/abs/2202.03528", "_Copula Conformal Prediction for Multi-Step Time Series Forecasting_|https://arxiv.org/pdf/2212.03281.pdf"]}, {"counter": 7, "description": "**Figure 1:** The Google Neural Machine Translation - GNMT architecture ([Source", "domain": "arxiv.org", "id": 100055, "label": "arxiv.org|7", "main": 0, "shape": "dot", "size": 22, "url": "arxiv.org", "urls": ["**Figure 2:** Mathematical operations in DeepAR during training ([Source|https://arxiv.org/pdf/1704.04110.pdf", "_Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting_|https://arxiv.org/pdf/1912.09363.pdf", "High-Dimensional Multivariate Forecasting with Low-Rank Gaussian Copula Processes|https://arxiv.org/pdf/1910.03002.pdf", "**Figure 1:** The Google Neural Machine Translation - GNMT architecture ([Source|https://arxiv.org/pdf/1609.08144.pdf", "Google\u0027s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation|https://arxiv.org/abs/1609.08144", "DeepAR: Probabilistic forecasting with autoregressive recurrent networks|https://arxiv.org/pdf/1704.04110.pdf", "**Figure 3:** Mathematical operations in DeepAR training inference ([Source|https://arxiv.org/pdf/1704.04110.pdf"]}, {"counter": 1, "description": "Linkedin", "domain": "www.linkedin.com", "id": 100056, "label": "linkedin.com", "main": 0, "shape": "dot", "size": 10, "url": "www.linkedin.com", "urls": ["Linkedin|https://www.linkedin.com/in/nikos-kafritsas-b3699180/"]}, {"counter": 1, "description": "https://ts.gluon.ai/stable/api/gluonts/gluonts.model.deepar.html", "domain": "ts.gluon.ai", "id": 100057, "label": "ts.gluon.ai", "main": 0, "shape": "dot", "size": 10, "url": "ts.gluon.ai", "urls": ["https://ts.gluon.ai/stable/api/gluonts/gluonts.model.deepar.html|https://ts.gluon.ai/stable/api/gluonts/gluonts.model.deepar.html"]}, {"counter": 1, "description": "CLIP", "domain": "medium.com", "id": 100058, "label": "medium.com", "main": 0, "shape": "dot", "size": 10, "url": "medium.com", "urls": ["CLIP|https://medium.com/p/f8ee408958b1"]}, {"counter": 2, "description": "Google Colab", "domain": "colab.research.google.com", "id": 100059, "label": "colab.research.google.com|2", "main": 0, "shape": "dot", "size": 12, "url": "colab.research.google.com", "urls": ["page|https://colab.research.google.com/", "Google Colab|https://colab.research.google.com/"]}, {"counter": 1, "description": "here", "domain": "drive.google.com", "id": 100060, "label": "drive.google.com", "main": 0, "shape": "dot", "size": 10, "url": "drive.google.com", "urls": ["here|https://drive.google.com/file/d/1o0P0JnF6p40WGJv0zCGRtsh3BxdUF1iJ/view?usp=sharing"]}, {"counter": 2, "description": "newsletter", "domain": "towardsdatascience.com", "id": 100061, "label": "towardsdatascience.com|2", "main": 0, "shape": "dot", "size": 12, "url": "towardsdatascience.com", "urls": ["Join Medium|https://towardsdatascience.com/@nikoskafritsas/membership", "newsletter|https://towardsdatascience.com/subscribe/@nikoskafritsas"]}, {"counter": 1, "description": "Linkedin", "domain": "www.linkedin.com", "id": 100062, "label": "linkedin.com", "main": 0, "shape": "dot", "size": 10, "url": "www.linkedin.com", "urls": ["Linkedin|https://www.linkedin.com/in/nikos-kafritsas-b3699180/"]}, {"counter": 1, "description": "_Learning Transferable Visual Models From Natural Language Supervision_", "domain": "arxiv.org", "id": 100063, "label": "arxiv.org", "main": 0, "shape": "dot", "size": 10, "url": "arxiv.org", "urls": ["_Learning Transferable Visual Models From Natural Language Supervision_|https://arxiv.org/pdf/2103.00020.pdf"]}, {"counter": 1, "description": "Robust Speech Recognition via Large-Scale Weak Supervision", "domain": "cdn.openai.com", "id": 100064, "label": "cdn.openai.com", "main": 0, "shape": "dot", "size": 10, "url": "cdn.openai.com", "urls": ["Robust Speech Recognition via Large-Scale Weak Supervision|https://cdn.openai.com/papers/whisper.pdf"]}, {"counter": 1, "description": "PopcornMedia", "domain": "www.youtube.com", "id": 100065, "label": "youtube.com", "main": 0, "shape": "dot", "size": 10, "url": "www.youtube.com", "urls": ["PopcornMedia|https://www.youtube.com/channel/UCvaAInXbA_x30WlwC5F0GSw"]}, {"counter": 7, "description": "**Figure 1:** Top-level architecture of Whisper, along with its main components ([Source", "domain": "cdn.openai.com", "id": 100066, "label": "cdn.openai.com|7", "main": 0, "shape": "dot", "size": 22, "url": "cdn.openai.com", "urls": ["**Figure 3:**WER of Whisper vs other models with added noise ([Source|https://cdn.openai.com/papers/whisper.pdf", "**Table 1:** Whisper outperforms a SOTA wav2vec model on various datasets ([Source|https://cdn.openai.com/papers/whisper.pdf", "paper\u0027s Appendix|https://cdn.openai.com/papers/whisper.pdf", "**Figure 1:** Top-level architecture of Whisper, along with its main components ([Source|https://cdn.openai.com/papers/whisper.pdf", "**Figure 2:**Different Tasks in the Decoder part of Whisper ([Source|https://cdn.openai.com/papers/whisper.pdf", "Robust Speech Recognition via Large-Scale Weak Supervision|https://cdn.openai.com/papers/whisper.pdf", "**Table 2:** Whisper models against other SOTA models in terms of BLEU score on audio-to-English translation ([Source|https://cdn.openai.com/papers/whisper.pdf"]}, {"counter": 1, "description": "here", "domain": "jovian.ai", "id": 100067, "label": "jovian.ai", "main": 0, "shape": "dot", "size": 10, "url": "jovian.ai", "urls": ["here|https://jovian.ai/nkafr/whisper-openai"]}, {"counter": 2, "description": "ffmpeg", "domain": "ffmpeg.org", "id": 100068, "label": "ffmpeg.org|2", "main": 0, "shape": "dot", "size": 12, "url": "ffmpeg.org", "urls": ["ffmpeg|https://ffmpeg.org/"]}, {"counter": 1, "description": "original script", "domain": "github.com", "id": 100069, "label": "github.com", "main": 0, "shape": "dot", "size": 10, "url": "github.com", "urls": ["original script|https://github.com/openai/whisper/blob/main/whisper/transcribe.py"]}, {"counter": 1, "description": "newsletter", "domain": "towardsdatascience.com", "id": 100070, "label": "towardsdatascience.com", "main": 0, "shape": "dot", "size": 10, "url": "towardsdatascience.com", "urls": ["newsletter|https://towardsdatascience.com/subscribe/@nikoskafritsas"]}, {"counter": 1, "description": "Linkedin", "domain": "www.linkedin.com", "id": 100071, "label": "linkedin.com", "main": 0, "shape": "dot", "size": 10, "url": "www.linkedin.com", "urls": ["Linkedin|https://www.linkedin.com/in/nikos-kafritsas-b3699180/"]}, {"counter": 3, "description": "_Learning Transferable Visual Models From Natural Language Supervision_", "domain": "arxiv.org", "id": 100072, "label": "arxiv.org|3", "main": 0, "shape": "dot", "size": 14, "url": "arxiv.org", "urls": ["_Learning Transferable Visual Models From Natural Language Supervision_|https://arxiv.org/pdf/2103.00020.pdf", "A framework for self-supervised learning of speech representations|https://arxiv.org/pdf/2006.11477.pdf", "Attention Is All You Need|https://arxiv.org/abs/1706.03762"]}, {"counter": 1, "description": "https://archive.org/details/gettysburg_johng_librivox", "domain": "archive.org", "id": 100073, "label": "archive.org", "main": 0, "shape": "dot", "size": 10, "url": "archive.org", "urls": ["https://archive.org/details/gettysburg_johng_librivox|https://archive.org/details/gettysburg_johng_librivox"]}, {"counter": 1, "description": "PASOKwebTV", "domain": "www.youtube.com", "id": 100074, "label": "youtube.com", "main": 0, "shape": "dot", "size": 10, "url": "www.youtube.com", "urls": ["PASOKwebTV|https://www.youtube.com/c/pasokwebtv"]}, {"counter": 2, "description": "**CLIP** has been used to index photos on Unsplash", "domain": "twitter.com", "id": 100075, "label": "twitter.com|2", "main": 0, "shape": "dot", "size": 12, "url": "twitter.com", "urls": ["**CLIP** has been used to index photos on Unsplash|https://twitter.com/haltakov/status/1351271379103002632", "_Discriminator_ in GANs|https://twitter.com/sayantandas_/status/1351830997403254785"]}, {"counter": 1, "description": "ImageNet", "domain": "www.image-net.org", "id": 100076, "label": "image-net.org", "main": 0, "shape": "dot", "size": 10, "url": "www.image-net.org", "urls": ["ImageNet|https://www.image-net.org/update-mar-11-2021.php"]}, {"counter": 16, "description": "**Figure 1:** Contrastive Pre-training step of **CLIP**([Source", "domain": "arxiv.org", "id": 100077, "label": "arxiv.org|16", "main": 0, "shape": "dot", "size": 40, "url": "arxiv.org", "urls": ["**Figure 1:** Contrastive Pre-training step of **CLIP**([Source|https://arxiv.org/pdf/2103.00020.pdf", "**Figure 2:** [I|https://arxiv.org/pdf/2103.00020.pdf", "_Learning Transferable Visual Models From Natural Language Supervision_|https://arxiv.org/pdf/2103.00020.pdf", "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale|https://arxiv.org/abs/2010.11929", "_Rethinking few-shot image classification: a good embedding is all you need?_|https://arxiv.org/pdf/2003.11539.pdf", "DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting|https://arxiv.org/abs/2112.01518", "**Figure****3:******Zero-shot****classification****using****CLIP****([Source|https://arxiv.org/pdf/2103.00020.pdf", "**Figure 4:**Performance of CLIP against other models, in terms of few-shot classification ([Source|https://arxiv.org/pdf/2103.00020.pdf", "Hierarchical Text-Conditional Image Generation with CLIP Latents|https://arxiv.org/pdf/2204.06125.pdf", "**Figure 5:**Performance of CLIP against ResNet, in terms of distribution shift ([Source|https://arxiv.org/pdf/2103.00020.pdf", "I|https://arxiv.org/pdf/2103.00020.pdf", "**Figure 7:**Classification of digits ([Source|https://arxiv.org/pdf/2103.00020.pdf", "High-Resolution Image Synthesis with Latent Diffusion Models|https://arxiv.org/pdf/2112.10752.pdf", "_CONTRASTIVE LEARNING OF MEDICAL VISUAL REPRESENTATIONS FROM PAIRED IMAGES AND TEXT_|https://arxiv.org/pdf/2010.00747.pdf", "**Figure 6:**Number of floating point operations per image, for various models ([Source|https://arxiv.org/pdf/2103.00020.pdf", "Expanding Language-Image Pretrained Models for General Video Recognition|https://arxiv.org/abs/2208.02816"]}, {"counter": 1, "description": "paint.wtf", "domain": "paint.wtf", "id": 100078, "label": "paint.wtf", "main": 0, "shape": "dot", "size": 10, "url": "paint.wtf", "urls": ["paint.wtf|https://paint.wtf/"]}, {"counter": 1, "description": "here", "domain": "jovian.ai", "id": 100079, "label": "jovian.ai", "main": 0, "shape": "dot", "size": 10, "url": "jovian.ai", "urls": ["here|https://jovian.ai/nkafr/clip"]}, {"counter": 1, "description": "newsletter", "domain": "towardsdatascience.com", "id": 100080, "label": "towardsdatascience.com", "main": 0, "shape": "dot", "size": 10, "url": "towardsdatascience.com", "urls": ["newsletter|https://towardsdatascience.com/subscribe/@nikoskafritsas"]}, {"counter": 1, "description": "Linkedin", "domain": "www.linkedin.com", "id": 100081, "label": "linkedin.com", "main": 0, "shape": "dot", "size": 10, "url": "www.linkedin.com", "urls": ["Linkedin|https://www.linkedin.com/in/nikos-kafritsas-b3699180/"]}, {"counter": 1, "description": "_Language Models are Unsupervised Multitask Learners_", "domain": "d4mucfpksywv.cloudfront.net", "id": 100082, "label": "d4mucfpksywv.cloudfront.net", "main": 0, "shape": "dot", "size": 10, "url": "d4mucfpksywv.cloudfront.net", "urls": ["_Language Models are Unsupervised Multitask Learners_|https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf"]}, {"counter": 1, "description": "Conditional Prompt Learning for Vision-Language Models", "domain": "openaccess.thecvf.com", "id": 100083, "label": "openaccess.thecvf.com", "main": 0, "shape": "dot", "size": 10, "url": "openaccess.thecvf.com", "urls": ["Conditional Prompt Learning for Vision-Language Models|https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.pdf"]}, {"counter": 6, "description": "NER App with Streamlit, image by author ([Source", "domain": "huggingface.co", "id": 100084, "label": "huggingface.co|6", "main": 0, "shape": "dot", "size": 20, "url": "huggingface.co", "urls": ["Source|https://huggingface.co/datasets/wnut_17", "here|https://huggingface.co/spaces/nkaf/ner-tagger-streamlit/tree/main", "page|https://huggingface.co/", "here|https://huggingface.co/spaces/nkaf/ner-tagger-streamlit", "https://huggingface.co/spaces/nkaf/ner-tagger-streamlit|https://huggingface.co/spaces/nkaf/ner-tagger-streamlit", "NER App with Streamlit, image by author ([Source|https://huggingface.co/spaces/nkaf/ner-tagger-streamlit"]}, {"counter": 3, "description": "In my previous article", "domain": "medium.com", "id": 100085, "label": "medium.com|3", "main": 0, "shape": "dot", "size": 14, "url": "medium.com", "urls": ["Join Medium|https://medium.com/@nikoskafritsas/membership", " previous post|https://medium.com/p/274c6965e2d", "In my previous article|https://medium.com/p/274c6965e2d"]}, {"counter": 1, "description": "**Dash**", "domain": "dash.plotly.com", "id": 100086, "label": "dash.plotly.com", "main": 0, "shape": "dot", "size": 10, "url": "dash.plotly.com", "urls": ["**Dash**|https://dash.plotly.com/layout"]}, {"counter": 2, "description": "**Gradio**", "domain": "gradio.app", "id": 100087, "label": "gradio.app|2", "main": 0, "shape": "dot", "size": 12, "url": "gradio.app", "urls": ["**Gradio**|https://gradio.app"]}, {"counter": 1, "description": "**Streamlit Cloud**", "domain": "streamlit.io", "id": 100088, "label": "streamlit.io", "main": 0, "shape": "dot", "size": 10, "url": "streamlit.io", "urls": ["**Streamlit Cloud**|https://streamlit.io/cloud"]}, {"counter": 1, "description": "@st", "domain": "twitter.com", "id": 100089, "label": "twitter.com", "main": 0, "shape": "dot", "size": 10, "url": "twitter.com", "urls": ["@st|http://twitter.com/st"]}, {"counter": 1, "description": "a custom way", "domain": "discuss.streamlit.io", "id": 100090, "label": "discuss.streamlit.io", "main": 0, "shape": "dot", "size": 10, "url": "discuss.streamlit.io", "urls": ["a custom way|https://discuss.streamlit.io/t/a-download-button-with-custom-css/4220"]}, {"counter": 1, "description": "Git LFS", "domain": "git-lfs.github.com", "id": 100091, "label": "git-lfs.github.com", "main": 0, "shape": "dot", "size": 10, "url": "git-lfs.github.com", "urls": ["Git LFS|https://git-lfs.github.com"]}, {"counter": 1, "description": "here", "domain": "docs.github.com", "id": 100092, "label": "docs.github.com", "main": 0, "shape": "dot", "size": 10, "url": "docs.github.com", "urls": ["here|https://docs.github.com/en/repositories/working-with-files/managing-large-files/installing-git-large-file-storage"]}, {"counter": 1, "description": "newsletter", "domain": "towardsdatascience.com", "id": 100093, "label": "towardsdatascience.com", "main": 0, "shape": "dot", "size": 10, "url": "towardsdatascience.com", "urls": ["newsletter|https://towardsdatascience.com/subscribe/@nikoskafritsas"]}, {"counter": 1, "description": "Linkedin", "domain": "www.linkedin.com", "id": 100094, "label": "linkedin.com", "main": 0, "shape": "dot", "size": 10, "url": "www.linkedin.com", "urls": ["Linkedin|https://www.linkedin.com/in/nikos-kafritsas-b3699180/"]}, {"counter": 1, "description": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "domain": "arxiv.org", "id": 100095, "label": "arxiv.org", "main": 0, "shape": "dot", "size": 10, "url": "arxiv.org", "urls": ["RoBERTa: A Robustly Optimized BERT Pretraining Approach|https://arxiv.org/abs/1907.11692"]}, {"counter": 1, "description": "how to deploy this model with Streamlit!", "domain": "medium.com", "id": 100096, "label": "medium.com", "main": 0, "shape": "dot", "size": 10, "url": "medium.com", "urls": ["how to deploy this model with Streamlit!|https://medium.com/p/f157672f867f"]}, {"counter": 2, "description": "wnut_17", "domain": "huggingface.co", "id": 100097, "label": "huggingface.co|2", "main": 0, "shape": "dot", "size": 12, "url": "huggingface.co", "urls": ["Source|https://huggingface.co/datasets/wnut_17", "wnut_17|https://huggingface.co/datasets/wnut_17"]}, {"counter": 1, "description": "seqeval", "domain": "github.com", "id": 100098, "label": "github.com", "main": 0, "shape": "dot", "size": 10, "url": "github.com", "urls": ["seqeval|https://github.com/chakki-works/seqeval"]}, {"counter": 1, "description": "here", "domain": "jovian.ai", "id": 100099, "label": "jovian.ai", "main": 0, "shape": "dot", "size": 10, "url": "jovian.ai", "urls": ["here|https://jovian.ai/nkafr/ner-transformers"]}, {"counter": 1, "description": "newsletter", "domain": "towardsdatascience.com", "id": 100100, "label": "towardsdatascience.com", "main": 0, "shape": "dot", "size": 10, "url": "towardsdatascience.com", "urls": ["newsletter|https://towardsdatascience.com/subscribe/@nikoskafritsas"]}, {"counter": 1, "description": "Linkedin", "domain": "www.linkedin.com", "id": 100101, "label": "linkedin.com", "main": 0, "shape": "dot", "size": 10, "url": "www.linkedin.com", "urls": ["Linkedin|https://www.linkedin.com/in/nikos-kafritsas-b3699180/"]}, {"counter": 1, "description": "PEP 585", "domain": "www.python.org", "id": 100102, "label": "python.org", "main": 0, "shape": "dot", "size": 10, "url": "www.python.org", "urls": ["PEP 585|https://www.python.org/dev/peps/pep-0585/"]}, {"counter": 1, "description": "Zen of Python", "domain": "en.wikipedia.org", "id": 100103, "label": "en.wikipedia.org", "main": 0, "shape": "dot", "size": 10, "url": "en.wikipedia.org", "urls": ["Zen of Python|https://en.wikipedia.org/wiki/Zen_of_Python"]}, {"counter": 1, "description": "Linkedin", "domain": "www.linkedin.com", "id": 100104, "label": "linkedin.com", "main": 0, "shape": "dot", "size": 10, "url": "www.linkedin.com", "urls": ["Linkedin|https://www.linkedin.com/in/nikos-kafritsas-b3699180/"]}, {"counter": 1, "description": "**Figure3:** Encoding and Decoding in vanilla Transformer ([Source", "domain": "ai.googleblog.com", "id": 100105, "label": "ai.googleblog.com", "main": 0, "shape": "dot", "size": 10, "url": "ai.googleblog.com", "urls": ["**Figure3:** Encoding and Decoding in vanilla Transformer ([Source|https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"]}, {"counter": 11, "description": "**Figure 6:**The optimized Attention matrix of Block Recurrent Transformer for a single training step. Instead of calculating the full matrix, only the scores inside the 2 black tiles will be calculated. ([Source", "domain": "arxiv.org", "id": 100106, "label": "arxiv.org|11", "main": 0, "shape": "dot", "size": 30, "url": "arxiv.org", "urls": ["**Figure 6:**The optimized Attention matrix of Block Recurrent Transformer for a single training step. Instead of calculating the full matrix, only the scores inside the 2 black tiles will be calculated. ([Source|https://arxiv.org/pdf/2203.07852.pdf", "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale|https://arxiv.org/abs/2010.11929", "Block Recurrent Transformers|https://arxiv.org/pdf/2203.07852.pdf", "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches|https://arxiv.org/abs/1409.1259", "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding|https://arxiv.org/pdf/1810.04805v2.pdf", "Attention Is All You Need|https://arxiv.org/abs/1706.03762", " Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer|https://arxiv.org/pdf/1910.10683.pdf", "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention|https://arxiv.org/pdf/2006.16236.pdf", "Longformer: The Long-Document Transformer|https://arxiv.org/pdf/2004.05150.pdf", "Angelos Katharopoulos|https://arxiv.org/search/cs?searchtype=author\u0026query=Katharopoulos%2C+A", "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context|https://arxiv.org/pdf/1901.02860.pdf"]}, {"counter": 1, "description": "Linkedin", "domain": "www.linkedin.com", "id": 100107, "label": "linkedin.com", "main": 0, "shape": "dot", "size": 10, "url": "www.linkedin.com", "urls": ["Linkedin|https://www.linkedin.com/in/nikos-kafritsas-b3699180/"]}, {"counter": 1, "description": "https://blog.medium.com/evolving-the-partner-program-2613708f9f3c", "domain": "blog.medium.com", "id": 100108, "label": "blog.medium.com", "main": 0, "shape": "dot", "size": 10, "url": "blog.medium.com", "urls": ["https://blog.medium.com/evolving-the-partner-program-2613708f9f3c|https://blog.medium.com/evolving-the-partner-program-2613708f9f3c"]}, {"counter": 2, "description": "https://help.medium.com/hc/en-us/articles/4405927192215-About-referred-memberships", "domain": "help.medium.com", "id": 100109, "label": "help.medium.com|2", "main": 0, "shape": "dot", "size": 12, "url": "help.medium.com", "urls": ["Source: [https://help.medium.com/hc/en-us/articles/4405927192215-About-referred-memberships|https://help.medium.com/hc/en-us/articles/4405927192215-About-referred-memberships", "https://help.medium.com/hc/en-us/articles/4405927192215-About-referred-memberships|https://help.medium.com/hc/en-us/articles/4405927192215-About-referred-memberships"]}, {"counter": 3, "description": "page", "domain": "keisan.casio.com", "id": 100110, "label": "keisan.casio.com|3", "main": 0, "shape": "dot", "size": 14, "url": "keisan.casio.com", "urls": ["[Source|https://keisan.casio.com/exec/system/14013211335541", "page|https://keisan.casio.com/exec/system/14013211335541"]}, {"counter": 1, "description": "Linkedin", "domain": "www.linkedin.com", "id": 100111, "label": "linkedin.com", "main": 0, "shape": "dot", "size": 10, "url": "www.linkedin.com", "urls": ["Linkedin|https://www.linkedin.com/in/nikos-kafritsas-b3699180/"]}, {"counter": 2, "description": "E-book Writing Guide", "domain": "payhip.com", "id": 100112, "label": "payhip.com|2", "main": 0, "shape": "dot", "size": 12, "url": "payhip.com", "urls": ["E-book Writing Guide|https://payhip.com/b/3WwoR", "**How To Dominate Medium as a Freelance Writer**|https://payhip.com/b/3WwoR"]}, {"counter": 1, "description": "Linkedin", "domain": "www.linkedin.com", "id": 100113, "label": "linkedin.com", "main": 0, "shape": "dot", "size": 10, "url": "www.linkedin.com", "urls": ["Linkedin|https://www.linkedin.com/in/nikos-kafritsas-b3699180/"]}, {"counter": 3, "description": "here", "domain": "medium.com", "id": 100114, "label": "medium.com|3", "main": 0, "shape": "dot", "size": 14, "url": "medium.com", "urls": ["newsletter|https://medium.com/subscribe/@nikoskafritsas", "here|https://medium.com/@nikoskafritsas/list/setup-apple-m1-for-deep-learning-75e74b9f7cb4", "this list|https://medium.com/@nikoskafritsas/list/setup-apple-m1-for-deep-learning-75e74b9f7cb4"]}, {"counter": 1, "description": "Multi-Process Service", "domain": "docs.nvidia.com", "id": 100115, "label": "docs.nvidia.com", "main": 0, "shape": "dot", "size": 10, "url": "docs.nvidia.com", "urls": ["Multi-Process Service|https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf"]}, {"counter": 1, "description": "e PyTorch page", "domain": "pytorch.org", "id": 100116, "label": "pytorch.org", "main": 0, "shape": "dot", "size": 10, "url": "pytorch.org", "urls": ["e PyTorch page|https://pytorch.org/"]}, {"counter": 1, "description": "release list", "domain": "github.com", "id": 100117, "label": "github.com", "main": 0, "shape": "dot", "size": 10, "url": "github.com", "urls": ["release list|https://github.com/pytorch/pytorch/releases"]}, {"counter": 1, "description": "Linkedin", "domain": "www.linkedin.com", "id": 100118, "label": "linkedin.com", "main": 0, "shape": "dot", "size": 10, "url": "www.linkedin.com", "urls": ["Linkedin|https://www.linkedin.com/in/nikos-kafritsas-b3699180/"]}, {"counter": 1, "description": "newsletter", "domain": "medium.com", "id": 100119, "label": "medium.com", "main": 0, "shape": "dot", "size": 10, "url": "medium.com", "urls": ["newsletter|https://medium.com/subscribe/@nikoskafritsas"]}, {"counter": 1, "description": "Linkedin", "domain": "www.linkedin.com", "id": 100120, "label": "linkedin.com", "main": 0, "shape": "dot", "size": 10, "url": "www.linkedin.com", "urls": ["Linkedin|https://www.linkedin.com/in/nikos-kafritsas-b3699180/"]}, {"counter": 1, "description": "https://www.kaggle.com/datasets/zalando-research/fashionmnist", "domain": "www.kaggle.com", "id": 100121, "label": "kaggle.com", "main": 0, "shape": "dot", "size": 10, "url": "www.kaggle.com", "urls": ["https://www.kaggle.com/datasets/zalando-research/fashionmnist|https://www.kaggle.com/datasets/zalando-research/fashionmnist"]}, {"counter": 1, "description": "https://keras.io/keras_tuner/", "domain": "keras.io", "id": 100122, "label": "keras.io", "main": 0, "shape": "dot", "size": 10, "url": "keras.io", "urls": ["https://keras.io/keras_tuner/|https://keras.io/keras_tuner/"]}, {"counter": 2, "description": "URI", "domain": "live.dbpedia.org", "id": 100123, "label": "live.dbpedia.org|2", "main": 0, "shape": "dot", "size": 12, "url": "live.dbpedia.org", "urls": ["URI|http://live.dbpedia.org/ontology/"]}, {"counter": 1, "description": "newsletter", "domain": "medium.com", "id": 100124, "label": "medium.com", "main": 0, "shape": "dot", "size": 10, "url": "medium.com", "urls": ["newsletter|https://medium.com/subscribe/@nikoskafritsas"]}, {"counter": 1, "description": "Linkedin", "domain": "www.linkedin.com", "id": 100125, "label": "linkedin.com", "main": 0, "shape": "dot", "size": 10, "url": "www.linkedin.com", "urls": ["Linkedin|https://www.linkedin.com/in/nikos-kafritsas-b3699180/"]}, {"counter": 1, "description": "https://en.wikipedia.org/w/index.php?curid=10157789", "domain": "en.wikipedia.org", "id": 100126, "label": "en.wikipedia.org", "main": 0, "shape": "dot", "size": 10, "url": "en.wikipedia.org", "urls": ["https://en.wikipedia.org/w/index.php?curid=10157789|https://en.wikipedia.org/w/index.php?curid=10157789"]}, {"counter": 1, "description": "The Semantic Web", "domain": "web.archive.org", "id": 100127, "label": "web.archive.org", "main": 0, "shape": "dot", "size": 10, "url": "web.archive.org", "urls": ["The Semantic Web|https://web.archive.org/web/20171010210556/https://pdfs.semanticscholar.org/566c/1c6bd366b4c9e07fc37eb372771690d5ba31.pdf"]}, {"counter": 1, "description": "here", "domain": "jovian.ai", "id": 100128, "label": "jovian.ai", "main": 0, "shape": "dot", "size": 10, "url": "jovian.ai", "urls": ["here|https://jovian.ai/nkafr/create-image-classification-models-with-tensorflow-in-10-minutes"]}, {"counter": 1, "description": "newsletter", "domain": "towardsdatascience.com", "id": 100129, "label": "towardsdatascience.com", "main": 0, "shape": "dot", "size": 10, "url": "towardsdatascience.com", "urls": ["newsletter|https://towardsdatascience.com/subscribe/@nikoskafritsas"]}, {"counter": 1, "description": "Linkedin", "domain": "www.linkedin.com", "id": 100130, "label": "linkedin.com", "main": 0, "shape": "dot", "size": 10, "url": "www.linkedin.com", "urls": ["Linkedin|https://www.linkedin.com/in/nikos-kafritsas-b3699180/"]}, {"counter": 1, "description": "Join Medium", "domain": "medium.com", "id": 100131, "label": "medium.com", "main": 0, "shape": "dot", "size": 10, "url": "medium.com", "urls": ["Join Medium|https://medium.com/@nikoskafritsas/membership"]}, {"counter": 1, "description": "https://www.kaggle.com/datasets/zalando-research/fashionmnist", "domain": "www.kaggle.com", "id": 100132, "label": "kaggle.com", "main": 0, "shape": "dot", "size": 10, "url": "www.kaggle.com", "urls": ["https://www.kaggle.com/datasets/zalando-research/fashionmnist|https://www.kaggle.com/datasets/zalando-research/fashionmnist"]}, {"counter": 2, "description": "https://commons.wikimedia.org/w/index.php?curid=24913461", "domain": "commons.wikimedia.org", "id": 100133, "label": "commons.wikimedia.org|2", "main": 0, "shape": "dot", "size": 12, "url": "commons.wikimedia.org", "urls": ["https://commons.wikimedia.org/w/index.php?curid=24913461|https://commons.wikimedia.org/w/index.php?curid=24913461", "https://commons.wikimedia.org/w/index.php?curid=45679374|https://commons.wikimedia.org/w/index.php?curid=45679374"]}, {"counter": 1, "description": "here", "domain": "medium.com", "id": 100134, "label": "medium.com", "main": 0, "shape": "dot", "size": 10, "url": "medium.com", "urls": ["here|https://medium.com/@nikoskafritsas/list/setup-apple-m1-for-deep-learning-75e74b9f7cb4"]}, {"counter": 2, "description": "page", "domain": "github.com", "id": 100135, "label": "github.com|2", "main": 0, "shape": "dot", "size": 12, "url": "github.com", "urls": ["Image 1: The correct conda Miniforge package for Apple Silicon ([Source|https://github.com/conda-forge/miniforge", "page|https://github.com/conda-forge/miniforge"]}, {"counter": 2, "description": "AlexNet paper", "domain": "proceedings.neurips.cc", "id": 100136, "label": "proceedings.neurips.cc|2", "main": 0, "shape": "dot", "size": 12, "url": "proceedings.neurips.cc", "urls": ["ImageNet Classification with Deep Convolutional Neural Networks|https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf", "AlexNet paper|https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf"]}, {"counter": 1, "description": "incorrect", "domain": "towardsdatascience.com", "id": 100137, "label": "towardsdatascience.com", "main": 0, "shape": "dot", "size": 10, "url": "towardsdatascience.com", "urls": ["incorrect|https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0"]}, {"counter": 1, "description": "do not give the full picture", "domain": "www.youtube.com", "id": 100138, "label": "youtube.com", "main": 0, "shape": "dot", "size": 10, "url": "www.youtube.com", "urls": ["do not give the full picture|https://www.youtube.com/watch?v=S27pHKBEp30"]}, {"counter": 2, "description": "_Long Short-Term Memory Networks_\u2014 LSTMs", "domain": "www.bioinf.jku.at", "id": 100139, "label": "bioinf.jku.at|2", "main": 0, "shape": "dot", "size": 12, "url": "www.bioinf.jku.at", "urls": ["_Long Short-Term Memory Networks_\u2014 LSTMs|http://www.bioinf.jku.at/publications/older/2604.pdf", "Long Short-term Memory|http://www.bioinf.jku.at/publications/older/2604.pdf"]}, {"counter": 23, "description": "Gated Recurrent Units-GRU", "domain": "arxiv.org", "id": 100140, "label": "arxiv.org|23", "main": 0, "shape": "dot", "size": 52, "url": "arxiv.org", "urls": ["_Transformer_|https://arxiv.org/abs/1706.03762", "Time2vec layer|https://arxiv.org/pdf/1907.05321.pdf", "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling|https://arxiv.org/pdf/1803.01271.pdf", "Attention Is All You Need|https://arxiv.org/abs/1706.03762", "**_Vision Transformer (ViT)_**|https://arxiv.org/abs/2010.11929", "Sequence to Sequence Learning with Neural Networks|https://arxiv.org/abs/1409.3215", "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches|https://arxiv.org/abs/1409.1259", "Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting|https://arxiv.org/pdf/1912.09363.pdf", "Deep Visual-Semantic Alignments for Generating Image Descriptions|https://arxiv.org/abs/1412.2306", "**Figure 1**: The Google Neural Machine Translation - GNMT architecture ([Source|https://arxiv.org/pdf/1609.08144.pdf", "DeepAR: Probabilistic forecasting with autoregressive recurrent networks|https://arxiv.org/pdf/1704.04110.pdf", "Google\u0027s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation|https://arxiv.org/abs/1609.08144", "_Temporal convolutional networks for action segmentation and detection_|https://arxiv.org/pdf/1611.05267.pdf", "Amazon\u0027s _DeepAR_|https://arxiv.org/pdf/1704.04110.pdf", "**Figure 6:**A dilated convolution with filter size k = 3 and dilation factors d = 1, 2, 4 . The receptive field can cover all datapoints x_0...x_T from the input sequence. ([Source|https://arxiv.org/pdf/1803.01271.pdf", "the****original****paper|https://arxiv.org/abs/1609.08144", "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale|https://arxiv.org/abs/2010.11929", "**Google\u0027s** _Temporal Fusion Transformer_|https://arxiv.org/pdf/1912.09363.pdf", "**Figure 3:** The Temporal Fusion Transformer ([Source|https://arxiv.org/pdf/1912.09363.pdf", "_sequence-to-sequence_ architecture|https://arxiv.org/abs/1409.3215", "Gated Recurrent Units-GRU|https://arxiv.org/abs/1409.1259", "Time2Vec: Learning a Vector Representation of Time|https://arxiv.org/pdf/1907.05321.pdf", "**Figure 4:** DeepAR model architecture ([Source|https://arxiv.org/pdf/1704.04110.pdf"]}, {"counter": 1, "description": "article", "domain": "smerity.com", "id": 100141, "label": "smerity.com", "main": 0, "shape": "dot", "size": 10, "url": "smerity.com", "urls": ["article|https://smerity.com/articles/2016/google_nmt_arch.html"]}, {"counter": 2, "description": "**Figure 2**: The open-source Transformer Family ([Source", "domain": "github.com", "id": 100142, "label": "github.com|2", "main": 0, "shape": "dot", "size": 12, "url": "github.com", "urls": ["**Figure 2**: The open-source Transformer Family ([Source|https://github.com/thunlp/PLMpapers", "**Figure 7:**Architecture of 1st place solution ([Source|https://github.com/Shujun-He/Google-Brain-Ventilator"]}, {"counter": 1, "description": "ES-RNN", "domain": "eng.uber.com", "id": 100143, "label": "eng.uber.com", "main": 0, "shape": "dot", "size": 10, "url": "eng.uber.com", "urls": ["ES-RNN|https://eng.uber.com/m4-forecasting-competition/"]}, {"counter": 1, "description": "**Figure 5:** Effect of external static variables on forecasting ([Source", "domain": "blogger.googleusercontent.com", "id": 100144, "label": "blogger.googleusercontent.com", "main": 0, "shape": "dot", "size": 10, "url": "blogger.googleusercontent.com", "urls": ["**Figure 5:** Effect of external static variables on forecasting ([Source|https://blogger.googleusercontent.com/img/a/AVvXsEjn-GEpuwiBa4Od21FBnTST8-z2jAgyw3rq68AYtrBosFLBgIaFnLC2NV8hwlj8xiuU4Bc5ZKNHrDPldINdgkr8Y2TmekuDp0oLKq9yYCrpooZfwpwKT9MVwQ11LGsXqBckgiPAxoWRdvxAE3RoRn4BHxVhJmnQkZT-w6DdYXEA3yP0xUSdbYDITSgOjQ=s1138"]}, {"counter": 2, "description": "Ventilator Pressure Prediction", "domain": "www.kaggle.com", "id": 100145, "label": "kaggle.com|2", "main": 0, "shape": "dot", "size": 12, "url": "www.kaggle.com", "urls": ["multi-level deep architecture|https://www.kaggle.com/shujun717/1-solution-lstm-cnn-transformer-1-fold", "Ventilator Pressure Prediction|https://www.kaggle.com/c/ventilator-pressure-prediction"]}, {"counter": 1, "description": "Learning internal representations by error propagation", "domain": "apps.dtic.mil", "id": 100146, "label": "apps.dtic.mil", "main": 0, "shape": "dot", "size": 10, "url": "apps.dtic.mil", "urls": ["Learning internal representations by error propagation|https://apps.dtic.mil/dtic/tr/fulltext/u2/a164453.pdf"]}, {"counter": 1, "description": "The plagiarized", "domain": "web.archive.org", "id": 100147, "label": "web.archive.org", "main": 0, "shape": "dot", "size": 10, "url": "web.archive.org", "urls": ["The plagiarized|https://web.archive.org/web/20211008132717/https://www.analyticsvidhya.com/blog/2021/10/dynamic-time-warping-explained-using-python-and-har-dataset/"]}, {"counter": 4, "description": "The plagiarized", "domain": "web.archive.org", "id": 100148, "label": "web.archive.org|4", "main": 0, "shape": "dot", "size": 16, "url": "web.archive.org", "urls": ["Dynamic Time Warping|https://web.archive.org/web/20211008132717/https://en.wikipedia.org/wiki/Dynamic_time_warping", "Dynamic programming|https://web.archive.org/web/20211008132717/https://en.wikipedia.org/wiki/Dynamic_programming", "The plagiarized|https://web.archive.org/web/20211008132717/https://www.analyticsvidhya.com/blog/2021/10/dynamic-time-warping-explained-using-python-and-har-dataset/"]}, {"counter": 1, "description": "The community itself discovered the plagiarism of a paper", "domain": "www.plagiarismtoday.com", "id": 100149, "label": "plagiarismtoday.com", "main": 0, "shape": "dot", "size": 10, "url": "www.plagiarismtoday.com", "urls": ["The community itself discovered the plagiarism of a paper|https://www.plagiarismtoday.com/2019/10/16/why-siraj-ravals-plagiarism-is-the-future-of-plagiarism/"]}, {"counter": 1, "description": "best articles of the month\" list", "domain": "betterprogramming.pub", "id": 100150, "label": "betterprogramming.pub", "main": 0, "shape": "dot", "size": 10, "url": "betterprogramming.pub", "urls": ["best articles of the month\" list|https://betterprogramming.pub/the-best-of-programming-on-medium-october-2021-24a7e724f0ab"]}, {"counter": 2, "description": "his example", "domain": "medium.com", "id": 100151, "label": "medium.com|2", "main": 0, "shape": "dot", "size": 12, "url": "medium.com", "urls": ["his example|https://medium.com/@geotourloukis/dear-nikos-thank-you-for-your-response-37e179b34eec", "**Join Medium with my referral link - Nikos Kafritsas**|https://medium.com/@nikoskafritsas/membership"]}, {"counter": 1, "description": "here", "domain": "medium.com", "id": 100152, "label": "medium.com", "main": 0, "shape": "dot", "size": 10, "url": "medium.com", "urls": ["here|https://medium.com/@nikoskafritsas/list/setup-apple-m1-for-deep-learning-75e74b9f7cb4"]}, {"counter": 1, "description": "page", "domain": "github.com", "id": 100153, "label": "github.com", "main": 0, "shape": "dot", "size": 10, "url": "github.com", "urls": ["page|https://github.com/conda-forge/miniforge"]}, {"counter": 1, "description": "here", "domain": "pytorch.org", "id": 100154, "label": "pytorch.org", "main": 0, "shape": "dot", "size": 10, "url": "pytorch.org", "urls": ["here|https://pytorch.org/get-started/locally/"]}, {"counter": 1, "description": "here", "domain": "medium.com", "id": 100155, "label": "medium.com", "main": 0, "shape": "dot", "size": 10, "url": "medium.com", "urls": ["here|https://medium.com/@nikoskafritsas/list/setup-apple-m1-for-deep-learning-75e74b9f7cb4"]}, {"counter": 1, "description": "Handwritten Digits USPS (U.S. Postal Service) dataset", "domain": "www.kaggle.com", "id": 100156, "label": "kaggle.com", "main": 0, "shape": "dot", "size": 10, "url": "www.kaggle.com", "urls": ["Handwritten Digits USPS (U.S. Postal Service) dataset|https://www.kaggle.com/bistaumanga/usps-dataset?select=usps.h5"]}, {"counter": 1, "description": "book", "domain": "epdf.pub", "id": 100157, "label": "epdf.pub", "main": 0, "shape": "dot", "size": 10, "url": "epdf.pub", "urls": ["book|https://epdf.pub/matrix-methods-in-data-mining-and-pattern-recognition32204.html"]}, {"counter": 1, "description": "page", "domain": "github.com", "id": 100158, "label": "github.com", "main": 0, "shape": "dot", "size": 10, "url": "github.com", "urls": ["page|https://github.com/conda-forge/miniforge"]}, {"counter": 2, "description": "dtaidistance", "domain": "dtaidistance.readthedocs.io", "id": 100159, "label": "dtaidistance.readthedocs.io|2", "main": 0, "shape": "dot", "size": 12, "url": "dtaidistance.readthedocs.io", "urls": ["dtaidistance|https://dtaidistance.readthedocs.io/en/latest/index.html"]}, {"counter": 4, "description": "Needleman\u2013Wunsch", "domain": "en.wikipedia.org", "id": 100160, "label": "en.wikipedia.org|4", "main": 0, "shape": "dot", "size": 16, "url": "en.wikipedia.org", "urls": ["Needleman\u2013Wunsch|https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm", "https://en.wikipedia.org/wiki/Dynamic_programming|https://en.wikipedia.org/wiki/Dynamic_programming", "Levenshtein distance|https://en.wikipedia.org/wiki/Levenshtein_distance", "https://en.wikipedia.org/wiki/Dynamic_time_warping|https://en.wikipedia.org/wiki/Dynamic_time_warping"]}, {"counter": 1, "description": "UCI", "domain": "archive.ics.uci.edu", "id": 100161, "label": "archive.ics.uci.edu", "main": 0, "shape": "dot", "size": 10, "url": "archive.ics.uci.edu", "urls": ["UCI|https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones"]}, {"counter": 3, "description": "@50Hz", "domain": "twitter.com", "id": 100162, "label": "twitter.com|3", "main": 0, "shape": "dot", "size": 14, "url": "twitter.com", "urls": ["@50Hz|http://twitter.com/50Hz"]}], "user_image": "https://miro.medium.com/0*NnDr6FUWQ-3R8BSD.", "user_profile": "\n        \u003cb\u003eBIO\u003c/b\u003e: Data Scientist @ Persado || \ud83e\udd47Top Writer in Artificial Intelligence and Time Series \u003cbr\u003e\n\n        \u003cb\u003eArticles\u003c/b\u003e: 30 (31489 stemmed words) \u003cbr\u003e\n        \u003cb\u003eTop article\u003c/b\u003e: \u003ca href=\u0027https://towardsdatascience.com/the-best-deep-learning-models-for-time-series-forecasting-690767bc63f0?gi=3a7e472d8f8c\u0027\u003eThe Best Deep Learning Models for Time Series Forecasting (Towards Data Science)\u003c/a\u003e \u003cbr\u003e\n\n        \u003cb\u003ePublications\u003c/b\u003e: Towards Data Science(19), ILLUMINATION(3), Better Programming(3), Geek Culture(2), Towards AI(1), Better Marketing(1), New Writers Welcome(1) \u003cbr\u003e\n        \u003cb\u003eFollowers\u003c/b\u003e: 1939 \u003cbr\u003e\n        \n        \u003cb\u003eVoters - Followers % (Article AVG)\u003c/b\u003e: 3.2%\u003cbr\u003e\n        \u003cb\u003eClaps per Person (Article AVG)\u003c/b\u003e: 17.6\u003cbr\u003e\n        \u003cbr\u003e\n        \n        \u003cb\u003ePreferred Published Time\u003c/b\u003e: afternoon-early(7), evening-early(7), night-early(5), afternoon-late(5), evening-late(3), morning-morning(3) \u003cbr\u003e\n        \u003cb\u003ePreferred Article Length (stemmed)\u003c/b\u003e: medium(14), large(11), normal(3) \u003cbr\u003e\n        \u003cb\u003ePublished Frequency (AVG)\u003c/b\u003e: per 19.2 days (2021-09-14/2023-03-24) \u003cbr\u003e\n        \u003cb\u003eLast Seen \u003c/b\u003e: before 12 days\u003cbr\u003e\n\n        \u003cb\u003eExternal Domains per Article \u003c/b\u003e: 5.2\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 56.7% (31489 / 55572)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 10.0% (5564 / 55572)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 15.5% (4893 / 31489)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 16.4% (9100 / 55572)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 11.4% (6354 / 55572)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 32.5% (18062 / 55572)\u003cbr\u003e\n        \u003cbr\u003e\n        \n        \u003cb\u003eMost Common ChatGPT Keywords (UPA)\u003c/b\u003e:\u003cbr\u003e deep(15), learning(15), time(10), series(10), forecasting(9), model(6), transformers(6), data(6), tensorflow(6), attention(5), gpu(5), classification(5), interpretability(4), lstms(4), recognition(4), installation(4), deepar(3), copula(3), processing(3), neural(3), variables(3), distribution(3), sources(3), performance(3), training(3), web(3), types(3), python(3), writing(3), medium(3)\u003cbr\u003e\u003cbr\u003e\n\n        \u003cb\u003eMost Common Words (UPA)\u003c/b\u003e:\u003cbr\u003e also(30), let(29), first(28), closing(27), however(27), remarks(26), data(26), example(26), one(25), new(25), thank(25), time(25), reading(25), see(24), many(23), since(23), check(22), article(22), deep(22), linkedin(22), may(22), next(21), two(21), better(21), well(21), even(20), subscribe(20), best(20), instance(20), later(20)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams (UPA)\u003c/b\u003e:\u003cbr\u003e based first(17), remarks one(14), next given(11), information closing(11), even multiple(9), like follow(9), references two(9), instead set(8), second remarks(8), two various(8), would see(8), per data(7), published instance(7), found original(7), novel context(7)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams (UPA)\u003c/b\u003e:\u003cbr\u003e novel context since(5), neural capture words(4), lookback deep company(4), domain found original(4), datasets second remarks(4), important next given(3), normalize forecasting example(3), find references gpu(3), references gpu two(3), based first lightning(3)\u003cbr\u003e\u003cbr\u003e\n        \n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e time(412), models(298), series(267), use(236), model(207), data(188), also(187), learning(184), forecasting(178), let(136), deep(135), first(130), using(126), one(122), copula(119), figure(113), transformers(109), gaussian(105), attention(100), example(99), step(98), dataset(97), article(96), training(95), function(95), conda(92), follow(91), distribution(91), modeling(91), input(88)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e time series(252), deep learning(74), series forecasting(56), gaussian copula(42), temporal fusion(40), shown figure(30), multiple time(29), deep gpvar(28), zero shot(28), closing remarks(25), self attention(25), thank reading(24), data science(24), fusion transformers(23), learning models(21)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e time series forecasting(56), multiple time series(26), temporal fusion transformers(23), subscribe newsletter follow(19), newsletter follow linkedin(19), temporal fusion transformer(17), deep learning models(16), time series dataset(14), probability integral transformer(12), block recurrent transformers(12)\u003cbr\u003e\u003cbr\u003e\n        "};
    var nodes = new vis.DataSet(data.nodes)
    var edges = new vis.DataSet(data.edges)
    var data = {nodes: nodes, edges: edges};

    // create a network
    var container = document.getElementById("mynetwork");
    var options = {
        nodes: {
            font: {size: 18},
            color: "#ffffff",
            borderWidth: 2,
        },
        edges: {
            width: 2,
        },
        layout: {
            improvedLayout: false,
            randomSeed: 191006
        },

    };

    

    var network = new vis.Network(container, data, options);

    // add hyperlink and info panel to nodes
    document.getElementById('infopanel').innerHTML = '<h1 class="al">Explore <a href="https://medium.com/@nikoskafritsas" style="color: blue;">nikoskafritsas\'s</a> Medium.com sky &#128640;</h1>';
    document.getElementById('infopanel').innerHTML += '<div class="al"><img id="profile" src="https://miro.medium.com/0*NnDr6FUWQ-3R8BSD."</img></div>';
    document.getElementById('infopanel').innerHTML += `<br>
        <b>BIO</b>: Data Scientist @ Persado || Top Writer in Artificial Intelligence and Time Series <br>

        <b>Articles</b>: 30 (31489 stemmed words) <br>
        <b>Top article</b>: <a href='https://towardsdatascience.com/the-best-deep-learning-models-for-time-series-forecasting-690767bc63f0?gi=3a7e472d8f8c'>The Best Deep Learning Models for Time Series Forecasting (Towards Data Science)</a> <br>

        <b>Publications</b>: Towards Data Science(19), ILLUMINATION(3), Better Programming(3), Geek Culture(2), Towards AI(1), Better Marketing(1), New Writers Welcome(1) <br>
        <b>Followers</b>: 1939 <br>
        
        <b>Voters - Followers % (Article AVG)</b>: 3.2%<br>
        <b>Claps per Person (Article AVG)</b>: 17.6<br>
        <br>
        
        <b>Preferred Published Time</b>: afternoon-early(7), evening-early(7), night-early(5), afternoon-late(5), evening-late(3), morning-morning(3) <br>
        <b>Preferred Article Length (stemmed)</b>: medium(14), large(11), normal(3) <br>
        <b>Published Frequency (AVG)</b>: per 19.2 days (2021-09-14/2023-03-24) <br>
        <b>Last Seen </b>: before 12 days<br>

        <b>External Domains per Article </b>: 5.2<br>

        <br>
        <b>Stemmed words / words</b>: 56.7% (31489 / 55572)<br>
        <b>Unique words / words</b>: 10.0% (5564 / 55572)<br>
        <b>Unique words / words (stemmed)</b>: 15.5% (4893 / 31489)<br>
        <b>Verb / words</b>: 16.4% (9100 / 55572)<br>
        <b>Adj / words</b>: 11.4% (6354 / 55572)<br>
        <b>Noun / words</b>: 32.5% (18062 / 55572)<br>
        <br>
        
        <b>Most Common ChatGPT Keywords (UPA)</b>:<br> deep(15), learning(15), time(10), series(10), forecasting(9), model(6), transformers(6), data(6), tensorflow(6), attention(5), gpu(5), classification(5), interpretability(4), lstms(4), recognition(4), installation(4), deepar(3), copula(3), processing(3), neural(3), variables(3), distribution(3), sources(3), performance(3), training(3), web(3), types(3), python(3), writing(3), medium(3)<br><br>

        <b>Most Common Words (UPA)</b>:<br> also(30), let(29), first(28), closing(27), however(27), remarks(26), data(26), example(26), one(25), new(25), thank(25), time(25), reading(25), see(24), many(23), since(23), check(22), article(22), deep(22), linkedin(22), may(22), next(21), two(21), better(21), well(21), even(20), subscribe(20), best(20), instance(20), later(20)<br><br>
        <b>Most Common Bigrams (UPA)</b>:<br> based first(17), remarks one(14), next given(11), information closing(11), even multiple(9), like follow(9), references two(9), instead set(8), second remarks(8), two various(8), would see(8), per data(7), published instance(7), found original(7), novel context(7)<br><br>
        <b>Most Common Trigrams (UPA)</b>:<br> novel context since(5), neural capture words(4), lookback deep company(4), domain found original(4), datasets second remarks(4), important next given(3), normalize forecasting example(3), find references gpu(3), references gpu two(3), based first lightning(3)<br><br>
        
        <b>Most Common Words</b>:<br> time(412), models(298), series(267), use(236), model(207), data(188), also(187), learning(184), forecasting(178), let(136), deep(135), first(130), using(126), one(122), copula(119), figure(113), transformers(109), gaussian(105), attention(100), example(99), step(98), dataset(97), article(96), training(95), function(95), conda(92), follow(91), distribution(91), modeling(91), input(88)<br><br>
        <b>Most Common Bigrams</b>:<br> time series(252), deep learning(74), series forecasting(56), gaussian copula(42), temporal fusion(40), shown figure(30), multiple time(29), deep gpvar(28), zero shot(28), closing remarks(25), self attention(25), thank reading(24), data science(24), fusion transformers(23), learning models(21)<br><br>
        <b>Most Common Trigrams</b>:<br> time series forecasting(56), multiple time series(26), temporal fusion transformers(23), subscribe newsletter follow(19), newsletter follow linkedin(19), temporal fusion transformer(17), deep learning models(16), time series dataset(14), probability integral transformer(12), block recurrent transformers(12)<br><br>
        `;

    document.getElementById('infopanel').innerHTML += '<div class="al"><h3>&mdash; Description &mdash;</h3></div>';
    document.getElementById('infopanel').innerHTML += `<div><p>This is an analysis of Medium.com articles. The analysis includes the use of knowledge graphs to display the relationships between the articles and the external website domains referenced within them. Articles are represented by stars and external website domains by circles. Additionally, NLP techniques such as stemming and frequency analysis were utilized to gain a better understanding of the articles content.</p><p>For more information visit <a href="https://github.com/justdataplease/medium-sky">Github repo</a>.</p></div>`;

    network.on("click", function (params) {
        if (params.nodes.length === 1) {
            var node = nodes.get(params.nodes[0]);
            if (node && node.url) {
                var container = document.getElementById('infopanel');
                container.innerHTML = '';

                if (node.main_title != undefined) {
                    container.innerHTML += '<h3>' + node.main_title + '</h3>'
                    container.innerHTML += '<a href="' + node.url + '">' + node.url + '</a>';

                    container.innerHTML += '<h3>&mdash; Word Analysis &mdash; </h3>'
                    container.innerHTML += node.stats
                } else {
                    container.innerHTML += "<p>Domain &mdash; " + '<a href="https://' + node.domain + '">' + node.domain + "</a></p><br>";
                }
                ;


                container.innerHTML += "<h3>&mdash; Urls &mdash;</h3>"

                container.innerHTML += '<ol type="1">'

                for (var i = 0; i < node.urls.length; i++) {
                    let x = node.urls[i].split('|')
                    container.innerHTML += '<li>Title : ' + x[0];
                    container.innerHTML += '<a href="' + x[1] + '">' + x[1] + '</a></li><br>';
                }
                container.innerHTML += "</ol>";


            } else {
                document.getElementById("infopanel").innerHTML = "";
            }
        }
    });

</script>

</body>
</html>