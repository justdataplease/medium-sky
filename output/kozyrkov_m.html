<!DOCTYPE html>
<html>
<head>
    <title>Interactive Knowledge Graph with Hyperlinks, Descriptions, and Info Panel</title>
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script type="text/javascript" src="https://unpkg.com/vis-network/standalone/umd/vis-network.min.js"></script>
    <style type="text/css">
        body {
            margin: 0;
            padding: 0;
            font-family: Arial, sans-serif;
            background-color: #2e4482;
        }

        #mynetwork {
            width: calc(100% - 200px);
            height: 100%;
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        #infopanel {
            position: absolute;
            top: 0;
            right: 0;
            width: 500px;
            height: 100%;
            padding: 10px;
            overflow: auto;
            background-color: #ffffff;
            box-sizing: border-box;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.2);
        }

        #infopanel a {
            color: #2B7CE9;
            text-decoration: none;
            font-weight: bold;
        }

        #infopanel a:hover {
            text-decoration: underline;
        }

        #graphpanel {
            position: absolute;
            top: 0;
            left: 0;
            width: calc(100% - 200px);
            height: 100%;
            box-sizing: border-box;
            background-color: #f5f5f5;
        }

        #profile {
            border-radius: 1vh;
            width: 30%;
            height: auto;
        }

        .al {
            text-align: center;
        }

    </style>
</head>
<body>
<div id="graphpanel">
    <div id="mynetwork"></div>
</div>
<div id="infopanel"></div>
<script type="text/javascript">
    var data = {"edges": [{"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 1}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100002, "to": 1}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100003, "to": 1}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100004, "to": 1}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100005, "to": 1}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100006, "to": 1}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100007, "to": 1}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 1}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100009, "to": 2}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 2}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100010, "to": 2}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100011, "to": 2}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100012, "to": 2}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100013, "to": 2}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100014, "to": 2}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100005, "to": 2}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100006, "to": 2}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100007, "to": 2}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 2}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100010, "to": 3}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100015, "to": 3}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 3}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100009, "to": 3}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100016, "to": 3}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100013, "to": 3}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100017, "to": 3}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100018, "to": 3}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 4}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100010, "to": 4}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100015, "to": 4}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100013, "to": 4}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100017, "to": 4}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100019, "to": 4}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 15, "to": 4}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100014, "to": 4}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100005, "to": 4}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100006, "to": 4}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100007, "to": 4}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 4}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 5}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100020, "to": 5}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100021, "to": 5}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100022, "to": 5}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 29, "to": 5}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100023, "to": 5}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 14, "to": 5}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100005, "to": 5}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100006, "to": 5}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100007, "to": 5}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 5}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100014, "to": 5}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 6}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 6}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100014, "to": 6}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100005, "to": 6}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100006, "to": 6}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100007, "to": 6}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 7}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100005, "to": 7}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100004, "to": 7}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100014, "to": 7}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 8}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100024, "to": 8}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100014, "to": 8}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 9}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100025, "to": 9}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100021, "to": 9}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100014, "to": 9}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 10}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100006, "to": 10}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100026, "to": 10}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100027, "to": 10}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100028, "to": 10}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100029, "to": 10}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100005, "to": 10}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100004, "to": 10}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100014, "to": 10}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100007, "to": 10}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 10}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100030, "to": 11}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 11}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 13, "to": 11}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 14, "to": 11}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 12, "to": 11}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100022, "to": 11}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100005, "to": 11}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100006, "to": 11}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100007, "to": 11}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 11}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100025, "to": 12}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100031, "to": 12}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 12}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100032, "to": 12}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100022, "to": 12}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100004, "to": 12}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100005, "to": 12}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 13, "to": 12}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 14, "to": 12}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100006, "to": 12}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100007, "to": 12}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 12}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100025, "to": 13}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 13}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 13}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100005, "to": 13}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 14, "to": 13}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100022, "to": 13}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100006, "to": 13}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100007, "to": 13}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 14}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100022, "to": 14}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100004, "to": 14}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100005, "to": 14}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100006, "to": 14}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100007, "to": 14}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 14}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100027, "to": 15}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 15}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100033, "to": 15}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100034, "to": 15}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 16}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100035, "to": 16}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100021, "to": 16}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 16}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100014, "to": 16}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100017, "to": 17}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 17}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100004, "to": 17}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100036, "to": 17}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100022, "to": 17}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100014, "to": 17}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100037, "to": 18}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 18}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 18}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100021, "to": 18}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100022, "to": 18}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100014, "to": 18}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 22, "to": 18}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 19}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100038, "to": 19}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100039, "to": 19}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100021, "to": 19}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100014, "to": 19}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 20}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100004, "to": 20}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100014, "to": 20}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 20}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100005, "to": 20}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100006, "to": 20}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100007, "to": 20}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 21}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100040, "to": 21}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100014, "to": 21}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 22}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 22}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100025, "to": 22}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100021, "to": 22}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100022, "to": 22}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100005, "to": 22}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100006, "to": 22}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100007, "to": 22}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100014, "to": 22}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 23}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100035, "to": 23}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100004, "to": 23}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100022, "to": 23}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100005, "to": 23}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100006, "to": 23}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100007, "to": 23}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 23}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100014, "to": 23}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 24}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100021, "to": 24}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100035, "to": 24}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100006, "to": 24}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100022, "to": 24}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100005, "to": 24}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100007, "to": 24}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 24}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100014, "to": 24}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 25}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100004, "to": 25}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100017, "to": 26}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 26}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100005, "to": 26}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100022, "to": 26}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100006, "to": 26}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100007, "to": 26}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 26}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100004, "to": 27}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 27}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100005, "to": 27}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 29, "to": 27}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 28, "to": 27}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100022, "to": 27}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100006, "to": 27}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100007, "to": 27}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 27}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 28}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100022, "to": 28}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100005, "to": 28}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100006, "to": 28}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100007, "to": 28}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 28}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100004, "to": 29}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 29}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100022, "to": 29}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100005, "to": 29}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100006, "to": 29}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100007, "to": 29}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 29}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100001, "to": 30}, {"color": {"color": "#A7C7E7", "highlight": "#3c82ca"}, "font": {"color": "#808080", "size": 10}, "from": 20, "to": 30}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100004, "to": 30}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100041, "to": 30}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100021, "to": 30}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100022, "to": 30}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100005, "to": 30}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100006, "to": 30}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100007, "to": 30}, {"color": {"color": "#dbd7d7", "highlight": "#9a8f8f"}, "font": {"color": "#808080", "size": 10}, "from": 100008, "to": 30}], "nodes": [{"color": "#fdfd96", "counter": 1, "description": "Putting the fun in fundamentals! A collection of short videos to amuse beginners and experts alike", "domain": "https://kozyrkov.medium.com/everything-youve-ever-wanted-to-know-about-machine-learning-b396b0abee8c", "font": {"color": "#000000", "size": 20}, "id": 1, "label": "Everything you\u0027ve ev", "main": 1, "main_title": "Everything you\u0027ve ever wanted to know about machine learning", "shape": "star", "size": 34.72222222222222, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Everything you\u0027ve ever wanted to know about machine learning\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Putting the fun in fundamentals! A collection of short videos to amuse beginners and experts alike\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e This is a free, lovable YouTube course called \"Making Friends with Machine Learning\" (MFML) designed for everyone. The course is composed of short-form videos that are fun, informative, and easy to understand that caters to both beginners and experts. The videos are available in bite-sized versions (a few minutes long) for quick refreshers or long-form versions (up to an hour long) for a deeper dive. The topics covered include the basic concepts and algorithms in machine learning, data science, decision-making, model validation and debugging, and productionization. The course also covers advanced topics such as deep learning and neural networks. Completing this course will help viewers gain an intuitive and correct understanding of core machine learning concepts, avoid common errors, and communicate effectively with experts and non-experts. \n\nKEYWORDS\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027https://kozyrkov.medium.com/\u0027\u003eMedium\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-08-19 (\u0027afternoon\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.2%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 6.4 (222 / 1413)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 14\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1811\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 1118 (large)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 61.7% (1118 / 1811)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 41.0% (742 / 1811)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 31.8% (742 / 1811)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 16.2% (293 / 1811)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 10.4% (188 / 1811)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 32.5% (589 / 1811)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e machine learning, youtube course, beginners, experts, bite-sized, long-form, basic concepts, algorithms, data science, decision-making, model validation, debugging, productionization, deep learning, neural networks\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e mfml(128), steps(69), learning(31), machine(18), practices(18), course(16), basics(13), use(11), regression(9), understanding(9), data(8), models(8), engineer(7), testing(6), linear(6), validation(6), video(5), bite(5), liked(5), feature(5), training(5), decision(5), experts(4), introduction(4), make(4), friends(4), everyone(4), form(4), long(4), algorithms(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e steps mfml(65), machine learning(18), practices mfml(16), basics mfml(12), learning mfml(7), linear regression(5), learning steps(4), regression mfml(4), neural networks(4), bite sized(3), decision trees(3), deep learning(3), knowing machine(2), experts alike(2), make friends(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e learning steps mfml(4), linear regression mfml(3), deep learning mfml(3), knowing machine learning(2), make friends machine(2), friends machine learning(2), machine learning basics(2), learning basics mfml(2), mfml 006 simple(2), 006 simple linear(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://kozyrkov.medium.com/everything-youve-ever-wanted-to-know-about-machine-learning-b396b0abee8c", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "A first look at two major AI releases", "domain": "https://towardsdatascience.com/unboxing-google-bard-and-gpt-4-811896adf0e2?gi=4c18970079f5", "font": {"color": "#000000", "size": 20}, "id": 2, "label": "Unboxing Google Bard", "main": 1, "main_title": "Unboxing Google Bard and GPT-4", "shape": "star", "size": 30.666666666666668, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Unboxing Google Bard and GPT-4\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: A first look at two major AI releases\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The text is discussing two major releases by Google, the Google Bard and GPT, both of which are demonstrated side by side. The Google Bard is powered by the Lambda model, while GPT is available for free with a subscription to ChatGPT Plus. The video highlights the controversial question of epistemology and even asks both LLM teams which philosopher they prefer. ChatGPT is described as an efficient conversationalist designed to answer questions thoroughly, and users need to keep the conversation going. Finally, the author notes that personal preference matters in choosing an LLM, and there is no right answer.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2023-03-29 (\u0027evening\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 8.8 (76 / 666)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 9\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 2282\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 1061 (large)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 46.5% (1061 / 2282)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 32.5% (742 / 2282)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 22.8% (742 / 2282)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 19.5% (444 / 2282)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 9.4% (214 / 2282)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 23.4% (533 / 2282)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e google bard, gpt, unboxing, side-by-side, prompts, chatgpt, llm, controversy, epistemology, kant, hume\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e bard(17), conversational(17), one(17), chatgpt(15), prompts(14), liked(14), get(13), llm(13), thing(12), side(11), answers(10), responses(10), video(9), tried(9), opinion(9), team(9), hume(9), different(9), question(8), personally(8), gpt(7), first(7), might(7), right(7), epistemologically(7), looking(6), coming(6), showing(6), use(6), need(6)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e team hume(4), google bard(3), side side(3), controversial question(3), question epistemologically(3), one side(3), team kant(3), right answers(3), might liked(3), different llm(3), thing personally(3), personally preferred(3), bard gpt(2), cassie kozyrkov(2), go showing(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e controversial question epistemologically(3), question epistemologically make(2), epistemologically make argument(2), make argument one(2), argument one side(2), one side ask(2), side ask think(2), team kant team(2), kant team hume(2), another might liked(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/unboxing-google-bard-and-gpt-4-811896adf0e2?gi=4c18970079f5", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Let\u0027s take a look at the hottest AI tech released in the past week", "domain": "https://kozyrkov.medium.com/breaking-google-bard-and-gpt-4-354acebe545c", "font": {"color": "#000000", "size": 20}, "id": 3, "label": "Breaking: Google Bar", "main": 1, "main_title": "Breaking: Google Bard and GPT-4", "shape": "star", "size": 32.361111111111114, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Breaking: Google Bard and GPT-4\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Let\u0027s take a look at the hottest AI tech released in the past week\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e This text highlights the release of two large language models (LLMs), GPT-3 and BARD, from Google and OpenAI respectively. These LLMs represent breakthroughs in productivity and personal growth opportunities for those with access. However, getting started with these tools can be a challenge due to costs and waitlists. The text provides a brief explanation of what LLMs are and how they relate to Google\u0027s ChatGPT and Google\u0027s LLM called Bard. The appendix includes a cheatsheet for the acronyms mentioned in the text.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027https://kozyrkov.medium.com/\u0027\u003eMedium\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2023-03-22 (\u0027afternoon\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 6.4 (137 / 878)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 20\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 610\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 283 (normal)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 46.4% (283 / 610)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 47.0% (287 / 610)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 32.8% (287 / 610)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 18.9% (115 / 610)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 10.7% (65 / 610)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 21.5% (131 / 610)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e google, bard, gpt, large language model, productivity, personal growth, waitlist, technology, subscriptions, acronyms \nsummary\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e llm(11), video(7), everyone(5), bard(4), gpt(4), play(4), try(4), might(4), google(3), let(3), one(3), acronyms(3), least(3), interfaces(3), specific(3), chatgpt(3), looking(2), releases(2), week(2), last(2), days(2), large(2), language(2), model(2), mar(2), 2023(2), think(2), yes(2), know(2), explain(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e specific llm(3), last days(2), large language(2), language model(2), mar 2023(2), everyone everyone(2), might even(2), video show(2), llm play(2), breaking google(1), google bard(1), bard gpt(1), gpt let(1), let take(1), take looking(1)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e large language model(2), breaking google bard(1), google bard gpt(1), bard gpt let(1), gpt let take(1), let take looking(1), take looking hottest(1), looking hottest tech(1), hottest tech releases(1), tech releases past(1)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://kozyrkov.medium.com/breaking-google-bard-and-gpt-4-354acebe545c", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "(Besides the fact that you don\u0027t need technical skills to do it)", "domain": "https://kozyrkov.medium.com/why-everyone-should-try-gpt-4-even-the-ceo-1a00367c4c12", "font": {"color": "#000000", "size": 20}, "id": 4, "label": "Why everyone should ", "main": 1, "main_title": "Why everyone should try GPT-4, even the CEO", "shape": "star", "size": 41.52777777777778, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Why everyone should try GPT-4, even the CEO\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: (Besides the fact that you don\u0027t need technical skills to do it)\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article talks about the growing popularity of large language models (LLMs) such as GPT and Lamda, and their potential for increasing productivity. The author urges readers to try out these technologies personally before deciding whether they are useful or not. However, the author also cautions that LLMs are not perfect and may make factual errors, so users should avoid putting confidential or sensitive information through them. The article concludes by emphasizing the need to understand the potential and limitations of LLMs and to explore their user experience in order to make an informed decision about their adoption.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027https://kozyrkov.medium.com/\u0027\u003eMedium\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2023-03-20 (\u0027night\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.3%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 8.2 (467 / 3847)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 61\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1184\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 553 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 46.7% (553 / 1184)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 41.9% (496 / 1184)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 30.2% (496 / 1184)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 21.2% (251 / 1184)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 9.1% (108 / 1184)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 21.7% (257 / 1184)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e gpt, lamda, large language models, productivity, technology, adoption, potential, limitations, user experience, revolution\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e gpt(11), llms(11), tried(9), one(9), use(8), lamda(7), get(6), text(6), comes(5), technology(5), even(4), ceo(4), needs(4), chatgpt(4), free(4), instead(4), reading(4), time(4), hands(4), productivity(4), article(4), see(4), potential(4), fact(3), released(3), month(3), plus(3), impressed(3), many(3), people(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e gpt lamda(5), chatgpt plus(3), tried gpt(2), even ceo(2), stop reading(2), spend time(2), time get(2), get hands(2), hands dirty(2), dirty feel(2), feel free(2), suite needs(2), half article(2), large language(2), language models(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e spend time get(2), time get hands(2), get hands dirty(2), hands dirty feel(2), large language models(2), language models llms(2), llms technology enormous(2), technology enormous potential(2), enormous potential matter(2), potential matter would(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://kozyrkov.medium.com/why-everyone-should-try-gpt-4-even-the-ceo-1a00367c4c12", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Motivation and career insights I wish I hadn\u0027t had to work out for myself", "domain": "https://kozyrkov.medium.com/the-best-advice-i-never-got-for-a-technical-career-30a524a5a5dc", "font": {"color": "#000000", "size": 20}, "id": 5, "label": "The best advice I ne", "main": 1, "main_title": "The best advice I never got (for a technical career)", "shape": "star", "size": 32.69444444444444, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: The best advice I never got (for a technical career)\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Motivation and career insights I wish I hadn\u0027t had to work out for myself\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article is a compilation of data science career insights and helpful articles on topics such as impostor syndrome, stakeholder management, organizational culture, public speaking, motivation, and decision-making. The author shares her personal experiences and offers practical advice and tips for handling tricky interview questions, dealing with impostor syndrome, and giving effective technical talks. The article also features tutorials and tools for practicing data science skills, such as automating machine learning and classifying text and images. Lastly, the author offers advice on making new year resolutions and setting achievable goals for personal and professional growth.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027https://kozyrkov.medium.com/\u0027\u003eMedium\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2023-03-16 (\u0027afternoon\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 7.4 (149 / 1105)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 3\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 843\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 397 (normal)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 47.1% (397 / 843)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 44.4% (374 / 843)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 30.7% (374 / 843)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 19.6% (165 / 843)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 9.7% (82 / 843)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 25.1% (212 / 843)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e data science, career insights, impostor syndrome, public speaking, motivation, decision-making, stakeholder management, organizational culture, new year resolutions, tutorials\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e data(13), science(11), career(7), article(5), make(5), help(5), technical(4), get(4), interview(4), questions(4), job(4), impostor(4), ask(4), guide(4), speak(4), best(3), motivation(3), working(3), topics(3), hard(3), use(3), deal(3), skills(3), let(3), new(3), year(3), looking(3), hacks(3), talks(3), resolutions(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e data science(11), science career(3), technical career(2), article written(2), deal tricky(2), interview questions(2), impostor syndrome(2), make sure(2), hacks motivation(2), avoid equations(2), new year(2), year resolutions(2), best advice(1), advice never(1), never got(1)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e data science career(3), new year resolutions(2), best advice never(1), advice never got(1), never got technical(1), got technical career(1), technical career motivation(1), career motivation career(1), motivation career insights(1), career insights wish(1)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://kozyrkov.medium.com/the-best-advice-i-never-got-for-a-technical-career-30a524a5a5dc", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "The industry-wide neglect of data design and data quality (and what you can do about it)", "domain": "https://towardsdatascience.com/heres-why-your-efforts-extract-value-from-data-are-going-nowhere-8e4ffacbdbc0?gi=ce9b33a84f1d", "font": {"color": "#000000", "size": 20}, "id": 6, "label": "Here\u0027s why your effo", "main": 1, "main_title": "Here\u0027s why your efforts to extract value from data are going nowhere", "shape": "star", "size": 39.111111111111114, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Here\u0027s why your efforts to extract value from data are going nowhere\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: The industry-wide neglect of data design and data quality (and what you can do about it)\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The text discusses the importance of data quality in the data science and data engineering fields, and how it is a commonly neglected aspect of the industry. The author emphasizes that making good data is an art and that investing in good data is essential for successful projects. The article calls for a focus on data quality and recognition of the importance of specialized skills like data labeling, data curation, and data design. The author suggests the need for incentives for individuals responsible for data quality and education and resources to encourage people to pursue and appreciate these specialist roles. \n\nKEYWORDS\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2023-02-25 (\u0027evening\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.3%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 6.1 (380 / 2312)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 38\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1673\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 833 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 49.8% (833 / 1673)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 35.7% (598 / 1673)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 26.4% (598 / 1673)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 18.8% (315 / 1673)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 7.7% (129 / 1673)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 29.9% (500 / 1673)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e data quality, data engineering, data science, data labeling, data curation, specialized skills, data design, documentation, incentives, education\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e data(71), design(13), quality(13), job(13), make(11), skills(10), use(9), engineers(8), science(7), good(7), needs(6), say(6), whole(6), question(6), let(5), collection(5), garbage(5), well(5), gigo(5), managed(5), kind(5), careers(5), excellence(5), experience(5), specialists(5), neglect(4), first(4), new(4), person(4), documentation(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e data quality(10), data engineers(7), design data(5), data science(5), make data(5), data design(4), neglect data(3), engineers data(3), first place(3), good data(3), data scientists(3), product managed(3), data excellence(3), data labeling(3), data collection(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e data design data(3), data cards playbook(3), data engineers data(2), make data usable(2), make data first(2), data first place(2), investing good data(2), exactly kind everybody(2), kind everybody job(2), everybody job ends(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/heres-why-your-efforts-extract-value-from-data-are-going-nowhere-8e4ffacbdbc0?gi=ce9b33a84f1d", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "The bias-variance tradeoff, part 1 of 3", "domain": "https://towardsdatascience.com/is-there-always-a-tradeoff-between-bias-and-variance-5ca44398a552?gi=93f1b68c8d2a", "font": {"color": "#000000", "size": 20}, "id": 7, "label": "Is There Always a Tr", "main": 1, "main_title": "Is There Always a Tradeoff Between Bias and Variance?", "shape": "star", "size": 30.694444444444443, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Is There Always a Tradeoff Between Bias and Variance?\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: The bias-variance tradeoff, part 1 of 3\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article discusses the concept of bias-variance tradeoff, a popular term in statistics, and how it is related to mean squared error (MSE), a commonly used metric to evaluate the performance of predictive models. It also explains the practical recipe for selecting the right model complexity, tuning hyperparameters, and regularization, and the importance of minimizing loss function. The article emphasizes the need to strike a balance between bias and variance to achieve a better-performing model. It further highlights the problem of overfitting and the importance of choosing the right MSE score to measure the model\u0027s performance. In conclusion, the article recommends a beginner-friendly course on data science and machine learning automation to enhance one\u0027s learning.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2023-02-15 (\u0027afternoon\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 9.2 (77 / 707)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 3\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 820\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 387 (normal)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 47.2% (387 / 820)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 40.4% (331 / 820)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 27.4% (331 / 820)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 19.4% (159 / 820)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 10.4% (85 / 820)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 23.8% (195 / 820)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e bias variance tradeoff, mean squared error, model complexity, regularization, loss function, model performance, overfitting, data science, machine learning, beginner-friendly course, automation\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e mse(17), variance(13), model(13), bias(12), tradeoff(7), like(5), better(5), might(4), terms(4), make(4), perfect(4), archer(4), saying(4), alike(4), unhappy(4), understanding(3), word(3), want(3), formula(3), bias\u00b2(3), needed(3), error(3), loss(3), function(3), one(3), take(3), let(3), data(3), two(3), looking(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e bias variance(8), perfect model(4), tradeoff bias(3), variance tradeoff(3), mse bias\u00b2(3), bias\u00b2 variance(3), loss function(3), model alike(3), variance bias(2), might think(2), formula mse(2), mse might(2), standard deviation(2), became better(2), better archer(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e tradeoff bias variance(3), bias variance tradeoff(3), mse bias\u00b2 variance(3), perfect model alike(3), formula mse bias\u00b2(2), became better archer(2), tolstoy would saying(2), would saying perfect(2), saying perfect model(2), model alike unhappy(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/is-there-always-a-tradeoff-between-bias-and-variance-5ca44398a552?gi=93f1b68c8d2a", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "The bias-variance tradeoff, part 2 of 3", "domain": "https://towardsdatascience.com/overfitting-underfitting-and-regularization-7f83dd998a62?gi=f85e9cfc8b91", "font": {"color": "#000000", "size": 20}, "id": 8, "label": "Overfitting, Underfi", "main": 1, "main_title": "Overfitting, Underfitting, and Regularization", "shape": "star", "size": 30.0, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Overfitting, Underfitting, and Regularization\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: The bias-variance tradeoff, part 2 of 3\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article covers much of the basic terminology on overfitting, underfitting, and regularization in machine learning. It also provides key insights on the bias-variance tradeoff, the formula for MSE (mean squared error), and the importance of the quantity and quality of data for model performance. The article suggests taking a look at the previous part to make sure that the reader is well situated to absorb this one on overfitting and underfitting. It explains that the best model will have unavoidable errors, and that overfitting occurs when the model fits the noise in the data and is too complicated. On the other hand, underfitting occurs when the model is too simple to capture the complexities in the data. The article suggests regularization to prevent overfitting due to extra complexity in the model.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2023-02-15 (\u0027afternoon\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.0%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 8.5 (52 / 440)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 5\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 755\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 340 (normal)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 45.0% (340 / 755)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 39.7% (300 / 755)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 26.8% (300 / 755)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 21.3% (161 / 755)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 8.9% (67 / 755)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 23.7% (179 / 755)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e overfitting, underfitting, regularization, bias, variance tradeoff, mse, data, model, complexity, training set\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e model(18), mse(9), get(9), data(8), variance(6), training(6), regularizes(5), bias(5), information(5), boss(5), part(4), says(4), go(4), complexity(4), overfitting(3), underfitting(3), well(3), perfect(3), making(3), looking(3), better(3), errors(3), reality(3), learning(3), use(3), trying(3), set(3), complicated(3), called(3), score(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e bias variance(4), training mse(3), overfitting underfitting(2), variance tradeoff(2), go get(2), get information(2), learning model(2), mse low(2), reality get(2), model excellent(2), excellent training(2), underfitting regularizes(1), regularizes bias(1), tradeoff part(1), part part(1)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e bias variance tradeoff(2), model excellent training(2), overfitting underfitting regularizes(1), underfitting regularizes bias(1), regularizes bias variance(1), variance tradeoff part(1), tradeoff part part(1), part part covered(1), part covered much(1), covered much basic(1)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/overfitting-underfitting-and-regularization-7f83dd998a62?gi=f85e9cfc8b91", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "The bias-variance tradeoff, part 3 of 3", "domain": "https://towardsdatascience.com/the-bias-variance-tradeoff-explained-2d1311c2b7c2?gi=68a19a274735", "font": {"color": "#000000", "size": 20}, "id": 9, "label": "The Bias-Variance Tr", "main": 1, "main_title": "The Bias-Variance Tradeoff, Explained", "shape": "star", "size": 30.555555555555557, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: The Bias-Variance Tradeoff, Explained\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: The bias-variance tradeoff, part 3 of 3\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article discusses the bias-variance tradeoff and its importance in finding the optimal level of complexity for a model. It covers the basics of overfitting, underfitting, and regularization and emphasizes the need for a validation set and proper data splitting to accurately measure a model\u0027s performance. The goal is to achieve a balance between bias and variance that will result in the best possible model. The article also promotes the idea of starting from a simple model and gradually increasing complexity while observing changes in mean squared error and validation performance.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2023-02-15 (\u0027afternoon\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 7.1 (72 / 510)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 5\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 737\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 357 (normal)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 48.4% (357 / 737)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 41.0% (302 / 737)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 27.1% (302 / 737)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 19.9% (147 / 737)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 9.4% (69 / 737)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 25.4% (187 / 737)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e bias variance tradeoff, overfitting, underfitting, regularization, model performance, model complexity, validation set, test set, data splitting, tuning regularization\n\nsummary\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e models(12), bias(10), performance(9), variance(8), set(7), tradeoff(6), part(6), training(6), data(6), better(5), http(4), bit(4), regularized(4), things(4), applied(4), made(4), understand(3), course(3), overfitting(3), underfitting(3), best(3), one(3), well(3), complexity(3), without(3), improve(3), debugging(3), way(3), splitting(3), standard(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e bias variance(7), variance tradeoff(6), part http(4), http bit(4), models performance(3), training performance(3), well models(3), standard deviation(3), made things(3), tradeoff part(2), bit quaesita_bivar1(2), bit quaesita_bivar2(2), need know(2), overfitting underfitting(2), complexity ratchet(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e bias variance tradeoff(6), part http bit(4), variance tradeoff part(2), http bit quaesita_bivar1(2), http bit quaesita_bivar2(2), without improve real(2), improve real performance(2), real performance happens(2), performance happens applied(2), happens applied models(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/the-bias-variance-tradeoff-explained-2d1311c2b7c2?gi=68a19a274735", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "AI Art: Beauty And The Bias", "domain": "https://towardsdatascience.com/how-does-ai-see-your-country-3899e4057735?gi=37908d3059d3", "font": {"color": "#000000", "size": 20}, "id": 10, "label": "How does AI see your", "main": 1, "main_title": "How does AI see your country?", "shape": "star", "size": 32.833333333333336, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: How does AI see your country?\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: AI Art: Beauty And The Bias\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The text discusses an exhibition of AI-generated art that was created using different countries\u0027 names as prompts to generate images. The author used a new AI-generating tool called the midjourney art generator and posted the results on their blog. While the generator produced some beautiful and interesting images, the author notes that bias is inherent in selecting prompts, as well as in the images that the generator draws from. The author also suggests that using an automated inspiration tool rather than an artificial intelligence tool would be a better strategy. \n\nKEYWORDS\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2023-01-29 (\u0027afternoon\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 8.8 (154 / 1356)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 19\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1957\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 926 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 47.3% (926 / 1957)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 32.8% (641 / 1957)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 23.2% (641 / 1957)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 18.9% (369 / 1957)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 9.0% (177 / 1957)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 26.0% (508 / 1957)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e art, beauty, bias, countries, midjourney, exhibition, inspiration, ai, generator, prompts\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e prompt(24), image(18), art(17), midjourney(15), use(13), make(10), science(10), country(9), reflects(9), nation(9), liked(9), reroll(9), results(8), get(8), future(8), would(8), progress(8), bias(7), taking(7), past(7), first(7), see(6), generator(6), set(6), people(6), disappointment(6), beautiful(5), let(5), without(5), outputs(5)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e science progress(5), reflects nation(3), art generator(3), disappointment way(3), illustration animals(3), animals showed(3), set country(3), country science(3), progress taking(3), taking place(3), place future(3), beautiful bias(2), systems reflects(2), nation identical(2), prompt get(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e illustration animals showed(3), set country science(3), country science progress(3), science progress taking(3), progress taking place(3), taking place future(3), systems reflects nation(2), reflects nation identical(2), play art tools(2), art tools thinking(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/how-does-ai-see-your-country-3899e4057735?gi=37908d3059d3", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Willpower is not the solution! Here\u0027s how to do better\u2026", "domain": "https://kozyrkov.medium.com/your-new-years-resolutions-will-fail-1135ceebdd80", "font": {"color": "#000000", "size": 20}, "id": 11, "label": "Your New Year\u0027s Reso", "main": 1, "main_title": "Your New Year\u0027s Resolutions Will Fail", "shape": "star", "size": 30.555555555555557, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Your New Year\u0027s Resolutions Will Fail\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Willpower is not the solution! Here\u0027s how to do better\u2026\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article discusses the phenomenon of failed new year resolutions and suggests that willpower is not the solution. Instead, the author advocates for contingency planning, adjusting goals, and learning from mistakes. The article emphasizes the importance of self-improvement and motivation in achieving success, and offers tips for goal setting.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027https://kozyrkov.medium.com/\u0027\u003eMedium\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2023-01-08 (\u0027afternoon\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 9.5 (72 / 684)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 6\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 770\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 356 (normal)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 46.2% (356 / 770)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 39.6% (305 / 770)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 26.2% (305 / 770)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 20.8% (160 / 770)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 9.1% (70 / 770)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 23.1% (178 / 770)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e new year resolutions, willpower, success, failure, contingency planning, adjusting goals, learning from mistakes, self-improvement, motivation, goal setting\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e resolutions(13), plan(12), learning(9), mistakes(8), like(7), make(7), new(6), year(6), better(6), approach(5), goals(5), times(4), running(4), means(4), adjusting(4), fail(3), willpower(3), success(3), something(3), thing(3), failure(3), give(3), resolved(3), work(3), try(3), contingency(3), miles(3), week(3), get(3), forget(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e new year(4), year resolutions(4), learning something(3), learning mistakes(3), contingency plan(3), next times(2), redesigning approach(2), build plan(2), plan adapting(2), resolved running(2), make mistakes(2), mistakes forget(2), forget resolutions(2), resolutions means(2), means end(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e new year resolutions(4), make mistakes forget(2), mistakes forget resolutions(2), forget resolutions means(2), resolutions means end(2), plan adjusting goals(2), built plan handling(2), plan handling mistakes(2), handling mistakes adjusting(2), mistakes adjusting approach(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://kozyrkov.medium.com/your-new-years-resolutions-will-fail-1135ceebdd80", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "How to set better goals without shooting yourself in the foot", "domain": "https://kozyrkov.medium.com/are-your-goals-hurting-or-helping-you-ff09eaf40fcf", "font": {"color": "#000000", "size": 20}, "id": 12, "label": "Are your goals hurti", "main": 1, "main_title": "Are your goals hurting or helping you?", "shape": "star", "size": 31.13888888888889, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Are your goals hurting or helping you?\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: How to set better goals without shooting yourself in the foot\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article discusses the importance of setting clear and concrete goals in order to avoid vague outcomes and default behaviors that may harm your progress. The author suggests using outcome goals and process goals together, framing goals wisely, and being mindful of the goodhart law. Additionally, an actionable and measurable action plan with clear targets is needed to achieve success. By framing and focusing on motivation, you can increase your chances of success in achieving your goals. The author also warns against falling short of targets and the dangers of focusing on performance goals to the detriment of outcome goals. Finally, the importance of regularly evaluating progress and reassessing goals is emphasized.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027https://kozyrkov.medium.com/\u0027\u003eMedium\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2023-01-03 (\u0027evening\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 9.6 (93 / 897)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 9\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1698\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 799 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 47.1% (799 / 1698)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 32.7% (555 / 1698)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 23.7% (555 / 1698)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 20.4% (346 / 1698)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 9.9% (168 / 1698)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 23.4% (397 / 1698)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e goals, self improvement, outcome goals, process goals, framing, motivation, goodhart law, action plan, measurable progress, performance goals \n\nsummary\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e goal(50), outcome(14), measure(10), motivation(10), process(9), days(9), make(8), vague(8), take(8), window(8), better(7), resolutions(7), rat(7), help(6), framing(6), comes(6), performance(6), like(6), best(5), default(5), goodhart(5), law(5), target(5), every(5), serving(5), means(5), ends(5), started(5), matters(5), setting(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e outcome goal(14), process goal(9), goodhart law(5), performance goal(5), vague goal(4), proxy goal(4), self improvement(3), goal first(3), default take(3), take granted(3), asking whether(3), increases motivation(3), shoot foot(2), goal vague(2), vague concrete(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e default take granted(3), goal vague concrete(2), putting outcome goal(2), outcome goal first(2), framing goal wisely(2), risk falling victim(2), falling victim goodhart(2), victim goodhart law(2), easy deprioritize vague(2), deprioritize vague goal(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://kozyrkov.medium.com/are-your-goals-hurting-or-helping-you-ff09eaf40fcf", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "How motivational gurus prey on the vulnerable", "domain": "https://kozyrkov.medium.com/break-the-cycle-of-bad-self-improvement-advice-558e1ee2a487", "font": {"color": "#000000", "size": 20}, "id": 13, "label": "Break the cycle of b", "main": 1, "main_title": "Break the cycle of bad self-improvement advice", "shape": "star", "size": 34.888888888888886, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Break the cycle of bad self-improvement advice\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: How motivational gurus prey on the vulnerable\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The text talks about the dangers of blindly following self-improvement advice from motivational gurus and the importance of breaking the cycle of bad advice by being careful and using two good sources of advice. It suggests that, when seeking self-improvement advice, one should look to science and personalized findings rather than simply copying role models or relying on dogmatic views of scientific studies. Instead, readers are encouraged to study the best scientific studies on the subject, to personalize their approach to self-improvement, and to be wary of charlatans offering false hope.\n\nKEYWORDS\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027https://kozyrkov.medium.com/\u0027\u003eMedium\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2023-01-01 (\u0027evening\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.2%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 9.1 (228 / 2074)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 25\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1422\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 669 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 47.0% (669 / 1422)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 33.8% (481 / 1422)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 24.1% (481 / 1422)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 21.3% (303 / 1422)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 11.0% (156 / 1422)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 21.8% (310 / 1422)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e self-improvement, advice, motivation, vulnerable, commitment, science, statistics, caution, role models, personalized, skepticism\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e plan(14), people(11), different(10), self(9), science(9), improvement(8), copy(8), like(7), good(7), design(7), work(7), someone(7), fit(7), advice(6), get(6), might(6), average(6), need(6), think(6), approach(6), unique(6), make(6), new(5), apply(5), studying(5), personally(5), else(5), trying(5), stick(5), circumstances(5)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e self improvement(7), new year(4), someone else(4), role models(3), different people(3), people different(3), key upgrade(3), always good(2), decision scientist(2), source advice(2), many people(2), apply average(2), apply individuals(2), average human(2), design policies(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e different people different(3), self improvement program(2), someone else plan(2), circumstances unique key(2), unique key upgrade(2), key upgrade thoughtful(2), upgrade thoughtful skeptical(2), different someone circumstances(2), copy path serve(2), path serve well(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://kozyrkov.medium.com/break-the-cycle-of-bad-self-improvement-advice-558e1ee2a487", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Resolutions that actually work, according to a decision scientist", "domain": "https://kozyrkov.medium.com/the-15-new-years-resolutions-you-need-to-make-right-now-c0ec111bd212", "font": {"color": "#000000", "size": 20}, "id": 14, "label": "The best New Year\u0027s ", "main": 1, "main_title": "The best New Year\u0027s resolutions you can make", "shape": "star", "size": 36.77777777777778, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: The best New Year\u0027s resolutions you can make\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Resolutions that actually work, according to a decision scientist\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The text provides insights on how to make New Year resolutions actually work according to a decision scientist. It suggests personalizing resolutions instead of copying others, designing a failure action plan, using nudges and sludges, and negotiating with multiple selves. The author also emphasizes aligning incentives and setting realistic goals while focusing on short-term commitments as well.\n\nKEYWORDS\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027https://kozyrkov.medium.com/\u0027\u003eMedium\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-12-27 (\u0027evening\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.2%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 9.2 (296 / 2724)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 42\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1956\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 924 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 47.2% (924 / 1956)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 32.0% (625 / 1956)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 22.5% (625 / 1956)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 20.6% (403 / 1956)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 10.2% (200 / 1956)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 23.7% (463 / 1956)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e new year resolutions, personalization, failure action plan, nudges, sludges, negotiation skills, multiple selves, incentives, realistic goals, short-term commitments\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e make(22), resolution(18), resolve(18), planning(17), learned(12), new(11), run(11), take(10), goals(10), times(10), short(8), priority(8), year(7), stick(7), better(7), choices(7), chance(6), get(6), action(6), long(6), give(6), design(6), motivational(6), keeps(6), problem(6), work(5), success(5), different(5), might(5), fitness(5)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e new year(7), year resolution(6), long run(5), non priority(5), short run(5), make resolution(3), self improvement(3), feel free(3), give resolve(3), make space(3), multiple selves(3), according decisions(2), decisions scientist(2), make new(2), planning fitness(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e new year resolution(6), according decisions scientist(2), someone else planning(2), falling short targets(2), set realistic goals(2), every default take(2), default take granted(2), take granted missed(2), granted missed opportunity(2), missed opportunity better(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://kozyrkov.medium.com/the-15-new-years-resolutions-you-need-to-make-right-now-c0ec111bd212", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Phase 1 - Research AI", "domain": "https://kozyrkov.medium.com/2022-a-productivity-revolution-f34f32a27e5b", "font": {"color": "#000000", "size": 20}, "id": 15, "label": "2022: The year that ", "main": 1, "main_title": "2022: The year that changed the way we work", "shape": "star", "size": 39.22222222222222, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: 2022: The year that changed the way we work\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Phase 1 - Research AI\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The text discusses the productivity revolution that took place in 2022 and how it changed the way people work. While technology and data were already being used to improve productivity, 2022 saw a significant increase in the number of tools and proofs of concept being developed that fueled the revolution. Applied research played a big role in this development, with researchers inventing general-purpose algorithms and approaches that could be used to solve specific business problems. The rise of applied engineers and programming also allowed for automation of tasks, increasing personal productivity. The article highlights the importance of not ignoring recent developments in productivity tools and recognizes the potential of innovative technology to change our lives.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027https://kozyrkov.medium.com/\u0027\u003eMedium\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-12-24 (\u0027evening\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.3%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 7.6 (384 / 2910)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 40\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1721\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 791 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 46.0% (791 / 1721)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 34.5% (593 / 1721)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 24.8% (593 / 1721)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 20.0% (345 / 1721)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 9.2% (159 / 1721)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 23.3% (401 / 1721)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e productivity, technology, data, automation, personal productivity, applied research, programming, artificial intelligence, innovation, tools\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e productivity(24), 2022(12), could(11), used(10), tool(10), revolution(9), applied(9), year(8), write(8), much(8), personal(8), research(8), way(7), make(6), data(6), solutions(6), tech(6), many(5), one(5), would(5), people(5), look(5), everyone(5), engineers(5), automate(5), code(5), art(5), work(4), showing(4), thing(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e productivity revolution(5), personal productivity(5), productivity tool(4), traditional software(3), software engineers(3), year change(2), way work(2), make people(2), flash lightning(2), year 2022(2), 2022 remarkable(2), remarkable sheer(2), sheer amount(2), amount showing(2), showing witnessed(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e traditional software engineers(3), year 2022 remarkable(2), 2022 remarkable sheer(2), remarkable sheer amount(2), sheer amount showing(2), amount showing witnessed(2), showing witnessed frontiers(2), witnessed frontiers productivity(2), tool proofs concept(2), proofs concept productivity(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://kozyrkov.medium.com/2022-a-productivity-revolution-f34f32a27e5b", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Will AI fully exit the realm of science fiction and begin to change everything?", "domain": "https://kozyrkov.medium.com/ai-science-fiction-vs-reality-a8bee2ab711d", "font": {"color": "#000000", "size": 20}, "id": 16, "label": "AI: Science Fiction ", "main": 1, "main_title": "AI: Science Fiction vs Reality", "shape": "star", "size": 32.44444444444444, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: AI: Science Fiction vs Reality\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Will AI fully exit the realm of science fiction and begin to change everything?\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The text discusses the intersection of science fiction and reality, with the realm of science fiction beginning to change due to advancements in technology like machine learning and creating sentient beings. The homonym soup is also addressed, as the meaning of words can differ depending on context. Finally, the practical applications of technology and understanding its differences from science fiction are highlighted.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027https://kozyrkov.medium.com/\u0027\u003eMedium\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-12-21 (\u0027afternoon\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 10.3 (140 / 1436)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 16\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1417\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 673 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 47.5% (673 / 1417)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 37.3% (529 / 1417)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 26.6% (529 / 1417)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 19.2% (272 / 1417)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 9.2% (130 / 1417)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 25.1% (355 / 1417)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e science fiction, homonyms, machine learning, sentient beings, artificial intelligence, turing test, homonym soup, practical solutions, technology, nature, summary\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e fiction(18), science(16), useful(10), humans(9), questions(8), talking(7), version(7), people(7), differences(7), let(6), see(6), homonym(6), machine(6), fully(5), exit(5), day(5), looking(5), terms(5), one(5), answer(5), every(5), without(5), like(5), need(5), data(5), two(5), realm(4), change(4), everything(4), intelligence(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e science fiction(16), fully exit(4), exit realm(4), realm science(4), change everything(4), fiction begin(3), begin change(3), people useful(3), words mean(3), blog post(3), see someone(3), version questions(2), law people(2), useful words(2), mean differences(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e exit realm science(4), realm science fiction(4), fully exit realm(3), science fiction begin(3), fiction begin change(3), begin change everything(3), law people useful(2), people useful words(2), useful words mean(2), words mean differences(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://kozyrkov.medium.com/ai-science-fiction-vs-reality-a8bee2ab711d", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "The Revolutionary New Tool for Conversation Generation", "domain": "https://kozyrkov.medium.com/introducing-chatgpt-aa824ad89623", "font": {"color": "#000000", "size": 20}, "id": 17, "label": "Introducing ChatGPT!", "main": 1, "main_title": "Introducing ChatGPT!", "shape": "star", "size": 70.0, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Introducing ChatGPT!\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: The Revolutionary New Tool for Conversation Generation\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article introduces ChatGPT, a conversational AI tool that uses Generative Adversarial Networks (GANs) to generate text-based responses to user inputs, allowing for completely imaginary conversations. Although some of ChatGPT\u0027s responses may lack a basis in reality, this can be an advantage, since it allows for creative and imaginative thinking and problem-solving. However, users should be aware that ChatGPT is a \"bullshitter\" tool that is unconcerned with truth or accuracy. The article suggests that the increasing sophistication of AI technologies like ChatGPT requires society to develop new habits of trust and fact-checking to avoid the spread of misinformation.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027https://kozyrkov.medium.com/\u0027\u003eMedium\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-12-08 (\u0027night\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 1.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 5.6 (1492 / 8347)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 98\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1296\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 619 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 47.8% (619 / 1296)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 39.7% (514 / 1296)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 27.4% (514 / 1296)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 20.1% (260 / 1296)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 9.5% (123 / 1296)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 25.3% (328 / 1296)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e chatgpt, gans, conversational ai, reality, creativity, generative transformers, bullshitter, truth, trust, misinformation\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e chatgpt(31), gan(13), generative(11), using(11), output(11), reality(9), response(7), code(7), looking(6), new(5), conversations(5), try(5), take(5), human(5), touches(5), tangent(5), prompt(5), like(4), incredibly(4), creativity(4), would(4), possibilities(4), needed(4), write(4), work(4), bullshitters(4), truth(4), blog(4), post(4), get(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e touches reality(5), reality tangent(5), blog post(3), output touches(3), demystifying chatgpt(2), gan generative(2), networks generative(2), fake output(2), chatgpt take(2), chatgpt response(2), response touches(2), incredibly using(2), chatgpt allows(2), allows explore(2), explore possibilities(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e touches reality tangent(5), output touches reality(3), chatgpt response touches(2), response touches reality(2), chatgpt allows explore(2), allows explore possibilities(2), explore possibilities beyond(2), possibilities beyond constraints(2), beyond constraints everyday(2), constraints everyday reality(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://kozyrkov.medium.com/introducing-chatgpt-aa824ad89623", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "A primer on interpreting other people\u0027s hypothesis tests", "domain": "https://towardsdatascience.com/overusing-the-term-statistically-significant-makes-you-look-clueless-f96e1ad1a78e?gi=1437df710609", "font": {"color": "#000000", "size": 20}, "id": 18, "label": "Overusing the Term \"", "main": 1, "main_title": "Overusing the Term \"Statistically Significant\" Makes You Look Clueless", "shape": "star", "size": 31.11111111111111, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Overusing the Term \"Statistically Significant\" Makes You Look Clueless\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: A primer on interpreting other people\u0027s hypothesis tests\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The use of the term \"statistically significant\" is often overused and misinterpreted in casual conversation. Understanding how to interpret hypothesis tests requires a framework for statistical decision making and an appreciation for effect sizes. Operationalization and assumptions are important considerations in data analysis, and the significance level and sample size are key factors in statistical reasoning. It is important to ask good questions and structure decision-making processes based on probabilities, rather than blindly accepting conclusions based on statistical significance alone.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-11-30 (\u0027afternoon\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 8.5 (92 / 783)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 6\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1544\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 744 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 48.2% (744 / 1544)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 34.5% (532 / 1544)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 24.1% (532 / 1544)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 18.8% (291 / 1544)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 10.4% (161 / 1544)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 24.5% (379 / 1544)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e statistically significant, hypothesis testing, effect size, data analysis, decision making, operationalization, assumptions, statistical reasoning, significance level, sample size\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e statistical(20), friend(19), dye(19), significance(15), make(12), blue(12), important(11), question(10), might(10), different(9), findings(9), decision(8), people(7), hypothesis(7), testing(7), know(7), data(7), experts(7), something(7), term(6), quality(6), surprised(6), means(5), settings(5), ribbon(5), cheaper(5), suppliers(5), made(5), conclusions(5), interpretation(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e statistical significance(10), blue dye(8), hypothesis testing(5), term statistical(4), blue ribbon(4), data experts(3), decision make(3), default action(3), friend findings(3), good question(3), make curious(3), primer interpretation(2), interpretation people(2), people hypothesis(2), experts hardly(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e term statistical significance(4), primer interpretation people(2), interpretation people hypothesis(2), people hypothesis testing(2), data experts hardly(2), experts hardly ever(2), statistical decision make(2), human detectable visual(2), detectable visual different(2), hypothesis testing friend(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/overusing-the-term-statistically-significant-makes-you-look-clueless-f96e1ad1a78e?gi=1437df710609", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Introduction to \"good\" statistical estimators and their properties", "domain": "https://towardsdatascience.com/why-is-mse-bias%C2%B2-variance-dbdeda6f0e70?gi=0b11a22c40fa", "font": {"color": "#000000", "size": 20}, "id": 19, "label": "Why is MSE = Bias\u00b2 +", "main": 1, "main_title": "Why is MSE = Bias\u00b2 + Variance?", "shape": "star", "size": 32.5, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Why is MSE = Bias\u00b2 + Variance?\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Introduction to \"good\" statistical estimators and their properties\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e This article provides an introduction to the bias-variance tradeoff, a popular concept encountered in statistical estimation. It explains the key equation for the MSE, which involves both bias and variance, and offers a proof of the formula. The article discusses the core building blocks that make some estimators better than others, and provides a quick checklist of statistical concepts. The article concludes with a detailed explanation of deriving the MSE formula and an introduction to the bias-variance tradeoff in machine learning.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-11-27 (\u0027afternoon\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 3.8 (142 / 546)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 7\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1968\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 801 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 40.7% (801 / 1968)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 29.6% (582 / 1968)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 20.1% (582 / 1968)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 18.6% (367 / 1968)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 8.5% (168 / 1968)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 28.2% (555 / 1968)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e bias, variance, estimation, random variable, mean, model, population, probability, statistics, estimator\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e estimator(28), \u03b8hat(28), variance(19), formula(15), bias(13), get(9), expected(9), variable(8), __________x__________(8), useful(7), take(7), one(7), mse(6), statistical(6), property(6), mean(6), like(6), estimand(6), value(6), random(6), let(6), constants(6), article(5), looking(5), letters(5), probability(5), sample(5), data(5), thing(5), since(5)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e \u03b8hat \u03b8hat(11), random variable(6), __________x__________ __________x__________(5), bias\u00b2 variance(4), bias variance(4), variance tradeoff(4), expected value(4), unbiased estimator(4), mse bias\u00b2(3), greek letters(3), estimator \u03b8hat(3), bias \u03b8hat(3), chatty explanations(2), building blocks(2), loss function(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e \u03b8hat \u03b8hat \u03b8hat(7), bias variance tradeoff(4), mse bias\u00b2 variance(3), __________x__________ __________x__________ __________x__________(3), variable like height(2), __________x__________ __________x__________\u00b2 __________x__________(2), __________x__________\u00b2 __________x__________ __________x__________(2), estimator random variable(2), pick best estimator(2), bias\u00b2 variance introduction(1)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/why-is-mse-bias%C2%B2-variance-dbdeda6f0e70?gi=0b11a22c40fa", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Battling an embarrassing new alchemy for the digital era", "domain": "https://towardsdatascience.com/the-obscure-art-of-data-design-397ffb230596?gi=97b1cf797578", "font": {"color": "#000000", "size": 20}, "id": 20, "label": "The Obscure Art of D", "main": 1, "main_title": "The Obscure Art of Data Design", "shape": "star", "size": 33.19444444444444, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: The Obscure Art of Data Design\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Battling an embarrassing new alchemy for the digital era\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article talks about how data can be valuable if used and collected smartly. It highlights the importance of good data design skills and the need for efficient data collection without ignoring the importance of data cleaning. It also warns against the misuse of data and mathemagical thinking. The author recommends using the recently released training manual \u0027Data Cards Playbook\u0027 to make data usable and useful.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-11-22 (\u0027evening\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 6.9 (167 / 1153)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 12\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1209\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 599 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 49.5% (599 / 1209)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 41.4% (501 / 1209)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 30.3% (501 / 1209)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 19.0% (230 / 1209)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 9.8% (119 / 1209)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 26.6% (322 / 1209)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e data, design, collection, cleaning, usability, skills, data cards playbook, data quality, data designers, misuse of data\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e data(41), design(11), liked(7), people(7), collection(7), one(7), signal(7), looking(6), work(5), much(5), garbage(5), use(5), let(5), city(5), good(5), time(5), general(4), get(4), user(4), entry(4), answers(4), philadelphia(4), skills(4), making(4), hype(3), magic(3), far(3), example(3), takes(3), tell(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e data design(6), data collection(6), design data(4), data magic(3), data cards(3), cards playbook(3), general data(2), equate data(2), liberty bell(2), liked one(2), left devices(2), devices people(2), people find(2), find remarkable(2), remarkable ways(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e design data collection(3), data cards playbook(3), equate data magic(2), left devices people(2), devices people find(2), people find remarkable(2), find remarkable ways(2), remarkable ways foiling(2), ways foiling data(2), foiling data collection(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/the-obscure-art-of-data-design-397ffb230596?gi=97b1cf797578", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "A quick look at everyone\u0027s favorite loss function", "domain": "https://towardsdatascience.com/why-is-mean-squared-error-mse-so-popular-4320d5f003e5?gi=e11f4cf7d19d", "font": {"color": "#000000", "size": 20}, "id": 21, "label": "Why is Mean Squared ", "main": 1, "main_title": "Why is Mean Squared Error (MSE) So Popular?", "shape": "star", "size": 32.111111111111114, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Why is Mean Squared Error (MSE) So Popular?\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: A quick look at everyone\u0027s favorite loss function\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e This text discusses the mean squared error (MSE) as a popular and easy to use loss function in machine learning models. MSE is commonly used as a scoring or performance evaluation metric, but it has its flaws when used in optimization, leading to the use of other loss functions. However, it remains a popular choice due to its simplicity and ease of implementation.\n\nKEYWORDS\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-11-18 (\u0027evening\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 3.9 (128 / 501)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 6\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 940\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 443 (normal)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 47.1% (443 / 940)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 37.9% (356 / 940)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 25.5% (356 / 940)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 19.6% (184 / 940)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 10.5% (99 / 940)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 23.4% (220 / 940)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e mse, loss function, scoring metric, optimization, machine learning, performance evaluation, outliers, regression models, legendre gauss, objective function, automl\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e mse(16), function(15), squares(10), errors(9), models(8), meaning(7), loss(7), taking(7), evaluation(7), optimize(7), first(6), metric(6), used(6), reason(6), performance(5), popular(4), looking(4), thing(4), rmse(4), calculate(4), root(4), one(4), learning(4), quick(3), everyone(3), favorite(3), literally(3), names(3), liking(3), confusing(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e loss function(7), squares errors(6), meaning squares(4), errors mse(4), mse popular(3), performance evaluation(3), everyone favorite(2), creative hurts(2), mse rmse(2), taking squares(2), squares root(2), baby first(2), first loss(2), machine learning(2), scoring function(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e meaning squares errors(4), squares errors mse(4), taking squares root(2), baby first loss(2), first loss function(2), errors mse popular(1), mse popular quick(1), popular quick looking(1), quick looking everyone(1), looking everyone favorite(1)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/why-is-mean-squared-error-mse-so-popular-4320d5f003e5?gi=e11f4cf7d19d", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Don\u0027t let poets lie to you", "domain": "https://towardsdatascience.com/fooled-by-statistical-significance-7fed1bc2caf9?gi=0bc4c9901a7b", "font": {"color": "#000000", "size": 20}, "id": 22, "label": "Fooled by statistica", "main": 1, "main_title": "Fooled by statistical significance", "shape": "star", "size": 32.72222222222222, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Fooled by statistical significance\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Don\u0027t let poets lie to you\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e In this blog post, the author argues that the popular belief that statistical significance implies something important or momentous has fooled people into approaching statistics wrong. The use of the term \"statistically significant\" should not always be considered as meaningful, as significant results do not necessarily mean that something noteworthy or important took place. The author suggests being extra cautious when interpreting statistically significant results that are often irrelevant, confusing or misleading for non-experts. This blog post is designed for beginners and experts who want to learn the value of statistical decisions and hypothesis testing. \n\nKEYWORDS\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-10-29 (\u0027evening\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 5.5 (150 / 829)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 7\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1411\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 644 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 45.6% (644 / 1411)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 35.5% (501 / 1411)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 24.7% (501 / 1411)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 20.1% (283 / 1411)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 10.7% (151 / 1411)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 23.2% (328 / 1411)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e statistics, statistical significance, decision making, null hypothesis, data, jargon, risk, trustworthy, experts, interpretation\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e statistical(28), significance(18), decision(14), use(12), trust(11), someone(10), means(9), question(9), making(8), data(7), let(6), term(6), something(6), need(5), know(5), results(5), maker(5), liked(5), jargon(5), answer(5), person(5), else(5), poets(4), lie(4), people(4), interested(4), one(4), knowledge(4), get(4), set(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e statistical significance(14), decision maker(5), someone else(5), let poets(4), poets lie(4), decision making(4), blog post(3), else statistical(3), means something(2), need know(2), know much(2), significance results(2), article take(2), significance statistical(2), null hypothesis(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e let poets lie(4), someone else statistical(3), statistical significance results(2), statistical significance statistical(2), decision maker set(2), something perverse comical(2), perverse comical popularity(2), comical popularity prop(2), popularity prop rhetoric(2), prop rhetoric bullying(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/fooled-by-statistical-significance-7fed1bc2caf9?gi=0bc4c9901a7b", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Metric design for data scientists and business leaders", "domain": "https://towardsdatascience.com/whats-the-hardest-part-of-metric-design-de2919bbf2dd?gi=9301b19dc9aa", "font": {"color": "#000000", "size": 20}, "id": 23, "label": "What\u0027s the Hardest P", "main": 1, "main_title": "What\u0027s the Hardest Part of Metric Design?", "shape": "star", "size": 30.083333333333332, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: What\u0027s the Hardest Part of Metric Design?\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Metric design for data scientists and business leaders\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e This text discusses the importance of metric design in data-driven decision making and suggests that while the process of creating metrics is fairly straightforward, the hardest part is careful planning of decision criteria to ensure the metrics accurately reflect the desired outcomes. The article emphasizes the need for decision makers to thoroughly vet decision criteria, think about real-world aspects of the problem, and collaborate with technical experts to design effective metrics. The text also suggests that metric design should receive more attention in the data science community and recommends hiring decision science talent to help with this task. It concludes by promoting an applied course designed to teach the fundamentals of metric design to beginners and experts alike.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-10-25 (\u0027night\u0027, \u0027early\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.0%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 5.6 (55 / 310)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 1\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 895\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 441 (normal)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 49.3% (441 / 895)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 40.2% (360 / 895)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 27.8% (360 / 895)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 17.5% (157 / 895)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 9.9% (89 / 895)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 25.9% (232 / 895)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e metric design, decision maker, decision criteria, simulation model, data-driven decision making, decision intelligence, data science, statistical methodology, decision science talent, course\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e metric(20), designed(14), decision(14), data(10), one(8), part(7), leader(6), making(5), hardest(4), though(4), maker(4), need(4), way(4), good(4), skills(4), problem(4), wish(4), really(4), job(4), team(3), simply(3), liked(3), wrong(3), much(3), hard(3), magic(3), lamp(3), carefully(3), criteria(3), simulating(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e metric designed(12), hardest part(4), decision maker(4), good metric(3), decision criteria(3), really really(3), part metric(2), designed metric(2), business leader(2), designed simply(2), simply jotting(2), jotting formula(2), hard part(2), carefully wish(2), making sure(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e hardest part metric(2), part metric designed(2), good metric designed(2), metric designed simply(2), designed simply jotting(2), simply jotting formula(2), really really really(2), data driven decision(2), homework thinking metric(2), thinking metric asking(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/whats-the-hardest-part-of-metric-design-de2919bbf2dd?gi=9301b19dc9aa", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "What\u0027s the hardest part of metric design?", "domain": "https://towardsdatascience.com/metric-design-for-data-scientists-and-business-leaders-b8adaf46c00?gi=28eab2414de8", "font": {"color": "#000000", "size": 20}, "id": 24, "label": "Metric Design for Da", "main": 1, "main_title": "Metric Design for Data Scientists and Business Leaders", "shape": "star", "size": 33.861111111111114, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Metric Design for Data Scientists and Business Leaders\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: What\u0027s the hardest part of metric design?\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article discusses the challenges involved in designing metrics that are essential for data-driven decision-making. Metric design involves creating well-defined decision criteria based on well-designed metrics, the ability to collect data, and skills in calculating and interpreting metrics. Metrics require literature review, data collection, and a clear definition of concepts to operationalize them. Researchers use proxies to operationalize abstract concepts like happiness, creativity, attention, or memory, which are difficult to measure precisely. The hardest part of metric design is distilling the core essence of the metric and creating a measurable proxy that captures it. The article offers practical advice for designing metrics that matter, including distilling the core idea, creating a measurable proxy, choosing a metric name that makes sense, and not getting caught up in language debates. \n\nKEYWORDS\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-10-23 (\u0027morning\u0027, \u0027morning\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 6.3 (191 / 1212)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 5\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1083\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 513 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 47.4% (513 / 1083)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 40.6% (440 / 1083)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 28.3% (440 / 1083)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 20.0% (217 / 1083)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 9.6% (104 / 1083)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 22.3% (242 / 1083)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e metric design, data-driven decisions, decision criteria, well-designed metrics, statistical skills, uncertainty, proxies, operationalization, abstract concepts, core essence, measurable proxy, metric name, language debates\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e metrics(23), designed(12), making(9), measurable(9), happiness(8), one(7), liked(7), data(6), decision(6), needs(6), proxy(5), concept(5), operationalization(5), part(4), information(4), course(4), scientists(3), leaders(3), order(3), scientifically(3), study(3), define(3), learning(3), would(3), time(3), hard(3), way(3), right(3), attention(3), let(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e metrics designed(7), designed metrics(3), user happiness(3), measurable happiness(3), data scientists(2), business leaders(2), hardest part(2), order making(2), scientifically study(2), measurable proxy(2), proxy metrics(2), metrics capture(2), metrics name(2), making sense(2), language police(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e measurable proxy metrics(2), metrics designed data(1), designed data scientists(1), data scientists business(1), scientists business leaders(1), business leaders hardest(1), leaders hardest part(1), hardest part metrics(1), part metrics designed(1), metrics designed order(1)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/metric-design-for-data-scientists-and-business-leaders-b8adaf46c00?gi=28eab2414de8", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Optimizing different functions: MSE vs RMSE vs MAD", "domain": "https://towardsdatascience.com/whats-your-computer-s-favorite-metric-98c132b0488a?gi=7520417e71f9", "font": {"color": "#000000", "size": 20}, "id": 25, "label": "What\u0027s your computer", "main": 1, "main_title": "What\u0027s your computer\u0027s favorite metric?", "shape": "star", "size": 30.47222222222222, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: What\u0027s your computer\u0027s favorite metric?\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Optimizing different functions: MSE vs RMSE vs MAD\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article discusses the Machine Squared Error (MSE) as a popular and convenient metric for model performance evaluation and optimization. However, Mean Absolute Deviation (MAD) can sometimes be a better evaluation metric, especially in handling outliers. While the author points out that calculus makes optimizing MSE easy, they also emphasize the importance of choosing appropriate loss functions for different modeling problems. The article also invites readers to check out a YouTube course on applied data science from beginners to experts.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-10-16 (\u0027afternoon\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 6.6 (69 / 452)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 1\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1188\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 551 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 46.4% (551 / 1188)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 32.2% (382 / 1188)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 21.8% (382 / 1188)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 18.4% (219 / 1188)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 11.2% (133 / 1188)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 23.6% (280 / 1188)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e mse, rmse, mad, optimization, performance evaluation, loss function, outliers, calculus, machine learning, statistical inference, data science\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e mse(30), optimize(18), mad(13), rmse(12), modeling(11), function(10), squaring(10), machine(10), loss(8), error(7), use(7), love(7), easy(7), outlier(7), algorithm(6), calculus(6), thing(6), solution(6), one(5), convenient(5), takes(5), derivatives(5), metric(4), taught(4), evaluation(4), word(4), get(4), reading(4), work(4), best(4)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e loss function(6), optimize algorithm(4), optimize mse(4), taught calculus(3), super easy(3), use rmse(3), squaring root(3), favorite metric(2), mse rmse(2), rmse mad(2), mean squaring(2), squaring error(2), choice modeling(2), performance evaluation(2), modeling optimize(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e mean squaring error(2), mse bad humans(2), bad humans good(2), humans good machine(2), good reasons first(2), reasons first derivatives(2), first derivatives ever(2), derivatives ever taught(2), ever taught calculus(2), taught calculus squaring(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/whats-your-computer-s-favorite-metric-98c132b0488a?gi=7520417e71f9", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Should you trust your VP\u0027s feedback or your manager\u0027s?", "domain": "https://towardsdatascience.com/when-your-manager-isnt-your-best-guide-841f385760b9?gi=a362cee37132", "font": {"color": "#000000", "size": 20}, "id": 26, "label": "When Your Manager Is", "main": 1, "main_title": "When Your Manager Isn\u0027t Your Best Guide", "shape": "star", "size": 30.444444444444443, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: When Your Manager Isn\u0027t Your Best Guide\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Should you trust your VP\u0027s feedback or your manager\u0027s?\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article discusses the importance of feedback for managers and how to interpret and apply it effectively. It emphasizes the need for trust in communication, the importance of context, and the role of the manager in promoting a healthy team dynamic. The author also highlights potential sources of bias and suggests strategies for avoiding them.\n\n1. feedback \n2. manager \n3. trust \n4. team \n5. communication \n6. information \n7. incompetence \n8. exceptions \n9. context \n10. leadership\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-09-22 (\u0027afternoon\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 5.4 (68 / 365)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 1\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 857\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 364 (normal)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 42.5% (364 / 857)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 41.8% (358 / 857)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 29.5% (358 / 857)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 18.3% (157 / 857)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 7.6% (65 / 857)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 24.5% (210 / 857)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e feedback, manager, trust, team, communication, information, incompetence, exceptions, context, leadership \n\nsummary\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e manager(12), feedback(6), exceptions(6), use(5), work(5), one(4), time(4), projects(4), lack(4), well(4), good(4), team(4), liked(4), context(3), source(3), knows(3), information(3), list(3), way(3), sure(3), history(3), new(3), get(3), understanding(3), walk(3), trust(2), involved(2), rule(2), thumb(2), favors(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e rule thumb(2), use eye(2), problems team(2), understanding work(2), projects liked(2), manager best(1), best guide(1), guide trust(1), trust feedback(1), feedback manager(1), manager previous(1), previous article(1), article examined(1), examined poll(1), poll argued(1)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e manager best guide(1), best guide trust(1), guide trust feedback(1), trust feedback manager(1), feedback manager previous(1), manager previous article(1), previous article examined(1), article examined poll(1), examined poll argued(1), poll argued additional(1)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/when-your-manager-isnt-your-best-guide-841f385760b9?gi=a362cee37132", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "The reason that asking \"why?\" won\u0027t always lead to impact", "domain": "https://towardsdatascience.com/cynicism-a-must-have-for-data-scientists-729638b2f517?gi=12e7ef048289", "font": {"color": "#000000", "size": 20}, "id": 27, "label": "Cynicism: a must-hav", "main": 1, "main_title": "Cynicism: a must-have for data scientists", "shape": "star", "size": 31.77777777777778, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Cynicism: a must-have for data scientists\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: The reason that asking \"why?\" won\u0027t always lead to impact\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article discusses the stages of maturity in data scientist careers and encourages them to prioritize impactful work while being skeptical of incentives and clear communication. It also explores how managing overachievers and providing effective feedback can improve team productivity, and how relying solely on senior leader directives without careful examination can result in low-quality outcomes for the organization.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-09-22 (\u0027afternoon\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 6.2 (116 / 720)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 4\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1344\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 646 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 48.1% (646 / 1344)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 40.4% (543 / 1344)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 29.5% (543 / 1344)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 19.3% (260 / 1344)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 9.4% (126 / 1344)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 23.9% (321 / 1344)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e cynicism, data scientists, impact, maturity, career progression, skepticism, incentives, team productivity, overachievers, feedback, leadership, communication, decision making\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e one(12), data(11), stages(11), ask(9), taking(8), scientists(7), get(7), impactful(6), work(6), signal(6), overachiever(6), manager(6), reasons(5), team(5), organization(5), cynicism(4), question(4), many(4), makes(4), need(4), leader(4), might(4), telling(4), let(4), value(4), good(4), source(4), feedback(4), keep(3), situation(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e data scientists(7), source variance(3), face value(3), sisyphean spiral(2), every stages(2), senior leader(2), get clear(2), clear signal(2), signal straightforward(2), straightforward might(2), might seem(2), pots leader(2), leader boil(2), boil incentives(2), incentives withholding(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e get clear signal(2), clear signal straightforward(2), signal straightforward might(2), straightforward might seem(2), pots leader boil(2), leader boil incentives(2), boil incentives withholding(2), incentives withholding transparency(2), withholding transparency climb(2), transparency climb woodwork(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/cynicism-a-must-have-for-data-scientists-729638b2f517?gi=12e7ef048289", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Why taking leaders at face value is a risky career move", "domain": "https://kozyrkov.medium.com/is-your-boss-telling-you-the-truth-f03ae554ce4d", "font": {"color": "#000000", "size": 20}, "id": 28, "label": "Is your boss telling", "main": 1, "main_title": "Is your boss telling you the truth?", "shape": "star", "size": 31.694444444444443, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Is your boss telling you the truth?\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Why taking leaders at face value is a risky career move\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article discusses the importance of honesty and effective communication in leadership roles. It recognizes the possibility of bosses misrepresenting the truth or outright lying, highlighting the potential risks of taking leaders at face value. While good leaders are expected to give straight answers, bad ones might resort to lies for personal gain. However, revealing everything might not always be beneficial, and leaders must balance transparency with discretion. It is essential to understand the context and motivations behind a leader\u0027s actions to accurately interpret their words. Despite the challenges, getting to know a leader and their team personally is vital in building trust and ensuring success in the long run.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027https://kozyrkov.medium.com/\u0027\u003eMedium\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-09-21 (\u0027afternoon\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 7.0 (113 / 791)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 6\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 969\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 443 (normal)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 45.7% (443 / 969)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 41.5% (402 / 969)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 28.5% (402 / 969)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 18.8% (182 / 969)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 10.3% (100 / 969)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 25.9% (251 / 969)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e risky career move, lies, truth, leadership, communication, team, boss, honesty, interpretation, context\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e leader(15), boss(8), get(7), answer(6), good(5), personal(5), people(5), individual(5), truths(4), job(4), gives(4), one(4), programs(4), makes(4), group(4), responsible(4), levels(4), success(4), reading(4), tell(3), take(3), face(3), career(3), lie(3), lly(3), straight(3), might(3), developer(3), computer(3), use(3)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e straight answer(3), individual contributor(3), boss boss(3), tell truths(2), face value(2), usu lly(2), expect good(2), good leader(2), leader gives(2), gives straight(2), answer understood(2), understood job(2), personal career(2), developer programs(2), programs computer(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e expect good leader(2), good leader gives(2), leader gives straight(2), gives straight answer(2), straight answer understood(2), answer understood job(2), developer programs computer(2), programs computer leader(2), computer leader programs(2), leader programs people(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://kozyrkov.medium.com/is-your-boss-telling-you-the-truth-f03ae554ce4d", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "Questions you\u0027ll learn to ask when you mature as a data scientist", "domain": "https://towardsdatascience.com/before-you-ask-why-ask-who-b90841d578b4?gi=4d972a02e0cc", "font": {"color": "#000000", "size": 20}, "id": 29, "label": "Before You Ask \"Why\"", "main": 1, "main_title": "Before You Ask \"Why\"\u2026 Ask \"Who\"", "shape": "star", "size": 33.27777777777778, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: Before You Ask \"Why\"\u2026 Ask \"Who\"\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: Questions you\u0027ll learn to ask when you mature as a data scientist\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article discusses the stages of becoming a mature data scientist, beginning from the novice phase to the mature stage where the professional hasn\u0027t stopped asking questions. It suggests that people tend to assume they have arrived and get cynical if they do not meet their expectations. However, maturing in data science, as the article presents it, means asking questions to seek the truth, encouraging constructive criticism and dissent, and seeking personal growth. The discussion on project management, collaboration, and organizational alignment cap the article.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-09-16 (\u0027evening\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 7.4 (170 / 1257)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 4\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 644\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 268 (normal)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 41.6% (268 / 644)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 44.7% (288 / 644)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 29.2% (288 / 644)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 22.2% (143 / 644)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 5.1% (33 / 644)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 17.9% (115 / 644)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e asking questions, stages of becoming a mature data scientist, problem-solving, impact, organizational alignment, leadership, technical work, collaboration, project management, personal growth\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e ask(13), stage(9), get(7), mature(6), learn(5), data(5), question(4), scientist(4), enough(4), thinking(3), start(3), leader(3), good(3), truth(3), reading(3), lucky(2), without(2), problems(2), solving(2), many(2), cows(2), job(2), simply(2), put(2), stakeholders(2), might(2), reasons(2), withhold(2), lines(2), people(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e data scientist(4), ask ask(3), mature data(3), learn ask(2), ask mature(2), scientist stage(2), start ask(2), simply put(2), put stakeholders(2), stakeholders leader(2), leader might(2), might good(2), good reasons(2), reasons withhold(2), withhold truth(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e mature data scientist(3), ask mature data(2), data scientist stage(2), simply put stakeholders(2), put stakeholders leader(2), stakeholders leader might(2), leader might good(2), might good reasons(2), good reasons withhold(2), reasons withhold truth(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/before-you-ask-why-ask-who-b90841d578b4?gi=4d972a02e0cc", "urls": []}, {"color": "#fdfd96", "counter": 1, "description": "When simple random sampling is not that simple", "domain": "https://towardsdatascience.com/how-to-create-a-sampling-plan-for-your-data-project-3b14bfd81f3a?gi=9760a8c5a0f4", "font": {"color": "#000000", "size": 20}, "id": 30, "label": "How to create a samp", "main": 1, "main_title": "How to create a sampling plan for your data project", "shape": "star", "size": 30.194444444444443, "stats": "\n        \u003cb\u003eHeading 1\u003c/b\u003e: How to create a sampling plan for your data project\u003cbr\u003e\n        \u003cb\u003eHeading 2\u003c/b\u003e: When simple random sampling is not that simple\u003cbr\u003e\n        \u003cb\u003eChatGPT Summary\u003c/b\u003e:\u003cbr\u003e The article focuses on creating a sampling plan for real-world data using simple random sampling (SRS) as a statistical technique for designing the data collection process. The author uses a story to explain the importance of designing a sampling plan correctly. The article suggests that the sampling plan must cover real-world practical aspects of data collection and underscores the importance of effective data design as an art form to achieve better results. The article offers practical tips for working with secondary inherited data, including purchased data, and emphasizes the significance of paying attention to practical aspects and details of data collection. Finally, the article encourages readers to learn about the importance of data design, which is often overlooked in traditional college courses.\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003ePublication\u003c/b\u003e: \u003ca href=\u0027towardsdatascience.com\u0027\u003eTowards Data Science\u003c/a\u003e \u003cbr\u003e\n        \u003cb\u003ePublished At\u003c/b\u003e: 2022-08-17 (\u0027afternoon\u0027, \u0027late\u0027)\u003cbr\u003e\n        \u003cb\u003eVoters - Followers %\u003c/b\u003e: 0.0%\u003cbr\u003e\n        \u003cb\u003eClaps per Person\u003c/b\u003e: 4.4 (59 / 258)\u003cbr\u003e\n        \u003cb\u003eResponses\u003c/b\u003e: 1\u003cbr\u003e\n        \u003cbr\u003e\n        \u003cb\u003eWord Count (All)\u003c/b\u003e: 1891\u003cbr\u003e\n        \u003cb\u003eWord Count (Stemmed)\u003c/b\u003e: 875 (medium)\u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 46.3% (875 / 1891)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 32.0% (606 / 1891)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 22.5% (606 / 1891)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 20.3% (384 / 1891)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 8.3% (157 / 1891)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 22.1% (418 / 1891)\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eChatGPT Keywords\u003c/b\u003e:\u003cbr\u003e create sampling plan, simple random sampling, real-world data, data design, data analysis, data scientist, distribution, statistical techniques, data collection, sampling procedure \n\nsummary\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e write(36), data(24), sampling(21), trees(21), plan(17), hiker(17), instruct(11), details(10), time(8), one(8), use(8), random(7), real(7), world(7), forest(7), know(7), collecting(7), painted(7), recording(7), practical(6), selected(6), number(6), id(6), scheme(6), procedure(6), part(5), design(5), measure(5), professionals(5), need(5)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e sampling plan(7), real world(7), data collecting(6), sampling scheme(6), data design(4), foolproof instruct(4), collecting agent(4), sampling procedure(4), trees forest(3), selected trees(3), professionals statistician(3), make sure(3), measure recording(3), practical aspects(3), simple random(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e data collecting agent(4), simple random sampling(2), always strive giving(2), strive giving foolproof(2), giving foolproof instruct(2), foolproof instruct never(2), instruct never know(2), never know wild(2), know wild fool(2), wild fool appear(2)\u003cbr\u003e\u003cbr\u003e\n        \u003cbr\u003e\n        ", "url": "https://towardsdatascience.com/how-to-create-a-sampling-plan-for-your-data-project-3b14bfd81f3a?gi=9760a8c5a0f4", "urls": []}, {"counter": 659, "description": "**_everyone_**", "domain": "bit.ly", "id": 100001, "label": "bit.ly|659", "main": 0, "shape": "dot", "size": 52, "url": "bit.ly", "urls": ["awfully anthropomorphized language|http://bit.ly/quaesita_ethics", " MFML 084 - Making tiny changes to AI code|http://bit.ly/mfml_084", "learning process|http://bit.ly/quaesita_mrbean", "previous installment|https://bit.ly/quaesita_metricdesign", "decision-maker|http://bit.ly/quaesita_di", "watching a video|http://bit.ly/quaesita_ytunboxing", "here|http://bit.ly/quaesita_ethics", "AI researchers|http://bit.ly/quaesita_researchhire", " MFML 019 - How to avoid machine learning pitfalls|http://bit.ly/mfml_019", "data-driven decisions|http://bit.ly/quaesita_inspired", "automated inspiration|http://bit.ly/quaesita_history2", "stats|http://bit.ly/quaesita_statistics", "charlatans|http://bit.ly/quaesita_charlatan", "population|http://bit.ly/quaesita_popwrong", "**Statistics**|http://bit.ly/quaesita_statistics", "model performance score|http://bit.ly/mfml_039", " MFML 057 - Statistics versus \"statistics\"|http://bit.ly/mfml_057", "bit.ly/machinefriend|http://bit.ly/machinefriend", "pronouncing data with a capital \u2018D\u0027|http://bit.ly/quaesita_hist", "MFML 099 - SVMs and the kernel trick|http://bit.ly/mfml_099", "**Where does math impostor syndrome come from?**|http://bit.ly/quaesita_impostor", "data design|http://bit.ly/quaesita_philadelphia", "Data Cards Playbook|https://bit.ly/datacardsplaybook", "MFML 096 - What are perceptrons?|http://bit.ly/mfml_096", "Assume|http://bit.ly/quaesita_saddest", "change their minds|http://bit.ly/quaesita_statistics", "gentle intro blog post|http://bit.ly/quaesita_bivar", "MFML 101 - XAI and the interpretability debate|http://bit.ly/mfml_101", "evil genie|http://bit.ly/quaesita_genie", "MFML 116 - Understanding the components of your neural network|http://bit.ly/mfml_116", "data scientist|http://bit.ly/quaesita_datascim", "art and creativity|http://bit.ly/quaesita_drugs", "correlation|http://bit.ly/quaesita_correlation", "_estimand_|http://bit.ly/quaesita_vocab", "random sample|https://bit.ly/quaesita_srstrees1", "data collection process|http://bit.ly/quaesita_srstrees2", "fit|https://bit.ly/quaesita_msemad", "I do|https://bit.ly/linkedin_cassie", " MFML 022 - Skilled decision-makers|http://bit.ly/mfml_022", "significance level, power/sample size, and assumptions|http://bit.ly/quaesita_statistics", "GANs|http://bit.ly/mfml_015", " MFML 017 - Explainability and AI|http://bit.ly/mfml_017", "Gross|http://bit.ly/quaesita_scifi", "MFML 120 - When to use neural networks|http://bit.ly/mfml_120", " MFML 056 - How to speed up your ML/AI training phase|http://bit.ly/mfml_056", "the one you care about|http://bit.ly/quaesita_parrot", "zero variance|http://bit.ly/quaesita_gistlist", "random variable X|http://bit.ly/quaesita_distributions", "MFML 094 - Lazy learning and k-NN|http://bit.ly/mfml_094", "minicourse|http://bit.ly/quaesita_sministats", "bit.ly/mfml_006|http://bit.ly/mfml_006", "explanation featuring Mr. Bean|http://bit.ly/quaesita_mrbean", "operationalized|http://bit.ly/quaesita_opera", "data distribution|http://bit.ly/quaesita_distributions", "sci-fi-flavored names|http://bit.ly/mfml_096", " MFML 025 - Wish responsibly|http://bit.ly/mfml_025", "great for raising interesting questions|http://bit.ly/quaesita_inspired", "data engineers|https://bit.ly/quaesita_dataeng", "**metrics**|http://bit.ly/quaesita_dmguide", "**_everyone_**|http://bit.ly/quaesita_smfml1", "magic happens|http://bit.ly/quaesita_stc013", "Part 1|http://bit.ly/quaesita_bivar1", "data scientists|http://bit.ly/quaesita_datascim", "outliers|http://bit.ly/quaesita_ytoutliers", "most optimal! |http://bit.ly/mfml_046", "MAD|https://bit.ly/quaesita_msemad", "statistician|http://bit.ly/quaesita_statistics", " MFML 054 - Is training an AI system easy?|http://bit.ly/mfml_054", "**_model_**|https://bit.ly/quaesita_emperorm", "MFML 007 - Multiple linear regression|http://bit.ly/mfml_007", " MFML 029 - Where to start with applied AI?|http://bit.ly/mfml_029", "regularization|http://bit.ly/quaesita_059", "uncertainty|http://bit.ly/quaesita_uncertainty", "small detour|http://bit.ly/quaesita_lossvsmetric", "decision|http://bit.ly/quaesita_damnedlies", " MFML 021 - Why do businesses fail at machine learning?|http://bit.ly/mfml_021", "Again|http://bit.ly/quaesita_philadelphia", " MFML 034 - Semi-supervised learning|http://bit.ly/mfml_034", " MFML 078 - Productionization|http://bit.ly/mfml_078", "p-value|http://bit.ly/quaesita_pesky", " MFML 067 - What if you skip debugging?|http://bit.ly/mfml_067", "overfitting|http://bit.ly/quaesita_sydd", "data engineers|http://bit.ly/quaesita_dataeng", " MFML 083 - Be careful with chained models|http://bit.ly/mfml_083", "MFML 114 - What\u0027s inside a neural network?|http://bit.ly/mfml_114", "Image by the author. [AI-generated|https://bit.ly/quaesita_countries", "never trust|http://bit.ly/quaesita_donttrust", " MFML 089 - Live traffic experiments|http://bit.ly/mfml_089", "training phase|http://bit.ly/quaesita_mrbean", "MFML 008 - Feature engineering|http://bit.ly/mfml_008", "MFML 109 - When should you use linear regression?|http://bit.ly/mfml_109", "parabolae are super easy to optimize|http://bit.ly/quaesita_msefav", "here|http://bit.ly/quaesita_notyours", "here|http://bit.ly/quaesita_opera", "huge personal productivity opportunities|http://bit.ly/quaesita_2022", "MLOps|http://bit.ly/mfml_078", "here|https://bit.ly/quaesita_unboxing", "**Is data science a bubble?**|http://bit.ly/quaesita_bubble", "don\u0027t love|http://bit.ly/quaesita_ethics", "ML/AI models|http://bit.ly/quaesita_emperor", " MFML 040 - False positives and true negatives|http://bit.ly/mfml_040", "here\u0027s my list of questions you might like to ask yourself|https://bit.ly/quaesita_motivated", "more reasons|http://bit.ly/quaesita_falseboss", "data-driven decision|http://bit.ly/quaesita_inspired", "data-splitting|http://bit.ly/quaesita_sydd", "data-driven|http://bit.ly/quaesita_inspired", "P(_X_= _x_)|http://bit.ly/quaesita_distributions", "here|http://bit.ly/quaesita_scientists", "here|http://bit.ly/quaesita_ytunboxing", " MFML 007 - Multiple linear regression|http://bit.ly/mfml_007", "**metrics**|http://bit.ly/quaesita_opera", "defining clearly and unambiguously|http://bit.ly/quaesita_opera", "means to an end|https://bit.ly/quaesita_goalsetting", "data/AI/ML/statistics/analytics|http://bit.ly/quaesita_universe", "wishing irresponsibly|http://bit.ly/quaesita_genie", "mean|http://bit.ly/quaesita_gistlist", "field guide to a distribution\u0027s parameters|http://bit.ly/quaesita_lemur", "machine learning|http://bit.ly/quaesita_emperorm", "here|http://bit.ly/quaesita_2022", "Making Friends with Machine Learning|http://bit.ly/funaicourse", "operationalized|https://bit.ly/quaesita_metricdesign", "metrics|https://bit.ly/quaesita_metricdesign", "random variables|http://bit.ly/quaesita_distributions", "logicking it out with variances|http://bit.ly/quaesita_cynicism", "MFML 006 - Simple linear regression|http://bit.ly/mfml_006", " MFML 062 - Debugging your machine learning model|http://bit.ly/mfml_062", " MFML 049 - The danger of overfitting|http://bit.ly/mfml_049", " MFML 035 - Reinforcement learning|http://bit.ly/mfml_035", "**data**|http://bit.ly/quaesita_hist", "**sources of variance**|http://bit.ly/quaesita_falseboss", "validation set|http://bit.ly/quaesita_idiot", "[Part 1|http://bit.ly/quaesita_bivar1", " MFML 027 - Our AI future|http://bit.ly/mfml_027", "MFML 107 - What\u0027s so naive about Naive Bayes?|http://bit.ly/mfml_107", "this guide|http://bit.ly/quaesita_motivated", "decision science|http://bit.ly/quaesita_di", "regression|http://bit.ly/mfml_006", "MFML 092 - Opening the black box|http://bit.ly/mfml_092", "training set|http://bit.ly/quaesita_mrbean", "default action|http://bit.ly/quaesita_damnedlies", "_estimate_|http://bit.ly/quaesita_vocab", "rerunning it|http://bit.ly/quaesita_drugs", "loss function|http://bit.ly/quaesita_emperorm", "default|http://bit.ly/quaesita_damnedlies", "_pre_dict|http://bit.ly/quaesita_parrot", " MFML 087 - How to catch outliers and AI failures|http://bit.ly/mfml_087", "link|https://bit.ly/quaesita_unboxing", "Image created by the author. Learn more about it here: [http://bit.ly/quaesita_fail|http://bit.ly/quaesita_fail", "can\u0027t trust|http://bit.ly/quaesita_scientists", "MFML 102 - Decision trees and SVMs compared|http://bit.ly/mfml_102", "statistical|http://bit.ly/quaesita_statvocab", " MFML 010 - Why did we wait so long for AI?|http://bit.ly/mfml_010", "Part 1|http://bit.ly/quaesita_srstrees1", "the quality of answer|http://bit.ly/quaesita_falseboss", " MFML 069 - Model validation done right|http://bit.ly/mfml_069", "data|https://bit.ly/quaesita_srstrees1", " MFML 082 - The training-serving skew|http://bit.ly/mfml_082", "decision framing|http://bit.ly/quaesita_damnedlies", "data leadership|http://bit.ly/quaesita_dsleaders", "makes sense|https://bit.ly/quaesita_damnedlies", "me|http://bit.ly/quaesita_damnedlies", "discrete|http://bit.ly/quaesita_datatypes", "moments|http://bit.ly/quaesita_lemur", "MFML 111 - What are all those sigmoid functions for?|http://bit.ly/mfml_111", " MFML 037 - Data science flowchart|http://bit.ly/mfml_037", "Image classification|https://bit.ly/kozverteximage", "debugging set|http://bit.ly/mfml_062", "MSE|http://bit.ly/quaesita_babymse", " MFML 006 - Simple linear regression|http://bit.ly/mfml_006", " MFML 036 - What on earth is data science?|http://bit.ly/mfml_036", "come to two different - and completely valid - conclusions|http://bit.ly/quaesita_saddest", " MFML 060 - Features you should never use in AI|http://bit.ly/mfml_060", "test set|http://bit.ly/mfml_071", " MFML 081 - How often should you retrain your AI system?|http://bit.ly/mfml_081", " MFML 038 - Don\u0027t forget data!|http://bit.ly/mfml_038", "here|https://bit.ly/funaicourse", "fun article|http://bit.ly/quaesita_noeqns", " MFML 071 - What\u0027s the difference between testing and validation|http://bit.ly/mfml_071", "my take on that question|http://bit.ly/quaesita_scientists", " MFML 026 - AI is a team sport!|http://bit.ly/mfml_026", "_speaker_|http://bit.ly/talks_playlist", "data scientists|http://bit.ly/quaesita_bubblem", "summarizing|http://bit.ly/quaesita_vocab", " MFML 070 - Validation roulette|http://bit.ly/mfml_070", "more complexity than your information supports|http://bit.ly/mfml_049", "Learn more about that here.|http://bit.ly/quaesita_lossvsmetric", "validation set|http://bit.ly/quaesita_12steps", "SRS|http://bit.ly/quaesita_srstrees1", " MFML 009 - What is AI?|http://bit.ly/mfml_009", "AI|http://bit.ly/funaicourse", "**machine learning**|http://bit.ly/quaesita_simplest", "operationalize|http://bit.ly/quaesita_opera", "here|http://bit.ly/quaesita_aibiasm", "bias|http://bit.ly/quaesita_biasdef", "buy into the assumptions|http://bit.ly/quaesita_saddest", " MFML 052 - Exploratory data analysis (EDA)|http://bit.ly/mfml_052", "you choose to trust them|http://bit.ly/quaesita_scientists", "specific way of turning data into computer code|http://bit.ly/quaesita_aim", "this appendix|http://bit.ly/quaesita_trustvp", " MFML 005 - What\u0027s inside the black box?|http://bit.ly/mfml_005", "p|http://bit.ly/quaesita_pesky", "MFML 106 - Introduction to Bayes\u0027 Rule|http://bit.ly/mfml_106", "a solid rule of thumb|http://bit.ly/quaesita_cynicism", "decision-making|http://bit.ly/quaesita_di", " MFML 016 - Why trust AI?|http://bit.ly/mfml_016", "flexible|http://bit.ly/quaesita_emperor", "MFML 093 - Clustering and k-means|http://bit.ly/mfml_093", " MFML 046 - Loss functions|http://bit.ly/mfml_046", "Text classification|https://bit.ly/kozvertextext", " MFML 077 - The importance of testing|http://bit.ly/mfml_077", "MFML 104 - What is a random forest?|http://bit.ly/mfml_104", "parameters|http://bit.ly/quaesita_vocab", "Vertex AI|https://bit.ly/kozvertex", "I did it too|http://bit.ly/quaesita_lemur", " MFML 050 - Should you care about underfitting?|http://bit.ly/mfml_050", "null hypothesis|http://bit.ly/quaesita_damnedlies", "data engineering|http://bit.ly/quaesita_dataeng", "**Estimators**|http://bit.ly/quaesita_vocab", "bias|http://bit.ly/quaesita_aibiasm", "average human|https://bit.ly/netflix_average", " MFML 043 - Ground truth|http://bit.ly/mfml_043", " MFML 068 - What to do when model validation fails|http://bit.ly/mfml_068", "previous article|http://bit.ly/quaesita_who", " MFML 042 - Performance metrics|http://bit.ly/mfml_042", "moment|http://bit.ly/quaesita_lemur", "negative expected value|http://bit.ly/quaesita_gistlist", "p-value|http://bit.ly/quaesita_puppies", "wishful blindness|http://bit.ly/quaesita_confirmation", "sample size|http://bit.ly/quaesita_gistlist", " MFML 088 - AI safety and policy layers|http://bit.ly/mfml_088", "this form|http://bit.ly/makecassietalk", "analytics|http://bit.ly/quaesita_careeranalyst", " MFML 045 - What is optimization?|http://bit.ly/mfml_045", "Primary vs inherited data|http://bit.ly/quaesita_provenance", "MFML 098 - What are support vectors?|http://bit.ly/mfml_098", "statisticians|http://bit.ly/quaesita_pointofstats", " MFML 051 - The importance of data splitting|http://bit.ly/mfml_051", "previous post|https://bit.ly/quaesita_mseeg", "MFML 118 - Gotchas, pros, and cons of deep learning|http://bit.ly/mfml_118", "science fiction for guidance|http://bit.ly/quaesita_scifi", " MFML 020 - Decision Intelligence|http://bit.ly/mfml_020", " MFML 047 - Setting launch criteria|http://bit.ly/mfml_047", "ChatGPT|http://bit.ly/quaesita_chattygpt", "loss function|http://bit.ly/quaesita_emperor", "neural networks|http://bit.ly/quaesita_emperor", "Forget the Robots, Here\u0027s How AI Will Get You|http://bit.ly/quaesita_ethics", "here|https://bit.ly/quaesita_bardrelease", "MFML 110 - Logistic regression|http://bit.ly/mfml_110", " MFML 065 - Understanding k-fold cross-validation|http://bit.ly/mfml_065", "_x_ that _X_can take|http://bit.ly/quaesita_distributions", "previous article|http://bit.ly/quaesita_cynicism", " MFML 075 - Statistical significance|http://bit.ly/mfml_075", "machine learning course|http://bit.ly/mfml_006", "risk settings|http://bit.ly/quaesita_statistics", "correlated|http://bit.ly/quaesita_correlation", " MFML 080 - Solving AI latency problems|http://bit.ly/mfml_080", "statisticians|http://bit.ly/quaesita_statistics", " MFML Part 4 - Guide to AI algorithms|http://bit.ly/mfml_part4", "data-driven|https://bit.ly/quaesita_inspired", "http://bit.ly/quaesita_p1|http://bit.ly/quaesita_p1", " MFML 061 - Can you skip the training phase in AI?|http://bit.ly/mfml_061", "making data useful|http://bit.ly/quaesita_datascim", "MFML 095 - The curse of dimensionality|http://bit.ly/mfml_095", "distribution|http://bit.ly/quaesita_distributions", "here|http://bit.ly/quaesita_mseformula", "one course of action over another|http://bit.ly/quaesita_damnedlies", "AI anthropomorphization|http://bit.ly/quaesita_ethics", "MFML 115 - Using AI for automatic feature extraction|http://bit.ly/mfml_115", "here|http://bit.ly/quaesita_chattygpt", "estimate|http://bit.ly/quaesita_vocab", " MFML 002 - Why use machine learning?|http://bit.ly/mfml_002", " MFML 000 - Welcome|http://bit.ly/mfml_000", "less gaudy|http://bit.ly/quaesita_chattygpt", "applied ML/AI|http://bit.ly/quaesita_fail", " MFML 058 - When your machine learning project takes forever|http://bit.ly/mfml_058", "here|https://bit.ly/mfml_long", " MFML 024 - Preventable disasters|http://bit.ly/mfml_024", " most optimal! |http://bit.ly/mfml_046", "MFML 117 - Backpropagation|http://bit.ly/mfml_117", "statistics|http://bit.ly/quaesita_statistics", "p|http://bit.ly/quaesita_puppies", "validate this model|http://bit.ly/quaesita_idiot", " MFML 044 - Precision vs recall|http://bit.ly/mfml_044", "data experts|http://bit.ly/quaesita_universe", "AI notebooks|https://bit.ly/kozvertexnotebooks", "continuous|http://bit.ly/quaesita_datatypes", "write|http://bit.ly/quaesita_chattygpt", "metric|https://bit.ly/quaesita_metricdesign", "actions|http://bit.ly/quaesita_hypexample", "data science|http://bit.ly/mfml_036", " MFML 031 - Instances, features, and targets|http://bit.ly/mfml_031", "Philadelphia Problem|http://bit.ly/quaesita_philadelphia", "optimization algorithm|http://bit.ly/mfml_045", "four-way split|http://bit.ly/quaesita_history2", "This is the first output from running the same query I used for my [viral ChatGPT post in December 2022|http://bit.ly/quaesita_chattygpt", "http://bit.ly/quaesita_ytunboxing|http://bit.ly/quaesita_ytunboxing", " MFML 004 - How to test ML|http://bit.ly/mfml_004", "model|http://bit.ly/quaesita_emperorm", "errors|http://bit.ly/quaesita_babymse", "machine learning|http://bit.ly/quaesita_simplestm", " |http://bit.ly/mfml_091", "overfitting|http://bit.ly/mfml_049", "Get it here: [bit.ly/datacardsplaybook|https://bit.ly/datacardsplaybook", "Why Businesses Fail At AI|http://bit.ly/quaesita_failm", "personalized calibration|https://bit.ly/quaesita_motivated", "phenomenal manager|http://bit.ly/quaesita_dsleaders", "here|http://bit.ly/quaesita_badadvice", " MFML 063 - Hyperparameter tuning|http://bit.ly/mfml_063", " MFML 003 - How does machine learning work?|http://bit.ly/mfml_003", "A screenshot from the [unboxing video|http://bit.ly/quaesita_ytunboxing", "data charlatanism|http://bit.ly/quaesita_charlatan", " MFML 028 - The 12 steps of AI|http://bit.ly/mfml_028", "[data|http://bit.ly/quaesita_hist", "I am|https://bit.ly/insta_cassie", "here|http://bit.ly/quaesita_drugs", " MFML 055 - A dataset\u0027s idea shape|http://bit.ly/mfml_055", "here|http://bit.ly/quaesita_gpt4ceo", "how statistics works|http://bit.ly/quaesita_saddest", " MFML 018 - Intro to training, validation, and testing|http://bit.ly/mfml_018", "science|http://bit.ly/quaesita_scientists", " MFML 059 - Regularization|http://bit.ly/mfml_059", "Part 3|http://bit.ly/quaesita_bivar3", " MFML 001 - What is machine learning?|http://bit.ly/mfml_001", "here|http://bit.ly/quaesita_damnedlies", "data|http://bit.ly/quaesita_hist", "un_certain_ty|http://bit.ly/quaesita_un_certain_ty", "utterer|http://bit.ly/quaesita_charlatan", "previous article|https://bit.ly/quaesita_mseeg", " MFML 073 - Interpreting AI test output|http://bit.ly/mfml_073", "next installment|https://bit.ly/quaesita_hardmetrics", " MFML 072 - The 12 steps of statistics|http://bit.ly/mfml_072", " MFML 039 - What is \"good behavior\" for AI?|http://bit.ly/mfml_039", " |http://bit.ly/mfml_090", "MFML 112 - How to do ranking at scale|http://bit.ly/mfml_112", "Part 2|http://bit.ly/quaesita_bivar2", "MFML 119 - Neural network architecture|http://bit.ly/mfml_119", " MFML 053 - How to select an AI algorithm|http://bit.ly/mfml_053", " MFML 008 - Feature engineering|http://bit.ly/mfml_008", "here|http://bit.ly/quaesita_babymse", " MFML 033 - Unsupervised learning|http://bit.ly/mfml_033", " MFML 011 -Algorithms, data, and compute|http://bit.ly/mfml_011", " MFML 085 - When your AI model fails retesting|http://bit.ly/mfml_085", "ask **_who_**|http://bit.ly/quaesita_**_who_**", " MFML 076 - What should you do if testing fails|http://bit.ly/mfml_076", "isn\u0027t trivial|http://bit.ly/quaesita_srstrees2", "here|http://bit.ly/quaesita_falseboss", "definitely the human|http://bit.ly/quaesita_drugs", "null hypothesis|http://bit.ly/quaesita_fisher", "algorithms|http://bit.ly/quaesita_emperorm", "error|http://bit.ly/quaesita_msefav", "Data Cards Playbook|http://bit.ly/datacardsplaybook", "use the same function for both|http://bit.ly/quaesita_lossvsmetric", "much less attention|http://bit.ly/quaesita_first", "MFML 097 - Maximal margin classifiers|http://bit.ly/mfml_097", "loss function|http://bit.ly/quaesita_msefav", "using patterns in labeled image data|http://bit.ly/quaesita_emperorm", "before they\u0027re ready for them|http://bit.ly/quaesita_22", "MSE|http://bit.ly/quaesita_msefav", "hyperparameter|http://bit.ly/mfml_063", "statistical|http://bit.ly/quaesita_statistics", "outcome goal|http://bit.ly/quaesita_motivated", "my list of motivation-hacking questions|http://bit.ly/quaesita_motivated", "scientifically|http://bit.ly/quaesita_scientists", "AI idiots|http://bit.ly/quaesita_idiot", " MFML 074 - Understanding p-values|http://bit.ly/mfml_074", "Video classification|https://bit.ly/kozvertexvideo", "information|http://bit.ly/quaesita_hist", "MFML 113 - Introduction to deep learning|http://bit.ly/mfml_113", "data science|http://bit.ly/quaesita_datasci", "here|http://bit.ly/quaesita_trustvp", "statistical sampling|http://bit.ly/quaesita_vocab", "posers|http://bit.ly/quaesita_charlatan", "statistical glossary|http://bit.ly/quaesita_gistlist", "senior stakeholders|http://bit.ly/quaesita_falseboss", "data scientists|https://bit.ly/quaesita_roles", " MFML 086 - The danger of the long tail in AI|http://bit.ly/mfml_086", "step-by-step process|http://bit.ly/mfml_12steps", "over a century to stub its toe|http://bit.ly/quaesita_dearsir", " MFML 012 - Real applications|http://bit.ly/mfml_012", " MFML 041 - Confusion matrix|http://bit.ly/mfml_041", "MFML 108 - What do all regression models have in common?|http://bit.ly/mfml_108", "some math|http://bit.ly/mfml_113", "assumptions|http://bit.ly/quaesita_saddest", "**Estimands**|http://bit.ly/quaesita_vocab", "operationalization|https://bit.ly/quaesita_metricdesign", "datasets|https://bit.ly/quaesita_srstrees1", "ML for tabular data|https://bit.ly/kozvertextables", "new approach to programming|http://bit.ly/quaesita_sbucks", " MFML 014 - Human creativity in AI|http://bit.ly/mfml_014", "039|http://bit.ly/quaesita_039", "rejected|http://bit.ly/quaesita_fisher", " MFML 015 - How Do GANs work?|http://bit.ly/mfml_015", "[Part 2|http://bit.ly/quaesita_bivar2", "this|http://bit.ly/quaesita_damnedlies", "units or categories|http://bit.ly/quaesita_datatypes", "ML/AI|http://bit.ly/quaesita_emperor", " MFML 032 - Supervised learning|http://bit.ly/mfml_032", " MFML 023 - Reliable or unreliable?|http://bit.ly/mfml_023", "Real world data collection|https://bit.ly/quaesita_srstrees1", "AI art generator|http://bit.ly/quaesita_drugs", "little detour|http://bit.ly/quaesita_statistics", "DALL\u00b7E 2|https://bit.ly/quaesita_ytdalle", " MFML 064 - What is a holdout set and how do you use it?|http://bit.ly/mfml_064", "optimization|https://bit.ly/mfml_045", "AI|http://bit.ly/quaesita_aim", " MFML 079 - Repurposing data safely|http://bit.ly/mfml_079", "potential productivity boost|http://bit.ly/quaesita_2022", "MFML 100 - What is a decision tree?|http://bit.ly/mfml_100", "An analogy for AI by the author from the article \"[Why Businesses Fail at Machine Learning.|http://bit.ly/quaesita_failm", "**loss function**|http://bit.ly/quaesita_emperorm", "underfitting|http://bit.ly/mfml_050", "happiness|http://bit.ly/quaesita_opera", " MFML 030 - Classification vs regression|http://bit.ly/mfml_030", "MFML 105 - Ensemble models|http://bit.ly/mfml_105", " MFML 066 - Advanced AI debuggin|http://bit.ly/mfml_066", "data science|http://bit.ly/quaesita_datascim", " MFML 013 - How to find good AI use cases|http://bit.ly/mfml_013", "analytics|http://bit.ly/quaesita_battle", "earlier article|http://bit.ly/quaesita_insignif", "MFML 103 - Boosted Aggregation a.k.a. Bagging|http://bit.ly/mfml_103", " MFML 048 - Data engineering|http://bit.ly/mfml_048"]}, {"counter": 1, "description": "Image by Randall Munroe, [xkcd.com", "domain": "what-if.xkcd.com", "id": 100002, "label": "what-if.xkcd.com", "main": 0, "shape": "dot", "size": 10, "url": "what-if.xkcd.com", "urls": ["Image by Randall Munroe, [xkcd.com|https://what-if.xkcd.com/5/"]}, {"counter": 1, "description": "CC", "domain": "xkcd.com", "id": 100003, "label": "xkcd.com", "main": 0, "shape": "dot", "size": 10, "url": "xkcd.com", "urls": ["CC|https://xkcd.com/license.html"]}, {"counter": 21, "description": "Hal Ableson", "domain": "en.wikipedia.org", "id": 100004, "label": "en.wikipedia.org|21", "main": 0, "shape": "dot", "size": 50, "url": "en.wikipedia.org", "urls": ["In Greek mythology, Zeus punished Sisyphus for cheating death twice by forcing him to roll an immense boulder up a hill only for it to roll down every time it neared the top, repeating this action for eternity. Painting by Titian in the Museo del Prado, source: [Wikipedia|https://en.wikipedia.org/wiki/Sisyphus", "Midas|https://en.wikipedia.org/wiki/Midas", "Tolstoy|https://en.wikipedia.org/wiki/Anna_Karenina_principle", "Liberty Bell|https://en.wikipedia.org/wiki/Liberty_Bell", "simulation|https://en.wikipedia.org/wiki/Simulation", "Illustration from the Wikipedia page on elevation measurement using a hypsometer: [https://en.wikipedia.org/wiki/Hypsometer|https://en.wikipedia.org/wiki/Hypsometer", "list|https://en.wikipedia.org/wiki/List_of_sovereign_states", "Goodhart\u0027s Law|https://en.wikipedia.org/wiki/Goodhart%27s_law", "fisherman and his wife|https://en.wikipedia.org/wiki/The_Fisherman_and_His_Wife", "formula here|https://en.wikipedia.org/wiki/Median_absolute_deviation", "economics says about this|https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem", "GEOM|https://en.wikipedia.org/wiki/Geometric_distribution", "Hal Ableson|https://en.wikipedia.org/wiki/Hal_Abelson", "transformer|https://en.wikipedia.org/wiki/Transformer_(machine_learning_model", "Sisyphean|https://en.wikipedia.org/wiki/Sisyphus", "Ceteris paribus|https://en.wikipedia.org/wiki/Ceteris_paribus", "whole formula|https://en.wikipedia.org/wiki/Mean_squared_error#Predictor", "choice architecture ideas from behavioral economics|https://en.wikipedia.org/wiki/Nudge_theory"]}, {"counter": 33, "description": "Twitter", "domain": "twitter.com", "id": 100005, "label": "twitter.com|33", "main": 0, "shape": "dot", "size": 52, "url": "twitter.com", "urls": ["Twitter|https://twitter.com/quaesita", "poll|https://twitter.com/quaesita/status/1570855013190348802", "homework for the interested reader|https://twitter.com/kareem_carr/status/1535402776276217859?t=R60xEOxxTYkBq7mkj8CvnQ\u0026s=09", "[Your author|https://twitter.com/quaesita", "too many fingers|https://twitter.com/quaesita/status/1610296941623283712", "the poll below|https://twitter.com/quaesita/status/1570855013190348802", "[Twitter poll link|https://twitter.com/quaesita/status/1570855013190348802", "like it|https://twitter.com/quaesita/status/1609627580289646592", "a poll|https://twitter.com/quaesita/status/1570855013190348802", "like it|https://twitter.com/quaesita/status/1610376439257354240", "here|https://twitter.com/quaesita/status/1572999334806814723"]}, {"counter": 22, "description": "YouTube", "domain": "www.youtube.com", "id": 100006, "label": "youtube.com|22", "main": 0, "shape": "dot", "size": 52, "url": "www.youtube.com", "urls": ["Disney song for you|https://www.youtube.com/watch?v=L0MK7qz13bU", "DALL\u00b7E 2|https://www.youtube.com/watch?v=nr8sSFHT7Ew", "YouTube|https://www.youtube.com/channel/UCbOX--VOebPe-MMRkatFRxw"]}, {"counter": 19, "description": "Substack", "domain": "decision.substack.com", "id": 100007, "label": "decision.substack.com|19", "main": 0, "shape": "dot", "size": 46, "url": "decision.substack.com", "urls": ["Substack|http://decision.substack.com", "Substack|http://decision.substack.com/"]}, {"counter": 28, "description": "LinkedIn", "domain": "www.linkedin.com", "id": 100008, "label": "linkedin.com|28", "main": 0, "shape": "dot", "size": 52, "url": "www.linkedin.com", "urls": ["LinkedIn|https://www.linkedin.com/in/kozyrkov/", "decision scientist|https://www.linkedin.com/in/kozyrkov/", "Mahima Pushkarna|https://www.linkedin.com/in/mahimapushkarna/", "here|https://www.linkedin.com/posts/kozyrkov_bigideas2023-activity-7008755618324824064-BO2Y?", "recovering statistician|https://www.linkedin.com/in/kozyrkov/", "#statistics|https://www.linkedin.com/feed/hashtag/?keywords=statistics\u0026highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6691746839667978240"]}, {"counter": 2, "description": "Your [author", "domain": "decision.wtf", "id": 100009, "label": "decision.wtf|2", "main": 0, "shape": "dot", "size": 12, "url": "decision.wtf", "urls": ["Your [author|https://decision.wtf/"]}, {"counter": 9, "description": "ChatGPT", "domain": "chat.openai.com", "id": 100010, "label": "chat.openai.com|9", "main": 0, "shape": "dot", "size": 26, "url": "chat.openai.com", "urls": ["GPT-4|https://chat.openai.com", "sign up for ChatGPT Plus|https://chat.openai.com", "ChatGPT Plus|https://chat.openai.com/", "ChatGPT|https://chat.openai.com", "waitlist|https://chat.openai.com", "ChatGPT Plus|https://chat.openai.com"]}, {"counter": 1, "description": "here", "domain": "bard.google.com", "id": 100011, "label": "bard.google.com", "main": 0, "shape": "dot", "size": 10, "url": "bard.google.com", "urls": ["here|https://bard.google.com/"]}, {"counter": 1, "description": "LLM Red Team", "domain": "cdn.openai.com", "id": 100012, "label": "cdn.openai.com", "main": 0, "shape": "dot", "size": 10, "url": "cdn.openai.com", "urls": ["LLM Red Team|https://cdn.openai.com/papers/gpt-4.pdf"]}, {"counter": 5, "description": "Med-PaLM 2", "domain": "blog.google", "id": 100013, "label": "blog.google|5", "main": 0, "shape": "dot", "size": 18, "url": "blog.google", "urls": ["LaMDA|https://blog.google/technology/ai/lamda/", "Med-PaLM 2|https://blog.google/technology/health/ai-llm-medpalm-research-thecheckup/"]}, {"counter": 17, "description": "AutoML", "domain": "console.cloud.google.com", "id": 100014, "label": "console.cloud.google.com|17", "main": 0, "shape": "dot", "size": 42, "url": "console.cloud.google.com", "urls": ["AutoML|https://console.cloud.google.com/?walkthrough_id=automl_quickstart"]}, {"counter": 7, "description": "Bard", "domain": "t.co", "id": 100015, "label": "t.co|7", "main": 0, "shape": "dot", "size": 22, "url": "t.co", "urls": ["Bard|https://t.co/dMIayxUP6n", "sign up for Bard|https://t.co/dMIayxUP6n", "waitlist|https://t.co/dMIayxUP6n"]}, {"counter": 1, "description": "Cliffs Notes", "domain": "www.cliffsnotes.com", "id": 100016, "label": "cliffsnotes.com", "main": 0, "shape": "dot", "size": 10, "url": "www.cliffsnotes.com", "urls": ["Cliffs Notes|https://www.cliffsnotes.com/"]}, {"counter": 9, "description": "GPT-4", "domain": "openai.com", "id": 100017, "label": "openai.com|9", "main": 0, "shape": "dot", "size": 26, "url": "openai.com", "urls": ["GPT-4|https://openai.com/research/gpt-4", "OpenAI|https://openai.com/", "Image created by the author using several tools, including [DALL\u00b7E 2|https://openai.com/dall-e-2/", "Guess where the title for this blog post came from\u2026 this is ChatGPT in action! Try it here, no tech skills needed: [https://openai.com/blog/chatgpt/|https://openai.com/blog/chatgpt/", "http://openai.com/blog/chatgpt/|https://openai.com/blog/chatgpt/", "give it a try|http://openai.com/blog/chatgpt/"]}, {"counter": 1, "description": "Google", "domain": "research.google", "id": 100018, "label": "research.google", "main": 0, "shape": "dot", "size": 10, "url": "research.google", "urls": ["Google|https://research.google/"]}, {"counter": 1, "description": "factoid", "domain": "www.forbes.com", "id": 100019, "label": "forbes.com", "main": 0, "shape": "dot", "size": 10, "url": "www.forbes.com", "urls": ["factoid|https://www.forbes.com/sites/gilpress/2023/03/06/generative-ai-and-the-future-of-creative-jobs/?sh=1f9f02286617"]}, {"counter": 1, "description": "**How to deal with tricky questions**", "domain": "medium.com", "id": 100020, "label": "medium.com", "main": 0, "shape": "dot", "size": 10, "url": "medium.com", "urls": ["**How to deal with tricky questions**|https://medium.com/hackernoon/how-to-deal-with-tricky-questions-2fc3103ab5a8"]}, {"counter": 10, "description": "**Data scientist: The sexiest job of the 22nd century**", "domain": "towardsdatascience.com", "id": 100021, "label": "towardsdatascience.com|10", "main": 0, "shape": "dot", "size": 28, "url": "towardsdatascience.com", "urls": ["**How to be an AI idiot**|https://towardsdatascience.com/how-to-be-an-ai-idiot-8559c65d91a8", "**Forget the robots! Here\u0027s how AI will get you**|https://towardsdatascience.com/forget-the-robots-heres-how-ai-will-get-you-b674c28d6a34", "**Stats Gist List: An irreverent statistician\u0027s guide to jargon**|https://towardsdatascience.com/stats-gist-list-an-irreverent-statisticians-guide-to-jargon-be8173df090d", "**Data Science Leaders: There are too many of you**|https://towardsdatascience.com/data-science-leaders-there-are-too-many-of-you-37bff8088505", "**Statistics for people in a hurry**|https://towardsdatascience.com/statistics-for-people-in-a-hurry-a9613c0ed0b", "**Why arguing about metrics is a waste of time**|https://towardsdatascience.com/why-arguing-about-metrics-is-a-waste-of-time-b1c6f9026724", "**Data scientist: The sexiest job of the 22nd century**|https://towardsdatascience.com/the-sexiest-job-of-the-22nd-century-ffe753e1d155", "**Operationalization: the art and science of making metrics**|https://towardsdatascience.com/operationalization-the-art-and-science-of-making-metrics-31770d94998f", "**Never start with a hypothesis**|https://towardsdatascience.com/hypothesis-testing-decoded-for-movers-and-shakers-bfc2bc34da41", "**How to work with someone else\u0027s data**|https://towardsdatascience.com/how-to-work-with-someone-elses-data-f33485d79ed4"]}, {"counter": 20, "description": "**Speaking at conferences: A complete guide**", "domain": "kozyrkov.medium.com", "id": 100022, "label": "kozyrkov.medium.com|20", "main": 0, "shape": "dot", "size": 48, "url": "kozyrkov.medium.com", "urls": ["**Using AI as a perception-altering drug**|https://kozyrkov.medium.com/using-ai-as-a-perception-altering-drug-dd648bbf6e4a", "**How to hack your motivation, according to a decision scientist**|https://kozyrkov.medium.com/how-to-hack-your-motivation-according-to-a-decision-scientist-b948c2b4049e", "**Speaking at conferences: A complete guide**|https://kozyrkov.medium.com/dressing-for-the-stage-theatre-rules-apply-90a29614ad67", "**Why do we trust scientists?**|https://kozyrkov.medium.com/why-do-we-trust-scientists-98c24e3b9f0e", "this playbook|https://kozyrkov.medium.com/how-to-hack-your-motivation-according-to-a-decision-scientist-b948c2b4049e", "**Join Medium**|https://kozyrkov.medium.com/membership"]}, {"counter": 1, "description": "**Why I avoid equations in my talks**", "domain": "hackernoon.com", "id": 100023, "label": "hackernoon.com", "main": 0, "shape": "dot", "size": 10, "url": "hackernoon.com", "urls": ["**Why I avoid equations in my talks**|https://hackernoon.com/why-i-avoid-equations-in-my-talks-f7b6f7aac23a"]}, {"counter": 1, "description": "Anna Karenina", "domain": "www.goodreads.com", "id": 100024, "label": "goodreads.com", "main": 0, "shape": "dot", "size": 10, "url": "www.goodreads.com", "urls": ["Anna Karenina|https://www.goodreads.com/work/quotes/2507928"]}, {"counter": 4, "description": "Our dessert will be served in a nutshell. Image by the author.", "domain": "miro.medium.com", "id": 100025, "label": "miro.medium.com|4", "main": 0, "shape": "dot", "size": 16, "url": "miro.medium.com", "urls": ["Screenshot from thesaurus.com. My other thesaurus is terrible, terrible, and also terrible.|https://miro.medium.com/0*e_QdEEWNqxNd-Dic", "Our dessert will be served in a nutshell. Image by the author.|https://miro.medium.com/0*hTg7MrIJ0tNgJ-X6", "All copyright belongs to the author.|https://miro.medium.com/0*fUMOLGqMDq18K3S0", "All copyright belongs to the author.|https://miro.medium.com/0*VmgalO-4qWxMLPM9"]}, {"counter": 1, "description": "Discord", "domain": "discord.com", "id": 100026, "label": "discord.com", "main": 0, "shape": "dot", "size": 10, "url": "discord.com", "urls": ["Discord|https://discord.com/"]}, {"counter": 5, "description": "Midjourney bot", "domain": "midjourney.com", "id": 100027, "label": "midjourney.com|5", "main": 0, "shape": "dot", "size": 18, "url": "midjourney.com", "urls": ["Midjourney bot|https://midjourney.com/home/?callbackUrl=%2Fapp%2F", "Midjourney|https://midjourney.com/home/?callbackUrl=%2Fapp%2F", "Mech suits! Mech suits and jetpacks for everyone! Happy 2023! Image created by the author with [Midjourney|https://midjourney.com/home/?callbackUrl=%2Fapp%2F", "Image created by the author with [Midjourney|https://midjourney.com/home/?callbackUrl=%2Fapp%2F"]}, {"counter": 1, "description": "set up", "domain": "docs.midjourney.com", "id": 100028, "label": "docs.midjourney.com", "main": 0, "shape": "dot", "size": 10, "url": "docs.midjourney.com", "urls": ["set up|https://docs.midjourney.com/"]}, {"counter": 2, "description": "A screenshot of a [Midjourney newbies channel", "domain": "midjourney.gitbook.io", "id": 100029, "label": "midjourney.gitbook.io|2", "main": 0, "shape": "dot", "size": 12, "url": "midjourney.gitbook.io", "urls": ["A screenshot of a [Midjourney newbies channel|https://midjourney.gitbook.io/docs/", "Midjourney****newbies****channel|https://midjourney.gitbook.io/docs/"]}, {"counter": 1, "description": "probably never said", "domain": "www.businessinsider.com", "id": 100030, "label": "businessinsider.com", "main": 0, "shape": "dot", "size": 10, "url": "www.businessinsider.com", "urls": ["probably never said|https://www.businessinsider.com/misattributed-quotes-2013-10"]}, {"counter": 1, "description": "SMART", "domain": "expertprogrammanagement.com", "id": 100031, "label": "expertprogrammanagement.com", "main": 0, "shape": "dot", "size": 10, "url": "expertprogrammanagement.com", "urls": ["SMART|https://expertprogrammanagement.com/2017/01/goal-setting-outcome-performance-process-goals/"]}, {"counter": 1, "description": "seminal paper", "domain": "www.unipa.it", "id": 100032, "label": "unipa.it", "main": 0, "shape": "dot", "size": 10, "url": "www.unipa.it", "urls": ["seminal paper|https://www.unipa.it/dipartimenti/dems/.content/documenti/corsi/aprile2020/decision_making/1981-Tversky-and-Kahneman---The-framing-of-decisions-and-the-psychology-of-choice.pdf"]}, {"counter": 1, "description": "an app", "domain": "www.techradar.com", "id": 100033, "label": "techradar.com", "main": 0, "shape": "dot", "size": 10, "url": "www.techradar.com", "urls": ["an app|https://www.techradar.com/news/best-speech-to-text-app"]}, {"counter": 4, "description": "Gboard", "domain": "play.google.com", "id": 100034, "label": "play.google.com|4", "main": 0, "shape": "dot", "size": 16, "url": "play.google.com", "urls": ["Lensa|https://play.google.com/store/apps/details?id=com.lensa.app\u0026hl=en\u0026gl=US", "Adjusting the author\u0027s selfie with [Lensa|https://play.google.com/store/apps/details?id=com.lensa.app\u0026hl=en\u0026gl=US", "Gboard|https://play.google.com/store/apps/details?id=com.google.android.inputmethod.latin\u0026hl=en\u0026gl=US"]}, {"counter": 3, "description": "Image source: [Pixabay", "domain": "pixabay.com", "id": 100035, "label": "pixabay.com|3", "main": 0, "shape": "dot", "size": 14, "url": "pixabay.com", "urls": ["Image source: [Pixabay|https://pixabay.com/photos/birth-baby-stork-961805/", "Image source: [Pixabay|https://pixabay.com/illustrations/genie-desert-aladin-alladin-lamp-4198396/", "Image source: [Pixabay|https://pixabay.com/photos/measure-yardstick-tape-ruler-1509707/"]}, {"counter": 1, "description": "impossible, ha!", "domain": "genius.com", "id": 100036, "label": "genius.com", "main": 0, "shape": "dot", "size": 10, "url": "genius.com", "urls": ["impossible, ha!|https://genius.com/Bette-midler-wind-beneath-my-wings-lyrics"]}, {"counter": 1, "description": "classic", "domain": "www.stat.columbia.edu", "id": 100037, "label": "stat.columbia.edu", "main": 0, "shape": "dot", "size": 10, "url": "www.stat.columbia.edu", "urls": ["classic|http://www.stat.columbia.edu/~gelman/research/published/signif4.pdf"]}, {"counter": 1, "description": "chapter 7 of a master\u0027s level statistical inference textbook", "domain": "mybiostats.files.wordpress.com", "id": 100038, "label": "mybiostats.files.wordpress.com", "main": 0, "shape": "dot", "size": 10, "url": "mybiostats.files.wordpress.com", "urls": ["chapter 7 of a master\u0027s level statistical inference textbook|https://mybiostats.files.wordpress.com/2015/03/casella-berger.pdf"]}, {"counter": 1, "description": "this", "domain": "www.unil.ch", "id": 100039, "label": "unil.ch", "main": 0, "shape": "dot", "size": 10, "url": "www.unil.ch", "urls": ["this|https://www.unil.ch/files/live/sites/issr/files/shared/8._Telechargement/Cours_MA_quantiBasel/Chapter_7_Point_Estimation.pdf"]}, {"counter": 1, "description": "Legendre and Gauss used at the turn of the 19th century", "domain": "blog.bookstellyouwhy.com", "id": 100040, "label": "blog.bookstellyouwhy.com", "main": 0, "shape": "dot", "size": 10, "url": "blog.bookstellyouwhy.com", "urls": ["Legendre and Gauss used at the turn of the 19th century|https://blog.bookstellyouwhy.com/carl-friedrich-gauss-and-the-method-of-least-squares"]}, {"counter": 2, "description": "hypsometer", "domain": "bigtrees.forestry.ubc.ca", "id": 100041, "label": "bigtrees.forestry.ubc.ca|2", "main": 0, "shape": "dot", "size": 12, "url": "bigtrees.forestry.ubc.ca", "urls": ["on a slope or obscured|https://bigtrees.forestry.ubc.ca/measuring-trees/height-measurements/", "hypsometer|https://bigtrees.forestry.ubc.ca/measuring-trees/height-measurements/"]}], "user_image": "https://miro.medium.com/1*IL0mnvzNcpG2ZD0JBqo7zQ.jpeg", "user_profile": "\n        \u003cb\u003eBIO\u003c/b\u003e: Chief Decision Scientist, Google. \u2764\ufe0f Stats, ML/AI, data, puns, art, theatre, decision science. All views are my own. twitter.com/quaesita \u003cbr\u003e\n\n        \u003cb\u003eArticles\u003c/b\u003e: 30 (18418 stemmed words) \u003cbr\u003e\n        \u003cb\u003eTop article\u003c/b\u003e: \u003ca href=\u0027https://kozyrkov.medium.com/introducing-chatgpt-aa824ad89623\u0027\u003eIntroducing ChatGPT! (Medium)\u003c/a\u003e \u003cbr\u003e\n\n        \u003cb\u003ePublications\u003c/b\u003e: Towards Data Science(18), Medium(12) \u003cbr\u003e\n        \u003cb\u003eFollowers\u003c/b\u003e: 134817 \u003cbr\u003e\n        \n        \u003cb\u003eVoters - Followers % (Article AVG)\u003c/b\u003e: 0.1%\u003cbr\u003e\n        \u003cb\u003eClaps per Person (Article AVG)\u003c/b\u003e: 7.2\u003cbr\u003e\n        \u003cbr\u003e\n        \n        \u003cb\u003ePreferred Published Time\u003c/b\u003e: afternoon-late(8), afternoon-early(8), evening-late(7), night-early(3), evening-early(3), morning-morning(1) \u003cbr\u003e\n        \u003cb\u003ePreferred Published Day\u003c/b\u003e: Wednesday(9), Sunday(6), Thursday(4), Tuesday(4), Friday(3), Saturday(3), Monday(1) \u003cbr\u003e\n        \u003cb\u003ePreferred Article Length (stemmed)\u003c/b\u003e: medium(17), normal(11), large(2) \u003cbr\u003e\n        \u003cb\u003ePublished Frequency (AVG)\u003c/b\u003e: per 7.7 days (2022-08-17/2023-03-29) \u003cbr\u003e\n        \u003cb\u003eLast Seen \u003c/b\u003e: before 7 days\u003cbr\u003e\n\n        \u003cb\u003eExternal Domains per Article \u003c/b\u003e: 6.3\u003cbr\u003e\n\n        \u003cbr\u003e\n        \u003cb\u003eStemmed words / words\u003c/b\u003e: 47.4% (18418 / 38895)\u003cbr\u003e\n        \u003cb\u003eUnique words / words\u003c/b\u003e: 11.7% (4560 / 38895)\u003cbr\u003e\n        \u003cb\u003eUnique words / words (stemmed)\u003c/b\u003e: 22.1% (4072 / 18418)\u003cbr\u003e\n        \u003cb\u003eVerb / words\u003c/b\u003e: 19.5% (7582 / 38895)\u003cbr\u003e\n        \u003cb\u003eAdj / words\u003c/b\u003e: 9.5% (3704 / 38895)\u003cbr\u003e\n        \u003cb\u003eNoun / words\u003c/b\u003e: 24.6% (9583 / 38895)\u003cbr\u003e\n        \u003cbr\u003e\n        \n        \u003cb\u003eMost Common ChatGPT Keywords (UPA)\u003c/b\u003e:\u003cbr\u003e data(15), model(10), statistical(10), science(8), learning(6), design(6), decision(6), machine(5), personal(5), performance(5), bias(5), productivity(4), technology(4), motivation(4), skills(4), variance(4), plan(4), goals(4), making(4), metric(4), leadership(4), course(3), gpt(3), language(3), resolutions(3), new(3), year(3), management(3), career(3), incentives(3)\u003cbr\u003e\u003cbr\u003e\n\n        \u003cb\u003eMost Common Words (UPA)\u003c/b\u003e:\u003cbr\u003e reading(29), get(28), let(28), thanks(28), data(27), youtube(27), good(27), one(27), find(26), made(26), even(26), course(25), fun(25), beginners(23), experts(23), better(23), alike(23), applied(23), amusement(23), looking(23), interested(22), friends(22), use(22), best(22), favorite(22), might(22), see(21), also(21), never(21), would(21)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams (UPA)\u003c/b\u003e:\u003cbr\u003e find cassie(11), connect better(11), clap something(9), hitting need(8), might sure(8), amusement work(8), let whole(8), also less(8), help science(7), think video(7), good experts(7), better medium(7), find statistician(7), anything could(7), great interested(7)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams (UPA)\u003c/b\u003e:\u003cbr\u003e find statistician cassie(5), connect better medium(4), good image experts(4), decision come touch(4), statistician cassie two(4), process people someone(4), first person much(4), often focus even(4), help science towards(3), think video skills(3)\u003cbr\u003e\u003cbr\u003e\n        \n        \u003cb\u003eMost Common Words\u003c/b\u003e:\u003cbr\u003e data(255), one(136), get(131), mfml(128), use(111), make(110), good(87), people(81), let(81), mse(79), course(74), might(70), steps(69), looking(69), liked(68), science(68), better(68), work(65), take(65), statistical(62), decision(60), reading(59), first(59), write(59), need(58), way(58), learning(57), chatgpt(56), goal(56), fun(54)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Bigrams\u003c/b\u003e:\u003cbr\u003e steps mfml(65), thanks reading(27), machine learning(26), statistical significance(25), experts alike(24), beginners experts(23), data science(23), bias variance(23), course fun(22), applied course(22), made amusement(22), cassie kozyrkov(21), fun looking(21), fun beginners(21), alike one(21)\u003cbr\u003e\u003cbr\u003e\n        \u003cb\u003eMost Common Trigrams\u003c/b\u003e:\u003cbr\u003e beginners experts alike(23), course fun looking(21), fun beginners experts(21), experts alike one(21), alike one made(21), one made amusement(21), connect cassie kozyrkov(19), cassie kozyrkov let(19), kozyrkov let friends(19), let friends find(19)\u003cbr\u003e\u003cbr\u003e\n        "};
    var nodes = new vis.DataSet(data.nodes)
    var edges = new vis.DataSet(data.edges)
    var data = {nodes: nodes, edges: edges};

    // create a network
    var container = document.getElementById("mynetwork");
    var options = {
        nodes: {
            font: {size: 18},
            color: "#ffffff",
            borderWidth: 2,
        },
        edges: {
            width: 2,
        },
        layout: {
            improvedLayout: false,
            randomSeed: 191006
        },

    };

    
        options["physics"] = {
            barnesHut: {
                gravitationalConstant: -20000,
            },
            stabilization: true
        };

    

    var network = new vis.Network(container, data, options);

    // add hyperlink and info panel to nodes
    document.getElementById('infopanel').innerHTML = '<h1 class="al">Explore <a href="https://medium.com/@kozyrkov" style="color: blue;">kozyrkov\'s</a> Medium.com sky &#128640;</h1>';
    document.getElementById('infopanel').innerHTML += '<div class="al"><img id="profile" src="https://miro.medium.com/1*IL0mnvzNcpG2ZD0JBqo7zQ.jpeg"</img></div>';
    document.getElementById('infopanel').innerHTML += `<br>
        <b>BIO</b>: Chief Decision Scientist, Google.  Stats, ML/AI, data, puns, art, theatre, decision science. All views are my own. twitter.com/quaesita <br>

        <b>Articles</b>: 30 (18418 stemmed words) <br>
        <b>Top article</b>: <a href='https://kozyrkov.medium.com/introducing-chatgpt-aa824ad89623'>Introducing ChatGPT! (Medium)</a> <br>

        <b>Publications</b>: Towards Data Science(18), Medium(12) <br>
        <b>Followers</b>: 134817 <br>
        
        <b>Voters - Followers % (Article AVG)</b>: 0.1%<br>
        <b>Claps per Person (Article AVG)</b>: 7.2<br>
        <br>
        
        <b>Preferred Published Time</b>: afternoon-late(8), afternoon-early(8), evening-late(7), night-early(3), evening-early(3), morning-morning(1) <br>
        <b>Preferred Published Day</b>: Wednesday(9), Sunday(6), Thursday(4), Tuesday(4), Friday(3), Saturday(3), Monday(1) <br>
        <b>Preferred Article Length (stemmed)</b>: medium(17), normal(11), large(2) <br>
        <b>Published Frequency (AVG)</b>: per 7.7 days (2022-08-17/2023-03-29) <br>
        <b>Last Seen </b>: before 7 days<br>

        <b>External Domains per Article </b>: 6.3<br>

        <br>
        <b>Stemmed words / words</b>: 47.4% (18418 / 38895)<br>
        <b>Unique words / words</b>: 11.7% (4560 / 38895)<br>
        <b>Unique words / words (stemmed)</b>: 22.1% (4072 / 18418)<br>
        <b>Verb / words</b>: 19.5% (7582 / 38895)<br>
        <b>Adj / words</b>: 9.5% (3704 / 38895)<br>
        <b>Noun / words</b>: 24.6% (9583 / 38895)<br>
        <br>
        
        <b>Most Common ChatGPT Keywords (UPA)</b>:<br> data(15), model(10), statistical(10), science(8), learning(6), design(6), decision(6), machine(5), personal(5), performance(5), bias(5), productivity(4), technology(4), motivation(4), skills(4), variance(4), plan(4), goals(4), making(4), metric(4), leadership(4), course(3), gpt(3), language(3), resolutions(3), new(3), year(3), management(3), career(3), incentives(3)<br><br>

        <b>Most Common Words (UPA)</b>:<br> reading(29), get(28), let(28), thanks(28), data(27), youtube(27), good(27), one(27), find(26), made(26), even(26), course(25), fun(25), beginners(23), experts(23), better(23), alike(23), applied(23), amusement(23), looking(23), interested(22), friends(22), use(22), best(22), favorite(22), might(22), see(21), also(21), never(21), would(21)<br><br>
        <b>Most Common Bigrams (UPA)</b>:<br> find cassie(11), connect better(11), clap something(9), hitting need(8), might sure(8), amusement work(8), let whole(8), also less(8), help science(7), think video(7), good experts(7), better medium(7), find statistician(7), anything could(7), great interested(7)<br><br>
        <b>Most Common Trigrams (UPA)</b>:<br> find statistician cassie(5), connect better medium(4), good image experts(4), decision come touch(4), statistician cassie two(4), process people someone(4), first person much(4), often focus even(4), help science towards(3), think video skills(3)<br><br>
        
        <b>Most Common Words</b>:<br> data(255), one(136), get(131), mfml(128), use(111), make(110), good(87), people(81), let(81), mse(79), course(74), might(70), steps(69), looking(69), liked(68), science(68), better(68), work(65), take(65), statistical(62), decision(60), reading(59), first(59), write(59), need(58), way(58), learning(57), chatgpt(56), goal(56), fun(54)<br><br>
        <b>Most Common Bigrams</b>:<br> steps mfml(65), thanks reading(27), machine learning(26), statistical significance(25), experts alike(24), beginners experts(23), data science(23), bias variance(23), course fun(22), applied course(22), made amusement(22), cassie kozyrkov(21), fun looking(21), fun beginners(21), alike one(21)<br><br>
        <b>Most Common Trigrams</b>:<br> beginners experts alike(23), course fun looking(21), fun beginners experts(21), experts alike one(21), alike one made(21), one made amusement(21), connect cassie kozyrkov(19), cassie kozyrkov let(19), kozyrkov let friends(19), let friends find(19)<br><br>
        `;

    document.getElementById('infopanel').innerHTML += '<div class="al"><h3>&mdash; Description &mdash;</h3></div>';
    document.getElementById('infopanel').innerHTML += `<div><p>This is an analysis of Medium.com articles. The analysis includes the use of knowledge graphs to display the relationships between the articles and the external website domains referenced within them. Articles are represented by stars and external website domains by circles. Additionally, NLP techniques such as stemming and frequency analysis were utilized to gain a better understanding of the articles content.</p><p>For more information visit <a href="https://github.com/justdataplease/medium-sky">Github repo</a>.</p></div>`;

    network.on("click", function (params) {
        if (params.nodes.length === 1) {
            var node = nodes.get(params.nodes[0]);
            if (node && node.url) {
                var container = document.getElementById('infopanel');
                container.innerHTML = '';

                if (node.main_title != undefined) {
                    container.innerHTML += '<h3>' + node.main_title + '</h3>'
                    container.innerHTML += '<a href="' + node.url + '">' + node.url + '</a>';

                    container.innerHTML += '<h3>&mdash; Word Analysis &mdash; </h3>'
                    container.innerHTML += node.stats
                } else {
                    container.innerHTML += "<p>Domain &mdash; " + '<a href="https://' + node.domain + '">' + node.domain + "</a></p><br>";
                }
                ;


                container.innerHTML += "<h3>&mdash; Urls &mdash;</h3>"

                container.innerHTML += '<ol type="1">'

                for (var i = 0; i < node.urls.length; i++) {
                    let x = node.urls[i].split('|')
                    container.innerHTML += '<li>Title : ' + x[0];
                    container.innerHTML += '<a href="' + x[1] + '">' + x[1] + '</a></li><br>';
                }
                container.innerHTML += "</ol>";


            } else {
                document.getElementById("infopanel").innerHTML = "";
            }
        }
    });

</script>

</body>
</html>